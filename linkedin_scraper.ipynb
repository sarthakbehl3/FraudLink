{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import re\n",
    "import emoji\n",
    "from datetime import date\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6abf62",
   "metadata": {},
   "source": [
    "# Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805282b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify query and create LinkedIn search url\n",
    "\n",
    "query = 'enter query here'\n",
    "\n",
    "keywords = query.split(' ')\n",
    "url = 'https://www.linkedin.com/search/results/content/?keywords='\n",
    "for index, word in enumerate(keywords, start = 1):\n",
    "    if word[0] == '#':\n",
    "        url = url+'%23'+word[1:]\n",
    "    else:\n",
    "        url = url+word\n",
    "    if index != len(keywords):\n",
    "        url = url+'%20'\n",
    "url = url+'&origin=SWITCH_SEARCH_VERTICAL&sid=rTP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch chrome driver\n",
    "\n",
    "browser = webdriver.Chrome('chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to url, need to log in to LinkedIn before\n",
    "\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8382d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load all posts\n",
    "\n",
    "pause = 2\n",
    "\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    time.sleep(pause)\n",
    "    try:\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    except:\n",
    "        continue\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click \"Load more comments\" once for each post\n",
    "\n",
    "browser.execute_script(\"window.scrollTo(0, 0);\")\n",
    "buttons = browser.find_elements(By.XPATH, \"//span[text()='Load more comments']\")\n",
    "for button in buttons:\n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        button.click()\n",
    "    except Exception as e:\n",
    "        print(\"Cannot click button: {}\".format(e))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a389964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract browser source code and separate into individual containers for each post\n",
    "\n",
    "page = browser.page_source  \n",
    "linkedin_soup = bs(page.encode(\"utf-8\"), \"html\")\n",
    "linkedin_soup.prettify()\n",
    "containers = linkedin_soup.findAll(\"div\",{\"class\":\"feed-shared-update-v2 feed-shared-update-v2--minimal-padding full-height relative feed-shared-update-v2--e2e artdeco-card\"})\n",
    "containers2 = linkedin_soup.findAll(\"div\",{\"class\":\"feed-shared-update-v2 feed-shared-update-v2--minimal-padding full-height relative artdeco-card\"})\n",
    "containers3 = linkedin_soup.findAll(\"div\",{\"class\":\"feed-shared-update-v2 feed-shared-update-v2--minimal-padding full-height relative feed-shared-update-v2--e2e feed-shared-update-v2--wrapped\"})\n",
    "containers4 = linkedin_soup.findAll(\"div\",{\"class\":\"feed-shared-update-v2 feed-shared-update-v2--minimal-padding full-height relative feed-shared-update-v2--wrapped\"})\n",
    "\n",
    "containers.extend(containers2)\n",
    "containers.extend(containers3)\n",
    "containers.extend(containers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of posts scraped\n",
    "\n",
    "len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9421c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from source code\n",
    "\n",
    "dic = {\n",
    "    'Name': [],\n",
    "    'Biography': [],\n",
    "    'Date': [],\n",
    "    'Text': [],\n",
    "    'Comments Text': [],\n",
    "    'Likes': [],\n",
    "    'Comments': [],\n",
    "    'Reposts': [],\n",
    "    'Media Type': [],\n",
    "    'Media Link': [],\n",
    "    'Article Title': [],\n",
    "    'Article Link': [],\n",
    "    'Article Description': [],\n",
    "    'Query': [],\n",
    "    'Query Date': []\n",
    "}\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "curr_date = today.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "for container in containers:\n",
    "    try:\n",
    "\n",
    "        try:\n",
    "            name = container.find('span',{\"dir\":\"ltr\"}).text.strip()\n",
    "            dic['Name'].append(name)\n",
    "        except:\n",
    "            dic['Name'].append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            date_posted = container.find('span',{\"aria-hidden\":\"true\"}).text.strip()\n",
    "\n",
    "            if date_posted == 'Follow':\n",
    "                dic['Date'].append('N/A')\n",
    "            else:\n",
    "                dic['Date'].append(date_posted)\n",
    "\n",
    "        except:\n",
    "            dic['Date'].append(\"N/A\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            text = container.find('div',{\"class\":\"update-components-text relative feed-shared-update-v2__commentary\"}).text.strip()\n",
    "            dic['Text'].append(text)\n",
    "        except:\n",
    "            dic['Text'].append('')\n",
    "        \n",
    "        try:\n",
    "            bio = container.find('span',{\"class\":\"update-components-actor__description t-12 t-normal t-black--light\"}).text.strip()\n",
    "            dic['Biography'].append(bio)\n",
    "        except:\n",
    "            dic['Biography'].append('')\n",
    "        \n",
    "        try:\n",
    "            likes = container.find('span',{\"class\":\"social-details-social-counts__reactions-count\"}).text.strip()\n",
    "            dic['Likes'].append(likes)\n",
    "        except:\n",
    "            dic['Likes'].append(0)\n",
    "            \n",
    "        try:\n",
    "            comments = container.find('li',{\"class\":\"social-details-social-counts__item social-details-social-counts__comments\"}).text.strip()\n",
    "            dic['Comments'].append(comments)\n",
    "        except:\n",
    "            dic['Comments'].append(0)\n",
    "        \n",
    "        try:\n",
    "            reposts = container.find('button',{\"class\":\"ember-view t-black--light t-12 hoverable-link-text\"}).text.strip()\n",
    "            dic['Reposts'].append(reposts)\n",
    "        except:\n",
    "            dic['Reposts'].append(0)\n",
    "            \n",
    "        try:\n",
    "            comments_list = []\n",
    "            comments_text = container.findAll(\"div\",{\"class\":\"comments-comment-item-content-body break-words\"})\n",
    "            for comment_text in comments_text:\n",
    "                comments_list.append(comment_text.text.strip())\n",
    "            dic['Comments Text'].append(comments_list)\n",
    "        except:\n",
    "            dic['Comments Text'].append([])\n",
    "            \n",
    "        try:\n",
    "            article_img = container.find('div',{\"class\":\"update-components-article--with-large-image\"})\n",
    "            article_img_source_link = article_img.find(\"a\",{\"class\":\"app-aware-link update-components-article__image-link tap-target\"})['href']\n",
    "            article_img_title = article_img.find(\"span\",{\"dir\":\"ltr\"}).text.strip()\n",
    "            \n",
    "            try:\n",
    "                article_img_media_link = article_img.find(\"img\",{\"class\":\"ivm-view-attr__img--centered update-components-article__image lazy-image ember-view\"})['src']\n",
    "            except:\n",
    "                article_img_media_link = article_img.find(\"img\",{\"class\":\"ivm-view-attr__img--centered ivm-view-attr__img update-components-article__image lazy-image ember-view\"})['src']\n",
    "            \n",
    "            try:\n",
    "                article_img_desc = article_img.find('h2', {'class':\"update-components-article__description update-components-article__description--truncated t-12 t-black--light\"}).text.strip()\n",
    "                dic['Article Description'].append(article_img_desc)\n",
    "            except:\n",
    "                dic['Article Description'].append('N/A')\n",
    "\n",
    "            dic['Media Type'].append('Article (Large Image)')\n",
    "            dic['Media Link'].append(article_img_media_link)\n",
    "            dic['Article Title'].append(article_img_title)\n",
    "            dic['Article Link'].append(article_img_source_link)\n",
    "        except:\n",
    "            try:\n",
    "                article_no = container.find('div',{\"class\":\"update-components-article--with-no-image\"})\n",
    "                article_no_title = article_no.find(\"h2\",{\"class\":\"t-14 update-components-article__title break-words t-bold t-black\"}).text.strip()\n",
    "                article_no_source_link = article_no.find(\"a\",{\"class\":\"app-aware-link update-components-article__meta flex-grow-1 full-width tap-target display-flex justify-space-between align-items-flex-start\"})['href']\n",
    "                \n",
    "                try:\n",
    "                    article_no_description = article_no.find('h2', {'class':\"update-components-article__description update-components-article__description--truncated t-12 t-black--light\"}).text.strip()\n",
    "                except:\n",
    "                    article_no_description = 'N/A'\n",
    "\n",
    "                dic['Media Type'].append('Article (No Image)')\n",
    "                dic['Media Link'].append('N/A')\n",
    "                dic['Article Title'].append(article_no_title)\n",
    "                dic['Article Link'].append(article_no_source_link)\n",
    "                dic['Article Description'].append(article_no_description)\n",
    "            except:\n",
    "                try:\n",
    "                    article_small = container.find('div',{\"class\":\"update-components-article--with-small-image\"})\n",
    "                    article_small_media_link = article_small.find(\"img\",{\"class\":\"ivm-view-attr__img--centered update-components-article__image lazy-image ember-view\"})['src']\n",
    "                    article_small_source_link = article_small.find(\"a\",{\"class\":\"app-aware-link update-components-article__image-link tap-target\"})['href']\n",
    "                    article_small_title = article_small.find(\"span\",{\"dir\":\"ltr\"}).text.strip()\n",
    "                    \n",
    "                    dic['Media Type'].append('Article (Small Image)')\n",
    "                    dic['Media Link'].append(article_small_media_link)\n",
    "                    dic['Article Title'].append(article_small_title)\n",
    "                    dic['Article Link'].append(article_small_source_link)\n",
    "                    dic['Article Description'].append('N/A')\n",
    "                except:\n",
    "                    try:\n",
    "                        image = container.find('div',{\"class\":\"update-components-image__container\"})\n",
    "\n",
    "                        try:\n",
    "                            image_link = image.find('img',{'class':\"ivm-view-attr__img--centered update-components-image__image lazy-image ember-view\"})['src']\n",
    "                        except:\n",
    "                            image_link = image.find('img',{'class':\"ivm-view-attr__img--centered update-components-image__image update-components-image__image--constrained lazy-image ember-view\"})['src']\n",
    "                        \n",
    "                        dic['Media Type'].append('Image')\n",
    "                        dic['Media Link'].append(image_link)\n",
    "                        dic['Article Title'].append('N/A')\n",
    "                        dic['Article Link'].append('N/A')\n",
    "                        dic['Article Description'].append('N/A')\n",
    "                        \n",
    "                    except:\n",
    "                        try:\n",
    "                            video = container.find('div', {'class':\"update-components-linkedin-video__container\"})\n",
    "                            video_link = video.find('video', {'class':'vjs-tech'})['src']\n",
    "                            \n",
    "                            dic['Media Type'].append('Video')\n",
    "                            dic['Media Link'].append(video_link)\n",
    "                            dic['Article Title'].append('N/A')\n",
    "                            dic['Article Link'].append('N/A')\n",
    "                            dic['Article Description'].append('N/A')\n",
    "                        except:\n",
    "                            try:\n",
    "                                video = container.find('div', {'class':\"feed-shared-external-video feed-shared-update-v2__content\"})\n",
    "                                video_link = video.find('a', {'class':'app-aware-link tap-target external-video-viewer__play-link play-video'})['href']\n",
    "                                \n",
    "                                dic['Media Type'].append('Video')\n",
    "                                dic['Media Link'].append(video_link)\n",
    "                                dic['Article Title'].append('N/A')\n",
    "                                dic['Article Link'].append('N/A')\n",
    "                                dic['Article Description'].append('N/A')\n",
    "                            except:\n",
    "                                dic['Media Type'].append('None')\n",
    "                                dic['Media Link'].append('N/A')\n",
    "                                dic['Article Title'].append('N/A')\n",
    "                                dic['Article Link'].append('N/A')\n",
    "                                dic['Article Description'].append('N/A')\n",
    "        \n",
    "        dic['Query'].append(query)\n",
    "        dic['Query Date'].append(curr_date)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806091e",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of raw data points\n",
    "\n",
    "print('length of raw data: ', len(dic['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "\n",
    "def clean_text(text):\n",
    "    clean = emoji.replace_emoji(text)\n",
    "    clean = re.sub(r'http\\S+', '', clean)\n",
    "    clean = re.sub(r'www\\S+', '', clean)\n",
    "    clean = re.sub(\"\\\\xa0\", '', clean)\n",
    "    clean = re.sub(\"\\\\n\", '', clean)\n",
    "    clean = re.sub(\"‚Äô\", \"'\", clean)\n",
    "    return clean\n",
    "\n",
    "text_cols = [\n",
    "    'Name', \n",
    "    'Biography', \n",
    "    'Text', \n",
    "    'Article Title', \n",
    "    'Article Description'\n",
    "]\n",
    "\n",
    "for column in text_cols: \n",
    "    for i, text in enumerate(dic[column]):\n",
    "        clean = clean_text(text)\n",
    "        dic[column][i] = clean\n",
    "\n",
    "for i, comments in enumerate(dic['Comments Text']):\n",
    "    for j, text in enumerate(comments):\n",
    "        clean = clean_text(text)\n",
    "        dic['Comments Text'][i][j] = clean\n",
    "\n",
    "df = pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate posts\n",
    "\n",
    "df = df.drop_duplicates(subset=['Name', 'Biography', 'Date', 'Text', 'Media Type', 'Article Title', 'Article Description'])\n",
    "print('length after removing duplicates: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove posts with no text\n",
    "\n",
    "df = df[df['Text'] != '']\n",
    "print('length after removing posts with no text: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d148cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-English posts\n",
    "\n",
    "langs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        lang = detect(row['Text'])\n",
    "        langs.append(lang)\n",
    "    except:\n",
    "        try:\n",
    "            if row['Article Description'] == 'N/A':\n",
    "                raise Exception(\"\")\n",
    "            else:\n",
    "                lang = detect(row['Article Description'])\n",
    "                langs.append(lang)\n",
    "        except:\n",
    "            try:\n",
    "                lang = detect(row['Biography'])\n",
    "                langs.append(lang)\n",
    "            except:\n",
    "                langs.append('unknown')\n",
    "\n",
    "df['Language'] = langs\n",
    "df = df[df['Language'] == 'en']\n",
    "del df['Language']\n",
    "\n",
    "print('length after removing non-English posts: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean features\n",
    "\n",
    "df['Comments'] = df['Comments'].astype(str)\n",
    "df['Reposts'] = df['Reposts'].astype(str)\n",
    "df['Likes'] = df['Likes'].astype(str)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"Comments\"] == '':\n",
    "        row[\"Comments\"] = '0'\n",
    "\n",
    "    row['Comments'] = re.sub(\"[^0-9]\", \"\", row['Comments'])\n",
    "\n",
    "    if row[\"Reposts\"] == '':\n",
    "        row[\"Reposts\"] = '0'\n",
    "\n",
    "    row['Reposts'] = re.sub(\"[^0-9]\", \"\", row['Reposts'])\n",
    "\n",
    "    if row[\"Likes\"] == '':\n",
    "        row[\"Likes\"] = '0'\n",
    "\n",
    "    row['Likes'] = re.sub(\"[^0-9]\", \"\", row['Likes'])\n",
    "\n",
    "    row['Date'] = re.sub(\" •\", \"\", row['Date'])\n",
    "    row['Date'] = re.sub(\" Edited\", \"\", row['Date'])\n",
    "\n",
    "    if row['Comments Text'] == ['']: # this might be wrong, test it out\n",
    "        row['Comments Text'] = []\n",
    "\n",
    "\n",
    "df['Comments'] = df['Comments'].astype(int)\n",
    "df['Reposts'] = df['Reposts'].astype(int)\n",
    "df['Likes'] = df['Likes'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dates\n",
    "\n",
    "dates = []\n",
    "\n",
    "query_dates = list(pd.to_datetime(df['Query Date']))\n",
    "time_deltas = list(df['Date'])\n",
    "\n",
    "for i in range(0, len(query_dates)):\n",
    "    value = int(re.sub(\"[^0-9]\", \"\", time_deltas[i]))\n",
    "\n",
    "    if 'd' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(days=value))\n",
    "    elif 'mo' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(months=value))\n",
    "    elif 'm' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(minutes=value))\n",
    "    elif 'yr' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(years=value))\n",
    "    elif 'h' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(hours=value))\n",
    "    elif 'w' in time_deltas[i]:\n",
    "        dates.append(query_dates[i] - DateOffset(weeks=value))\n",
    "    else: \n",
    "        print(time_deltas[i])\n",
    "\n",
    "dates = pd.Series(dates).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "df['Date'] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display clean data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fecdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sanity check to make sure the scraper is working, and LinkedIn hasn't changed its source code.\n",
    "# Make sure the different media types are captured, and that there aren't too many \"None\". \n",
    "# In large scrapes, all media types should appear. \n",
    "# Media Types: Article (Large Image), Article (Small Image), Article (No Image), Image, Video, None\n",
    "\n",
    "df['Media Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scraped and cleaned data as json\n",
    "\n",
    "df.to_json(query+'.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
