{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBqAqr2XLMRP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTyUpoDN0Azr",
        "outputId": "cf303579-0516-4337-d3ea-27c50ba60469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cnn-text-classification-tf'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Total 234 (delta 0), reused 0 (delta 0), pack-reused 234\u001b[K\n",
            "Receiving objects: 100% (234/234), 1.11 MiB | 25.28 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone repo\n",
        "! git clone https://github.com/dennybritz/cnn-text-classification-tf.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_7PugZO2PTj"
      },
      "outputs": [],
      "source": [
        "# Downgrade Python (TF 2.x not supported for python >= 3.8)\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn3nHQwM2YCu",
        "outputId": "f35b0d39-af3b-41d0-c207-31010bb49e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "update-alternatives: error: no alternatives for python\n"
          ]
        }
      ],
      "source": [
        "!sudo update-alternatives --config python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "eExXONRg107T",
        "outputId": "a0c9c5d6-3dcf-484d-cab4-0975612ae84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 78 kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.38.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (2.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.50.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 67.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Install modules\n",
        "! pip install tensorflow==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZJxLHzU0Sf4",
        "outputId": "3e54db02-2160-4b91-8b14-bf2dc643718f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Loading data...\n",
            "WARNING:tensorflow:From cnn-text-classification-tf/train.py:55: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "Vocabulary Size: 53800\n",
            "Train/Dev split: 6619/735\n",
            "2022-11-30 21:56:57.368233: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-11-30 21:56:57.371977: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-11-30 21:56:57.372203: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x9f939c0 executing computations on platform Host. Devices:\n",
            "2022-11-30 21:56:57.372237: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/cnn-text-classification-tf/text_cnn.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "self.predictions shape: Tensor(\"output/Shape:0\", shape=(1,), dtype=int32)\n",
            "WARNING:tensorflow:From /content/cnn-text-classification-tf/text_cnn.py:79: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "predicted: Tensor(\"precision/Cast:0\", shape=(?,), dtype=float64)\n",
            "actual: Tensor(\"precision/Cast_1:0\", shape=(?,), dtype=float64)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Writing to /content/runs/1669845417\n",
            "\n",
            "/content/cnn-text-classification-tf/data_helpers.py:80: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  data = np.array(data)\n",
            "2022-11-30T21:56:59.912974: step 1, loss 1.84927, acc 0.546875, precision 0.137931, recall 0.5 f1 0.216216\n",
            "2022-11-30T21:57:01.241875: step 2, loss 1.32805, acc 0.65625, precision 0.230769, recall 0.2 f1 0.214286\n",
            "2022-11-30T21:57:02.573718: step 3, loss 2.43121, acc 0.5625, precision 0.333333, recall 0.12 f1 0.176471\n",
            "2022-11-30T21:57:03.931395: step 4, loss 2.3075, acc 0.6875, precision 0.375, recall 0.166667 f1 0.230769\n",
            "2022-11-30T21:57:05.266530: step 5, loss 1.69525, acc 0.65625, precision 0.25, recall 0.1875 f1 0.214286\n",
            "2022-11-30T21:57:06.602146: step 6, loss 1.86133, acc 0.671875, precision 0.2, recall 0.133333 f1 0.16\n",
            "2022-11-30T21:57:07.921486: step 7, loss 1.97991, acc 0.609375, precision 0.0625, recall 0.0909091 f1 0.0740741\n",
            "2022-11-30T21:57:09.240653: step 8, loss 2.11321, acc 0.5625, precision 0.235294, recall 0.210526 f1 0.222222\n",
            "2022-11-30T21:57:10.548067: step 9, loss 1.96039, acc 0.546875, precision 0.166667, recall 0.176471 f1 0.171429\n",
            "2022-11-30T21:57:11.866506: step 10, loss 1.50325, acc 0.640625, precision 0.285714, recall 0.235294 f1 0.258065\n",
            "2022-11-30T21:57:13.154015: step 11, loss 1.59411, acc 0.609375, precision 0.125, recall 0.153846 f1 0.137931\n",
            "2022-11-30T21:57:14.447079: step 12, loss 1.57722, acc 0.640625, precision 0.333333, recall 0.277778 f1 0.30303\n",
            "2022-11-30T21:57:15.768822: step 13, loss 1.29066, acc 0.6875, precision 0.0769231, recall 0.111111 f1 0.0909091\n",
            "2022-11-30T21:57:17.096559: step 14, loss 2.02015, acc 0.59375, precision 0.315789, recall 0.315789 f1 0.315789\n",
            "2022-11-30T21:57:18.418108: step 15, loss 1.82005, acc 0.59375, precision 0.235294, recall 0.235294 f1 0.235294\n",
            "2022-11-30T21:57:19.729710: step 16, loss 1.19283, acc 0.703125, precision 0.294118, recall 0.416667 f1 0.344828\n",
            "2022-11-30T21:57:21.049738: step 17, loss 1.31457, acc 0.65625, precision 0.25, recall 0.285714 f1 0.266667\n",
            "2022-11-30T21:57:22.360425: step 18, loss 1.21302, acc 0.734375, precision 0.533333, recall 0.444444 f1 0.484848\n",
            "2022-11-30T21:57:23.699959: step 19, loss 1.24068, acc 0.71875, precision 0.4375, recall 0.4375 f1 0.4375\n",
            "2022-11-30T21:57:25.032913: step 20, loss 0.957276, acc 0.796875, precision 0.555556, recall 0.357143 f1 0.434783\n",
            "2022-11-30T21:57:26.347716: step 21, loss 1.92429, acc 0.671875, precision 0.4, recall 0.210526 f1 0.275862\n",
            "2022-11-30T21:57:27.741991: step 22, loss 1.51066, acc 0.703125, precision 0.3, recall 0.2 f1 0.24\n",
            "2022-11-30T21:57:29.117395: step 23, loss 1.52732, acc 0.671875, precision 0.466667, recall 0.35 f1 0.4\n",
            "2022-11-30T21:57:30.481465: step 24, loss 1.42909, acc 0.65625, precision 0.0909091, recall 0.0769231 f1 0.0833333\n",
            "2022-11-30T21:57:31.802391: step 25, loss 1.30869, acc 0.6875, precision 0.363636, recall 0.235294 f1 0.285714\n",
            "2022-11-30T21:57:33.136367: step 26, loss 1.35049, acc 0.578125, precision 0.157895, recall 0.214286 f1 0.181818\n",
            "2022-11-30T21:57:34.495893: step 27, loss 1.53287, acc 0.609375, precision 0.333333, recall 0.315789 f1 0.324324\n",
            "2022-11-30T21:57:35.844127: step 28, loss 1.591, acc 0.65625, precision 0.307692, recall 0.235294 f1 0.266667\n",
            "2022-11-30T21:57:37.152260: step 29, loss 1.60988, acc 0.625, precision 0.25, recall 0.5 f1 0.333333\n",
            "2022-11-30T21:57:38.489388: step 30, loss 1.66642, acc 0.65625, precision 0.25, recall 0.416667 f1 0.3125\n",
            "2022-11-30T21:57:39.840875: step 31, loss 1.83392, acc 0.609375, precision 0.222222, recall 0.266667 f1 0.242424\n",
            "2022-11-30T21:57:41.188737: step 32, loss 1.38286, acc 0.640625, precision 0.3, recall 0.157895 f1 0.206897\n",
            "2022-11-30T21:57:42.519344: step 33, loss 1.41086, acc 0.671875, precision 0.388889, recall 0.411765 f1 0.4\n",
            "2022-11-30T21:57:43.842898: step 34, loss 1.59295, acc 0.671875, precision 0.5, recall 0.428571 f1 0.461538\n",
            "2022-11-30T21:57:45.172878: step 35, loss 1.45711, acc 0.65625, precision 0.411765, recall 0.368421 f1 0.388889\n",
            "2022-11-30T21:57:46.509581: step 36, loss 1.26048, acc 0.71875, precision 0.375, recall 0.428571 f1 0.4\n",
            "2022-11-30T21:57:47.879371: step 37, loss 1.1483, acc 0.75, precision 0.25, recall 0.3 f1 0.272727\n",
            "2022-11-30T21:57:49.219083: step 38, loss 1.77872, acc 0.546875, precision 0.35, recall 0.304348 f1 0.325581\n",
            "2022-11-30T21:57:50.565698: step 39, loss 1.78999, acc 0.625, precision 0.294118, recall 0.294118 f1 0.294118\n",
            "2022-11-30T21:57:51.870148: step 40, loss 2.09915, acc 0.65625, precision 0.416667, recall 0.25 f1 0.3125\n",
            "2022-11-30T21:57:53.241432: step 41, loss 1.85808, acc 0.640625, precision 0.333333, recall 0.210526 f1 0.258065\n",
            "2022-11-30T21:57:54.582697: step 42, loss 1.63553, acc 0.671875, precision 0.2, recall 0.25 f1 0.222222\n",
            "2022-11-30T21:57:55.947532: step 43, loss 1.28992, acc 0.65625, precision 0, recall 0 f1 nan\n",
            "2022-11-30T21:57:57.252010: step 44, loss 1.35036, acc 0.671875, precision 0.222222, recall 0.125 f1 0.16\n",
            "2022-11-30T21:57:58.622524: step 45, loss 1.65398, acc 0.625, precision 0.368421, recall 0.368421 f1 0.368421\n",
            "2022-11-30T21:57:59.977906: step 46, loss 1.75981, acc 0.484375, precision 0.148148, recall 0.285714 f1 0.195122\n",
            "2022-11-30T21:58:01.286785: step 47, loss 1.0918, acc 0.734375, precision 0.235294, recall 0.5 f1 0.32\n",
            "2022-11-30T21:58:02.634629: step 48, loss 1.37265, acc 0.703125, precision 0.333333, recall 0.461538 f1 0.387097\n",
            "2022-11-30T21:58:03.994104: step 49, loss 2.00842, acc 0.546875, precision 0.266667, recall 0.181818 f1 0.216216\n",
            "2022-11-30T21:58:05.383536: step 50, loss 1.47023, acc 0.65625, precision 0.214286, recall 0.214286 f1 0.214286\n",
            "2022-11-30T21:58:06.724927: step 51, loss 1.40587, acc 0.765625, precision 0.5, recall 0.2 f1 0.285714\n",
            "2022-11-30T21:58:08.061429: step 52, loss 1.21792, acc 0.71875, precision 0.25, recall 0.25 f1 0.25\n",
            "2022-11-30T21:58:09.437482: step 53, loss 1.69972, acc 0.65625, precision 0.6, recall 0.25 f1 0.352941\n",
            "2022-11-30T21:58:10.805511: step 54, loss 0.851422, acc 0.6875, precision 0.384615, recall 0.294118 f1 0.333333\n",
            "2022-11-30T21:58:12.144437: step 55, loss 1.14122, acc 0.671875, precision 0.2, recall 0.25 f1 0.222222\n",
            "2022-11-30T21:58:13.544898: step 56, loss 1.16645, acc 0.71875, precision 0.461538, recall 0.352941 f1 0.4\n",
            "2022-11-30T21:58:14.899126: step 57, loss 2.28479, acc 0.609375, precision 0.230769, recall 0.166667 f1 0.193548\n",
            "2022-11-30T21:58:16.221417: step 58, loss 1.35885, acc 0.65625, precision 0.538462, recall 0.304348 f1 0.388889\n",
            "2022-11-30T21:58:17.556080: step 59, loss 1.20488, acc 0.71875, precision 0.352941, recall 0.461538 f1 0.4\n",
            "2022-11-30T21:58:18.881629: step 60, loss 0.98841, acc 0.625, precision 0.181818, recall 0.4 f1 0.25\n",
            "2022-11-30T21:58:20.243137: step 61, loss 1.26399, acc 0.578125, precision 0.304348, recall 0.388889 f1 0.341463\n",
            "2022-11-30T21:58:21.939027: step 62, loss 2.08325, acc 0.578125, precision 0.375, recall 0.26087 f1 0.307692\n",
            "2022-11-30T21:58:24.131534: step 63, loss 1.60722, acc 0.59375, precision 0.333333, recall 0.3 f1 0.315789\n",
            "2022-11-30T21:58:26.262831: step 64, loss 1.69166, acc 0.6875, precision 0.181818, recall 0.153846 f1 0.166667\n",
            "2022-11-30T21:58:27.740738: step 65, loss 1.52394, acc 0.609375, precision 0.333333, recall 0.315789 f1 0.324324\n",
            "2022-11-30T21:58:29.105286: step 66, loss 1.09576, acc 0.71875, precision 0.307692, recall 0.307692 f1 0.307692\n",
            "2022-11-30T21:58:30.448887: step 67, loss 1.1392, acc 0.671875, precision 0.285714, recall 0.266667 f1 0.275862\n",
            "2022-11-30T21:58:31.775286: step 68, loss 1.20626, acc 0.671875, precision 0.428571, recall 0.315789 f1 0.363636\n",
            "2022-11-30T21:58:33.095275: step 69, loss 1.14364, acc 0.625, precision 0.133333, recall 0.153846 f1 0.142857\n",
            "2022-11-30T21:58:34.414534: step 70, loss 1.07398, acc 0.734375, precision 0.5, recall 0.411765 f1 0.451613\n",
            "2022-11-30T21:58:35.749627: step 71, loss 1.43967, acc 0.609375, precision 0.333333, recall 0.25 f1 0.285714\n",
            "2022-11-30T21:58:37.091693: step 72, loss 0.880877, acc 0.703125, precision 0.307692, recall 0.285714 f1 0.296296\n",
            "2022-11-30T21:58:38.430805: step 73, loss 1.74423, acc 0.5, precision 0.166667, recall 0.25 f1 0.2\n",
            "2022-11-30T21:58:39.778569: step 74, loss 1.24461, acc 0.703125, precision 0.444444, recall 0.470588 f1 0.457143\n",
            "2022-11-30T21:58:41.141470: step 75, loss 0.92078, acc 0.6875, precision 0.1875, recall 0.3 f1 0.230769\n",
            "2022-11-30T21:58:42.493904: step 76, loss 1.22918, acc 0.6875, precision 0.2, recall 0.272727 f1 0.230769\n",
            "2022-11-30T21:58:43.835010: step 77, loss 0.861294, acc 0.8125, precision 0.428571, recall 0.272727 f1 0.333333\n",
            "2022-11-30T21:58:45.156093: step 78, loss 1.09385, acc 0.703125, precision 0.357143, recall 0.333333 f1 0.344828\n",
            "2022-11-30T21:58:46.483293: step 79, loss 1.35008, acc 0.640625, precision 0.428571, recall 0.285714 f1 0.342857\n",
            "2022-11-30T21:58:47.787840: step 80, loss 1.63837, acc 0.5625, precision 0.285714, recall 0.08 f1 0.125\n",
            "2022-11-30T21:58:49.112261: step 81, loss 0.934253, acc 0.8125, precision 0.625, recall 0.357143 f1 0.454545\n",
            "2022-11-30T21:58:50.460696: step 82, loss 1.49501, acc 0.65625, precision 0.363636, recall 0.210526 f1 0.266667\n",
            "2022-11-30T21:58:51.802437: step 83, loss 1.02678, acc 0.734375, precision 0, recall 0 f1 nan\n",
            "2022-11-30T21:58:53.144661: step 84, loss 0.785385, acc 0.71875, precision 0.214286, recall 0.3 f1 0.25\n",
            "2022-11-30T21:58:54.468710: step 85, loss 1.72921, acc 0.59375, precision 0.411765, recall 0.304348 f1 0.35\n",
            "2022-11-30T21:58:55.792636: step 86, loss 0.728171, acc 0.703125, precision 0.230769, recall 0.25 f1 0.24\n",
            "2022-11-30T21:58:57.082592: step 87, loss 1.55104, acc 0.625, precision 0.318182, recall 0.4375 f1 0.368421\n",
            "2022-11-30T21:58:58.430996: step 88, loss 1.18266, acc 0.640625, precision 0.363636, recall 0.470588 f1 0.410256\n",
            "2022-11-30T21:58:59.809820: step 89, loss 1.18342, acc 0.65625, precision 0.368421, recall 0.411765 f1 0.388889\n",
            "2022-11-30T21:59:01.147701: step 90, loss 1.47245, acc 0.640625, precision 0.222222, recall 0.307692 f1 0.258065\n",
            "2022-11-30T21:59:02.463541: step 91, loss 1.09754, acc 0.640625, precision 0.388889, recall 0.368421 f1 0.378378\n",
            "2022-11-30T21:59:03.805277: step 92, loss 1.46014, acc 0.625, precision 0.125, recall 0.166667 f1 0.142857\n",
            "2022-11-30T21:59:05.145182: step 93, loss 1.19923, acc 0.65625, precision 0.352941, recall 0.352941 f1 0.352941\n",
            "2022-11-30T21:59:06.548125: step 94, loss 1.93448, acc 0.671875, precision 0.545455, recall 0.272727 f1 0.363636\n",
            "2022-11-30T21:59:07.881977: step 95, loss 1.15856, acc 0.71875, precision 0.428571, recall 0.375 f1 0.4\n",
            "2022-11-30T21:59:09.193828: step 96, loss 0.996072, acc 0.78125, precision 0.444444, recall 0.307692 f1 0.363636\n",
            "2022-11-30T21:59:10.530646: step 97, loss 0.641106, acc 0.796875, precision 0.533333, recall 0.571429 f1 0.551724\n",
            "2022-11-30T21:59:11.869565: step 98, loss 1.33089, acc 0.71875, precision 0.538462, recall 0.368421 f1 0.4375\n",
            "2022-11-30T21:59:13.213685: step 99, loss 1.49942, acc 0.703125, precision 0.692308, recall 0.375 f1 0.486486\n",
            "2022-11-30T21:59:14.542514: step 100, loss 0.725029, acc 0.765625, precision 0.461538, recall 0.428571 f1 0.444444\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T21:59:18.706086: step 100, loss 0.645797, acc 0.786395, precision 0.807018, recall 0.239583 f1 0.369478\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-100\n",
            "\n",
            "2022-11-30T21:59:20.172083: step 101, loss 0.780237, acc 0.75, precision 0.285714, recall 0.4 f1 0.333333\n",
            "2022-11-30T21:59:21.494553: step 102, loss 0.989201, acc 0.65625, precision 0.333333, recall 0.294118 f1 0.3125\n",
            "2022-11-30T21:59:22.814064: step 103, loss 0.941588, acc 0.75, precision 0.470588, recall 0.533333 f1 0.5\n",
            "2022-11-30T21:59:23.476395: step 104, loss 1.01246, acc 0.592593, precision 0.285714, recall 0.25 f1 0.266667\n",
            "2022-11-30T21:59:24.792712: step 105, loss 0.784809, acc 0.8125, precision 0.6875, recall 0.611111 f1 0.647059\n",
            "2022-11-30T21:59:26.152802: step 106, loss 0.825975, acc 0.734375, precision 0.55, recall 0.578947 f1 0.564103\n",
            "2022-11-30T21:59:27.483008: step 107, loss 0.631193, acc 0.765625, precision 0.5, recall 0.533333 f1 0.516129\n",
            "2022-11-30T21:59:28.823040: step 108, loss 0.951161, acc 0.6875, precision 0.454545, recall 0.555556 f1 0.5\n",
            "2022-11-30T21:59:30.190884: step 109, loss 1.01364, acc 0.640625, precision 0.333333, recall 0.4375 f1 0.378378\n",
            "2022-11-30T21:59:31.516792: step 110, loss 0.628857, acc 0.765625, precision 0.588235, recall 0.555556 f1 0.571429\n",
            "2022-11-30T21:59:32.855449: step 111, loss 0.923025, acc 0.75, precision 0.428571, recall 0.428571 f1 0.428571\n",
            "2022-11-30T21:59:34.181697: step 112, loss 1.11993, acc 0.71875, precision 0.4, recall 0.4 f1 0.4\n",
            "2022-11-30T21:59:35.485784: step 113, loss 0.734771, acc 0.765625, precision 0.444444, recall 0.285714 f1 0.347826\n",
            "2022-11-30T21:59:36.803142: step 114, loss 1.22963, acc 0.71875, precision 0.375, recall 0.1875 f1 0.25\n",
            "2022-11-30T21:59:38.132358: step 115, loss 0.775559, acc 0.78125, precision 0.363636, recall 0.363636 f1 0.363636\n",
            "2022-11-30T21:59:39.442001: step 116, loss 0.713558, acc 0.8125, precision 0.692308, recall 0.529412 f1 0.6\n",
            "2022-11-30T21:59:40.780836: step 117, loss 1.26524, acc 0.65625, precision 0.363636, recall 0.210526 f1 0.266667\n",
            "2022-11-30T21:59:42.115678: step 118, loss 0.894237, acc 0.75, precision 0.461538, recall 0.4 f1 0.428571\n",
            "2022-11-30T21:59:43.440321: step 119, loss 0.858196, acc 0.765625, precision 0.538462, recall 0.4375 f1 0.482759\n",
            "2022-11-30T21:59:44.794901: step 120, loss 0.770409, acc 0.734375, precision 0.357143, recall 0.384615 f1 0.37037\n",
            "2022-11-30T21:59:46.125364: step 121, loss 0.546901, acc 0.84375, precision 0.833333, recall 0.555556 f1 0.666667\n",
            "2022-11-30T21:59:47.474756: step 122, loss 0.7682, acc 0.796875, precision 0.583333, recall 0.466667 f1 0.518519\n",
            "2022-11-30T21:59:48.818922: step 123, loss 1.23728, acc 0.671875, precision 0.409091, recall 0.529412 f1 0.461538\n",
            "2022-11-30T21:59:50.175921: step 124, loss 0.775125, acc 0.78125, precision 0.52381, recall 0.733333 f1 0.611111\n",
            "2022-11-30T21:59:51.518212: step 125, loss 0.483051, acc 0.828125, precision 0.625, recall 0.666667 f1 0.645161\n",
            "2022-11-30T21:59:52.894112: step 126, loss 0.660781, acc 0.796875, precision 0.5, recall 0.615385 f1 0.551724\n",
            "2022-11-30T21:59:54.246456: step 127, loss 0.677247, acc 0.8125, precision 0.461538, recall 0.545455 f1 0.5\n",
            "2022-11-30T21:59:55.592468: step 128, loss 1.31923, acc 0.578125, precision 0.5, recall 0.37037 f1 0.425532\n",
            "2022-11-30T21:59:56.926902: step 129, loss 0.796655, acc 0.734375, precision 0.526316, recall 0.555556 f1 0.540541\n",
            "2022-11-30T21:59:58.297191: step 130, loss 0.789126, acc 0.671875, precision 0.214286, recall 0.230769 f1 0.222222\n",
            "2022-11-30T21:59:59.682872: step 131, loss 0.840995, acc 0.75, precision 0.5, recall 0.5 f1 0.5\n",
            "2022-11-30T22:00:01.031717: step 132, loss 0.590925, acc 0.8125, precision 0.5, recall 0.666667 f1 0.571429\n",
            "2022-11-30T22:00:02.360091: step 133, loss 0.7189, acc 0.71875, precision 0.428571, recall 0.375 f1 0.4\n",
            "2022-11-30T22:00:03.663258: step 134, loss 0.939789, acc 0.6875, precision 0.444444, recall 0.444444 f1 0.444444\n",
            "2022-11-30T22:00:05.001906: step 135, loss 0.713406, acc 0.78125, precision 0.375, recall 0.6 f1 0.461538\n",
            "2022-11-30T22:00:06.311568: step 136, loss 0.933408, acc 0.734375, precision 0.555556, recall 0.526316 f1 0.540541\n",
            "2022-11-30T22:00:07.701025: step 137, loss 0.797349, acc 0.75, precision 0.5, recall 0.375 f1 0.428571\n",
            "2022-11-30T22:00:09.036215: step 138, loss 0.582537, acc 0.828125, precision 0.6, recall 0.461538 f1 0.521739\n",
            "2022-11-30T22:00:10.394103: step 139, loss 0.574689, acc 0.765625, precision 0.545455, recall 0.375 f1 0.444444\n",
            "2022-11-30T22:00:11.747530: step 140, loss 0.553166, acc 0.828125, precision 0.444444, recall 0.4 f1 0.421053\n",
            "2022-11-30T22:00:13.109207: step 141, loss 1.14458, acc 0.71875, precision 0.333333, recall 0.2 f1 0.25\n",
            "2022-11-30T22:00:14.453823: step 142, loss 0.975307, acc 0.765625, precision 0.571429, recall 0.25 f1 0.347826\n",
            "2022-11-30T22:00:15.798017: step 143, loss 0.471989, acc 0.890625, precision 0.8125, recall 0.764706 f1 0.787879\n",
            "2022-11-30T22:00:17.142345: step 144, loss 1.16497, acc 0.65625, precision 0.428571, recall 0.3 f1 0.352941\n",
            "2022-11-30T22:00:18.503292: step 145, loss 0.574895, acc 0.78125, precision 0.583333, recall 0.4375 f1 0.5\n",
            "2022-11-30T22:00:19.841941: step 146, loss 0.711736, acc 0.75, precision 0.615385, recall 0.421053 f1 0.5\n",
            "2022-11-30T22:00:21.179881: step 147, loss 1.01989, acc 0.640625, precision 0.210526, recall 0.333333 f1 0.258065\n",
            "2022-11-30T22:00:22.523956: step 148, loss 0.586023, acc 0.828125, precision 0.75, recall 0.529412 f1 0.62069\n",
            "2022-11-30T22:00:23.882380: step 149, loss 0.742385, acc 0.71875, precision 0.45, recall 0.5625 f1 0.5\n",
            "2022-11-30T22:00:25.213022: step 150, loss 1.0367, acc 0.6875, precision 0.631579, recall 0.48 f1 0.545455\n",
            "2022-11-30T22:00:26.545983: step 151, loss 1.196, acc 0.671875, precision 0.363636, recall 0.533333 f1 0.432432\n",
            "2022-11-30T22:00:27.877538: step 152, loss 0.807453, acc 0.703125, precision 0.4, recall 0.714286 f1 0.512821\n",
            "2022-11-30T22:00:29.219177: step 153, loss 1.25944, acc 0.625, precision 0.266667, recall 0.235294 f1 0.25\n",
            "2022-11-30T22:00:30.574506: step 154, loss 0.740186, acc 0.75, precision 0.388889, recall 0.583333 f1 0.466667\n",
            "2022-11-30T22:00:31.894913: step 155, loss 0.746882, acc 0.6875, precision 0.434783, recall 0.588235 f1 0.5\n",
            "2022-11-30T22:00:33.226997: step 156, loss 0.626008, acc 0.78125, precision 0.666667, recall 0.526316 f1 0.588235\n",
            "2022-11-30T22:00:34.562171: step 157, loss 0.449308, acc 0.84375, precision 0.6, recall 0.5 f1 0.545455\n",
            "2022-11-30T22:00:35.905369: step 158, loss 0.697326, acc 0.828125, precision 0.8, recall 0.285714 f1 0.421053\n",
            "2022-11-30T22:00:37.225647: step 159, loss 1.09113, acc 0.71875, precision 0.75, recall 0.272727 f1 0.4\n",
            "2022-11-30T22:00:38.573219: step 160, loss 0.828989, acc 0.78125, precision 0.583333, recall 0.4375 f1 0.5\n",
            "2022-11-30T22:00:39.891373: step 161, loss 1.05583, acc 0.71875, precision 0.444444, recall 0.235294 f1 0.307692\n",
            "2022-11-30T22:00:41.225592: step 162, loss 0.755718, acc 0.75, precision 0.5, recall 0.4375 f1 0.466667\n",
            "2022-11-30T22:00:42.557886: step 163, loss 0.569448, acc 0.8125, precision 0.736842, recall 0.666667 f1 0.7\n",
            "2022-11-30T22:00:43.903789: step 164, loss 0.627009, acc 0.765625, precision 0.416667, recall 0.384615 f1 0.4\n",
            "2022-11-30T22:00:45.236842: step 165, loss 0.992211, acc 0.640625, precision 0.347826, recall 0.5 f1 0.410256\n",
            "2022-11-30T22:00:46.581536: step 166, loss 1.05557, acc 0.65625, precision 0.36, recall 0.6 f1 0.45\n",
            "2022-11-30T22:00:47.919839: step 167, loss 0.789954, acc 0.8125, precision 0.666667, recall 0.588235 f1 0.625\n",
            "2022-11-30T22:00:49.238876: step 168, loss 1.08875, acc 0.703125, precision 0.526316, recall 0.5 f1 0.512821\n",
            "2022-11-30T22:00:50.582444: step 169, loss 0.525972, acc 0.765625, precision 0.363636, recall 0.333333 f1 0.347826\n",
            "2022-11-30T22:00:51.909726: step 170, loss 0.799477, acc 0.78125, precision 0.583333, recall 0.4375 f1 0.5\n",
            "2022-11-30T22:00:53.258424: step 171, loss 0.731515, acc 0.71875, precision 0.6, recall 0.428571 f1 0.5\n",
            "2022-11-30T22:00:54.556970: step 172, loss 0.70199, acc 0.78125, precision 0.428571, recall 0.5 f1 0.461538\n",
            "2022-11-30T22:00:55.904250: step 173, loss 0.695149, acc 0.78125, precision 0.625, recall 0.555556 f1 0.588235\n",
            "2022-11-30T22:00:57.226702: step 174, loss 0.834806, acc 0.71875, precision 0.571429, recall 0.4 f1 0.470588\n",
            "2022-11-30T22:00:58.557124: step 175, loss 0.755528, acc 0.8125, precision 0.583333, recall 0.5 f1 0.538462\n",
            "2022-11-30T22:00:59.904411: step 176, loss 0.629638, acc 0.765625, precision 0.466667, recall 0.5 f1 0.482759\n",
            "2022-11-30T22:01:01.214655: step 177, loss 0.839077, acc 0.75, precision 0.5, recall 0.375 f1 0.428571\n",
            "2022-11-30T22:01:02.541418: step 178, loss 0.764806, acc 0.8125, precision 0.533333, recall 0.615385 f1 0.571429\n",
            "2022-11-30T22:01:03.918328: step 179, loss 0.856142, acc 0.78125, precision 0.444444, recall 0.307692 f1 0.363636\n",
            "2022-11-30T22:01:05.260339: step 180, loss 0.72222, acc 0.765625, precision 0.583333, recall 0.411765 f1 0.482759\n",
            "2022-11-30T22:01:06.608210: step 181, loss 0.50895, acc 0.765625, precision 0.375, recall 0.545455 f1 0.444444\n",
            "2022-11-30T22:01:07.956263: step 182, loss 0.5075, acc 0.796875, precision 0.619048, recall 0.722222 f1 0.666667\n",
            "2022-11-30T22:01:09.318088: step 183, loss 0.656858, acc 0.765625, precision 0.611111, recall 0.578947 f1 0.594595\n",
            "2022-11-30T22:01:10.659313: step 184, loss 0.559399, acc 0.78125, precision 0.428571, recall 0.5 f1 0.461538\n",
            "2022-11-30T22:01:12.016775: step 185, loss 0.895764, acc 0.765625, precision 0.375, recall 0.545455 f1 0.444444\n",
            "2022-11-30T22:01:13.387022: step 186, loss 0.434193, acc 0.84375, precision 0.647059, recall 0.733333 f1 0.6875\n",
            "2022-11-30T22:01:14.768965: step 187, loss 0.541846, acc 0.796875, precision 0.545455, recall 0.428571 f1 0.48\n",
            "2022-11-30T22:01:16.171479: step 188, loss 1.02579, acc 0.78125, precision 0.75, recall 0.333333 f1 0.461538\n",
            "2022-11-30T22:01:17.514331: step 189, loss 0.515434, acc 0.84375, precision 0.666667, recall 0.461538 f1 0.545455\n",
            "2022-11-30T22:01:18.855534: step 190, loss 0.810564, acc 0.6875, precision 0.4, recall 0.222222 f1 0.285714\n",
            "2022-11-30T22:01:20.191370: step 191, loss 0.776961, acc 0.765625, precision 0.375, recall 0.230769 f1 0.285714\n",
            "2022-11-30T22:01:21.562507: step 192, loss 1.1975, acc 0.65625, precision 0.357143, recall 0.277778 f1 0.3125\n",
            "2022-11-30T22:01:22.930115: step 193, loss 0.998426, acc 0.75, precision 0.555556, recall 0.294118 f1 0.384615\n",
            "2022-11-30T22:01:24.254497: step 194, loss 0.780294, acc 0.75, precision 0.5625, recall 0.5 f1 0.529412\n",
            "2022-11-30T22:01:25.599773: step 195, loss 0.932669, acc 0.6875, precision 0.352941, recall 0.4 f1 0.375\n",
            "2022-11-30T22:01:26.926138: step 196, loss 0.982907, acc 0.640625, precision 0.315789, recall 0.375 f1 0.342857\n",
            "2022-11-30T22:01:28.265041: step 197, loss 1.11334, acc 0.609375, precision 0.391304, recall 0.45 f1 0.418605\n",
            "2022-11-30T22:01:30.378000: step 198, loss 0.996552, acc 0.65625, precision 0.47619, recall 0.47619 f1 0.47619\n",
            "2022-11-30T22:01:32.519633: step 199, loss 0.860175, acc 0.65625, precision 0.32, recall 0.615385 f1 0.421053\n",
            "2022-11-30T22:01:34.434382: step 200, loss 0.601013, acc 0.65625, precision 0.444444, recall 0.631579 f1 0.521739\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:01:38.181677: step 200, loss 0.465584, acc 0.819048, precision 0.824176, recall 0.390625 f1 0.530035\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-200\n",
            "\n",
            "2022-11-30T22:01:39.628689: step 201, loss 0.954681, acc 0.71875, precision 0.45, recall 0.5625 f1 0.5\n",
            "2022-11-30T22:01:40.980446: step 202, loss 0.645218, acc 0.765625, precision 0.538462, recall 0.4375 f1 0.482759\n",
            "2022-11-30T22:01:42.342988: step 203, loss 0.69055, acc 0.765625, precision 0.636364, recall 0.388889 f1 0.482759\n",
            "2022-11-30T22:01:43.677190: step 204, loss 0.886441, acc 0.78125, precision 0.8, recall 0.4 f1 0.533333\n",
            "2022-11-30T22:01:45.010314: step 205, loss 0.680872, acc 0.765625, precision 0.538462, recall 0.4375 f1 0.482759\n",
            "2022-11-30T22:01:46.368749: step 206, loss 0.350736, acc 0.9375, precision 0.857143, recall 0.666667 f1 0.75\n",
            "2022-11-30T22:01:47.668461: step 207, loss 0.628721, acc 0.78125, precision 0.625, recall 0.3125 f1 0.416667\n",
            "2022-11-30T22:01:48.362919: step 208, loss 0.685426, acc 0.777778, precision 1, recall 0.4 f1 0.571429\n",
            "2022-11-30T22:01:49.677244: step 209, loss 0.266812, acc 0.875, precision 0.714286, recall 0.714286 f1 0.714286\n",
            "2022-11-30T22:01:50.988265: step 210, loss 0.372282, acc 0.875, precision 0.625, recall 0.833333 f1 0.714286\n",
            "2022-11-30T22:01:52.347515: step 211, loss 0.444438, acc 0.84375, precision 0.8125, recall 0.65 f1 0.722222\n",
            "2022-11-30T22:01:53.704186: step 212, loss 0.418327, acc 0.828125, precision 0.5, recall 0.727273 f1 0.592593\n",
            "2022-11-30T22:01:55.014096: step 213, loss 0.486251, acc 0.875, precision 0.545455, recall 0.666667 f1 0.6\n",
            "2022-11-30T22:01:56.355775: step 214, loss 0.634656, acc 0.828125, precision 0.615385, recall 0.571429 f1 0.592593\n",
            "2022-11-30T22:01:57.687479: step 215, loss 0.482978, acc 0.78125, precision 0.578947, recall 0.647059 f1 0.611111\n",
            "2022-11-30T22:01:59.031046: step 216, loss 0.565222, acc 0.78125, precision 0.571429, recall 0.266667 f1 0.363636\n",
            "2022-11-30T22:02:00.389692: step 217, loss 0.506462, acc 0.796875, precision 0.3, recall 0.333333 f1 0.315789\n",
            "2022-11-30T22:02:01.741633: step 218, loss 0.569797, acc 0.75, precision 0.545455, recall 0.352941 f1 0.428571\n",
            "2022-11-30T22:02:03.091762: step 219, loss 0.665044, acc 0.78125, precision 0.615385, recall 0.470588 f1 0.533333\n",
            "2022-11-30T22:02:04.495614: step 220, loss 0.236147, acc 0.875, precision 0.692308, recall 0.692308 f1 0.692308\n",
            "2022-11-30T22:02:05.847174: step 221, loss 0.261165, acc 0.921875, precision 0.8, recall 0.727273 f1 0.761905\n",
            "2022-11-30T22:02:07.177439: step 222, loss 0.769059, acc 0.671875, precision 0.388889, recall 0.411765 f1 0.4\n",
            "2022-11-30T22:02:08.529064: step 223, loss 0.728702, acc 0.78125, precision 0.666667, recall 0.526316 f1 0.588235\n",
            "2022-11-30T22:02:09.898047: step 224, loss 0.364199, acc 0.796875, precision 0.5, recall 0.615385 f1 0.551724\n",
            "2022-11-30T22:02:11.250919: step 225, loss 0.639142, acc 0.78125, precision 0.461538, recall 0.461538 f1 0.461538\n",
            "2022-11-30T22:02:12.614085: step 226, loss 0.73851, acc 0.765625, precision 0.65, recall 0.619048 f1 0.634146\n",
            "2022-11-30T22:02:13.957839: step 227, loss 0.461055, acc 0.8125, precision 0.416667, recall 0.5 f1 0.454545\n",
            "2022-11-30T22:02:15.297501: step 228, loss 0.353205, acc 0.875, precision 0.7, recall 0.583333 f1 0.636364\n",
            "2022-11-30T22:02:16.616468: step 229, loss 0.680188, acc 0.75, precision 0.538462, recall 0.411765 f1 0.466667\n",
            "2022-11-30T22:02:17.955749: step 230, loss 0.634355, acc 0.8125, precision 0.75, recall 0.5 f1 0.6\n",
            "2022-11-30T22:02:19.287328: step 231, loss 0.626374, acc 0.8125, precision 0.583333, recall 0.5 f1 0.538462\n",
            "2022-11-30T22:02:20.624621: step 232, loss 0.549164, acc 0.796875, precision 0.363636, recall 0.4 f1 0.380952\n",
            "2022-11-30T22:02:21.981709: step 233, loss 0.366506, acc 0.828125, precision 0.5, recall 0.636364 f1 0.56\n",
            "2022-11-30T22:02:23.344289: step 234, loss 0.379581, acc 0.84375, precision 0.5, recall 0.4 f1 0.444444\n",
            "2022-11-30T22:02:24.671112: step 235, loss 0.571689, acc 0.828125, precision 0.75, recall 0.4 f1 0.521739\n",
            "2022-11-30T22:02:26.003540: step 236, loss 0.4884, acc 0.8125, precision 0.692308, recall 0.529412 f1 0.6\n",
            "2022-11-30T22:02:27.346811: step 237, loss 0.73536, acc 0.765625, precision 0.7, recall 0.368421 f1 0.482759\n",
            "2022-11-30T22:02:28.691739: step 238, loss 0.37261, acc 0.84375, precision 0.545455, recall 0.545455 f1 0.545455\n",
            "2022-11-30T22:02:30.036427: step 239, loss 0.447862, acc 0.828125, precision 0.583333, recall 0.538462 f1 0.56\n",
            "2022-11-30T22:02:31.363513: step 240, loss 0.309847, acc 0.84375, precision 0.625, recall 0.416667 f1 0.5\n",
            "2022-11-30T22:02:32.726749: step 241, loss 0.32255, acc 0.859375, precision 0.666667, recall 0.714286 f1 0.689655\n",
            "2022-11-30T22:02:34.045333: step 242, loss 0.345211, acc 0.859375, precision 0.714286, recall 0.833333 f1 0.769231\n",
            "2022-11-30T22:02:35.471419: step 243, loss 0.400839, acc 0.8125, precision 0.625, recall 0.625 f1 0.625\n",
            "2022-11-30T22:02:36.820710: step 244, loss 0.372697, acc 0.828125, precision 0.714286, recall 0.588235 f1 0.645161\n",
            "2022-11-30T22:02:38.145465: step 245, loss 0.48035, acc 0.78125, precision 0.526316, recall 0.666667 f1 0.588235\n",
            "2022-11-30T22:02:39.488598: step 246, loss 0.688389, acc 0.734375, precision 0.692308, recall 0.409091 f1 0.514286\n",
            "2022-11-30T22:02:40.801259: step 247, loss 0.558217, acc 0.828125, precision 0.75, recall 0.631579 f1 0.685714\n",
            "2022-11-30T22:02:42.122880: step 248, loss 0.6209, acc 0.75, precision 0.5, recall 0.5625 f1 0.529412\n",
            "2022-11-30T22:02:43.461514: step 249, loss 0.514514, acc 0.84375, precision 0.764706, recall 0.684211 f1 0.722222\n",
            "2022-11-30T22:02:44.802544: step 250, loss 0.411031, acc 0.84375, precision 0.769231, recall 0.588235 f1 0.666667\n",
            "2022-11-30T22:02:46.101961: step 251, loss 0.620862, acc 0.734375, precision 0.52381, recall 0.611111 f1 0.564103\n",
            "2022-11-30T22:02:47.434920: step 252, loss 0.37831, acc 0.84375, precision 0.684211, recall 0.764706 f1 0.722222\n",
            "2022-11-30T22:02:48.780902: step 253, loss 0.59006, acc 0.78125, precision 0.588235, recall 0.588235 f1 0.588235\n",
            "2022-11-30T22:02:50.111353: step 254, loss 0.647511, acc 0.828125, precision 0.611111, recall 0.733333 f1 0.666667\n",
            "2022-11-30T22:02:51.431242: step 255, loss 0.458889, acc 0.890625, precision 0.923077, recall 0.666667 f1 0.774194\n",
            "2022-11-30T22:02:52.808565: step 256, loss 0.61359, acc 0.78125, precision 0.7, recall 0.388889 f1 0.5\n",
            "2022-11-30T22:02:54.125953: step 257, loss 0.474076, acc 0.84375, precision 0.625, recall 0.714286 f1 0.666667\n",
            "2022-11-30T22:02:55.454975: step 258, loss 0.616616, acc 0.796875, precision 0.619048, recall 0.722222 f1 0.666667\n",
            "2022-11-30T22:02:56.787016: step 259, loss 0.394048, acc 0.796875, precision 0.375, recall 0.272727 f1 0.315789\n",
            "2022-11-30T22:02:58.101823: step 260, loss 0.611691, acc 0.8125, precision 0.7, recall 0.7 f1 0.7\n",
            "2022-11-30T22:02:59.456738: step 261, loss 0.713795, acc 0.75, precision 0.619048, recall 0.619048 f1 0.619048\n",
            "2022-11-30T22:03:00.786441: step 262, loss 0.523446, acc 0.8125, precision 0.733333, recall 0.578947 f1 0.647059\n",
            "2022-11-30T22:03:02.111702: step 263, loss 0.634792, acc 0.828125, precision 0.8, recall 0.470588 f1 0.592593\n",
            "2022-11-30T22:03:03.462613: step 264, loss 0.560361, acc 0.796875, precision 0.588235, recall 0.625 f1 0.606061\n",
            "2022-11-30T22:03:04.817421: step 265, loss 0.23491, acc 0.953125, precision 0.785714, recall 1 f1 0.88\n",
            "2022-11-30T22:03:06.201651: step 266, loss 0.767417, acc 0.703125, precision 0.5, recall 0.315789 f1 0.387097\n",
            "2022-11-30T22:03:07.522732: step 267, loss 0.551004, acc 0.8125, precision 0.769231, recall 0.526316 f1 0.625\n",
            "2022-11-30T22:03:08.874767: step 268, loss 0.662785, acc 0.75, precision 0.461538, recall 0.4 f1 0.428571\n",
            "2022-11-30T22:03:10.234183: step 269, loss 0.381298, acc 0.84375, precision 0.681818, recall 0.833333 f1 0.75\n",
            "2022-11-30T22:03:11.606753: step 270, loss 0.561697, acc 0.75, precision 0.461538, recall 0.4 f1 0.428571\n",
            "2022-11-30T22:03:12.938328: step 271, loss 0.425068, acc 0.765625, precision 0.6, recall 0.631579 f1 0.615385\n",
            "2022-11-30T22:03:14.285973: step 272, loss 0.575739, acc 0.765625, precision 0.588235, recall 0.555556 f1 0.571429\n",
            "2022-11-30T22:03:15.637128: step 273, loss 0.516471, acc 0.84375, precision 0.764706, recall 0.684211 f1 0.722222\n",
            "2022-11-30T22:03:16.973984: step 274, loss 0.388324, acc 0.828125, precision 0.608696, recall 0.875 f1 0.717949\n",
            "2022-11-30T22:03:18.328908: step 275, loss 0.494754, acc 0.765625, precision 0.55, recall 0.647059 f1 0.594595\n",
            "2022-11-30T22:03:19.655939: step 276, loss 0.36562, acc 0.90625, precision 0.8125, recall 0.8125 f1 0.8125\n",
            "2022-11-30T22:03:20.993883: step 277, loss 0.589074, acc 0.78125, precision 0.727273, recall 0.421053 f1 0.533333\n",
            "2022-11-30T22:03:22.307335: step 278, loss 0.661244, acc 0.765625, precision 0.625, recall 0.526316 f1 0.571429\n",
            "2022-11-30T22:03:23.708169: step 279, loss 0.513354, acc 0.8125, precision 0.647059, recall 0.647059 f1 0.647059\n",
            "2022-11-30T22:03:25.110436: step 280, loss 0.522169, acc 0.84375, precision 0.727273, recall 0.533333 f1 0.615385\n",
            "2022-11-30T22:03:26.469405: step 281, loss 0.564205, acc 0.765625, precision 0.75, recall 0.315789 f1 0.444444\n",
            "2022-11-30T22:03:27.813238: step 282, loss 0.631016, acc 0.796875, precision 0.583333, recall 0.466667 f1 0.518519\n",
            "2022-11-30T22:03:29.144682: step 283, loss 0.572564, acc 0.796875, precision 0.888889, recall 0.4 f1 0.551724\n",
            "2022-11-30T22:03:30.478275: step 284, loss 0.432551, acc 0.84375, precision 0.733333, recall 0.647059 f1 0.6875\n",
            "2022-11-30T22:03:31.807196: step 285, loss 0.606696, acc 0.765625, precision 0.533333, recall 0.5 f1 0.516129\n",
            "2022-11-30T22:03:33.146430: step 286, loss 0.686029, acc 0.75, precision 0.444444, recall 0.571429 f1 0.5\n",
            "2022-11-30T22:03:34.487286: step 287, loss 0.6944, acc 0.6875, precision 0.473684, recall 0.473684 f1 0.473684\n",
            "2022-11-30T22:03:35.875975: step 288, loss 0.622483, acc 0.765625, precision 0.5, recall 0.466667 f1 0.482759\n",
            "2022-11-30T22:03:37.243181: step 289, loss 0.802833, acc 0.71875, precision 0.642857, recall 0.409091 f1 0.5\n",
            "2022-11-30T22:03:38.572093: step 290, loss 0.484513, acc 0.796875, precision 0.615385, recall 0.5 f1 0.551724\n",
            "2022-11-30T22:03:39.892402: step 291, loss 0.427762, acc 0.828125, precision 0.6, recall 0.642857 f1 0.62069\n",
            "2022-11-30T22:03:41.217574: step 292, loss 0.488312, acc 0.796875, precision 0.555556, recall 0.666667 f1 0.606061\n",
            "2022-11-30T22:03:42.563976: step 293, loss 0.425056, acc 0.78125, precision 0.590909, recall 0.722222 f1 0.65\n",
            "2022-11-30T22:03:43.920614: step 294, loss 0.389534, acc 0.84375, precision 0.625, recall 0.714286 f1 0.666667\n",
            "2022-11-30T22:03:45.262846: step 295, loss 0.375625, acc 0.84375, precision 0.636364, recall 0.538462 f1 0.583333\n",
            "2022-11-30T22:03:46.602809: step 296, loss 0.402793, acc 0.8125, precision 0.571429, recall 0.307692 f1 0.4\n",
            "2022-11-30T22:03:47.925115: step 297, loss 0.382346, acc 0.875, precision 0.777778, recall 0.538462 f1 0.636364\n",
            "2022-11-30T22:03:49.275259: step 298, loss 0.719063, acc 0.75, precision 0.5, recall 0.125 f1 0.2\n",
            "2022-11-30T22:03:50.623558: step 299, loss 0.756161, acc 0.734375, precision 0.666667, recall 0.380952 f1 0.484848\n",
            "2022-11-30T22:03:51.959199: step 300, loss 0.398161, acc 0.828125, precision 0.857143, recall 0.375 f1 0.521739\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:03:55.753734: step 300, loss 0.510532, acc 0.817687, precision 0.914286, recall 0.333333 f1 0.48855\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-300\n",
            "\n",
            "2022-11-30T22:03:57.204555: step 301, loss 0.458077, acc 0.859375, precision 0.777778, recall 0.5 f1 0.608696\n",
            "2022-11-30T22:03:58.558648: step 302, loss 0.294988, acc 0.890625, precision 0.722222, recall 0.866667 f1 0.787879\n",
            "2022-11-30T22:03:59.899679: step 303, loss 0.377383, acc 0.828125, precision 0.6, recall 0.8 f1 0.685714\n",
            "2022-11-30T22:04:01.239788: step 304, loss 0.597504, acc 0.765625, precision 0.647059, recall 0.55 f1 0.594595\n",
            "2022-11-30T22:04:02.570260: step 305, loss 0.561246, acc 0.765625, precision 0.541667, recall 0.764706 f1 0.634146\n",
            "2022-11-30T22:04:03.933449: step 306, loss 0.560649, acc 0.84375, precision 0.642857, recall 0.642857 f1 0.642857\n",
            "2022-11-30T22:04:05.243075: step 307, loss 0.577955, acc 0.78125, precision 0.5, recall 0.642857 f1 0.5625\n",
            "2022-11-30T22:04:06.640173: step 308, loss 0.777799, acc 0.71875, precision 0.583333, recall 0.35 f1 0.4375\n",
            "2022-11-30T22:04:07.988822: step 309, loss 0.334094, acc 0.859375, precision 0.769231, recall 0.625 f1 0.689655\n",
            "2022-11-30T22:04:09.310827: step 310, loss 0.511801, acc 0.828125, precision 0.8, recall 0.6 f1 0.685714\n",
            "2022-11-30T22:04:10.631482: step 311, loss 0.608517, acc 0.765625, precision 0.8, recall 0.380952 f1 0.516129\n",
            "2022-11-30T22:04:11.312476: step 312, loss 0.391364, acc 0.888889, precision 1, recall 0.571429 f1 0.727273\n",
            "2022-11-30T22:04:12.670142: step 313, loss 0.273758, acc 0.875, precision 0.666667, recall 0.857143 f1 0.75\n",
            "2022-11-30T22:04:14.039926: step 314, loss 0.483148, acc 0.765625, precision 0.565217, recall 0.722222 f1 0.634146\n",
            "2022-11-30T22:04:15.411231: step 315, loss 0.477351, acc 0.84375, precision 0.708333, recall 0.85 f1 0.772727\n",
            "2022-11-30T22:04:16.762858: step 316, loss 0.465916, acc 0.8125, precision 0.785714, recall 0.55 f1 0.647059\n",
            "2022-11-30T22:04:18.103991: step 317, loss 0.499596, acc 0.859375, precision 0.764706, recall 0.722222 f1 0.742857\n",
            "2022-11-30T22:04:19.455748: step 318, loss 0.411363, acc 0.84375, precision 0.846154, recall 0.578947 f1 0.6875\n",
            "2022-11-30T22:04:20.798393: step 319, loss 0.459007, acc 0.78125, precision 0.5, recall 0.571429 f1 0.533333\n",
            "2022-11-30T22:04:22.153362: step 320, loss 0.44693, acc 0.8125, precision 0.625, recall 0.625 f1 0.625\n",
            "2022-11-30T22:04:23.506360: step 321, loss 0.742697, acc 0.75, precision 0.6, recall 0.473684 f1 0.529412\n",
            "2022-11-30T22:04:24.861643: step 322, loss 0.227519, acc 0.890625, precision 0.75, recall 0.545455 f1 0.631579\n",
            "2022-11-30T22:04:26.200822: step 323, loss 0.302679, acc 0.875, precision 0.85, recall 0.772727 f1 0.809524\n",
            "2022-11-30T22:04:27.528497: step 324, loss 0.456855, acc 0.828125, precision 0.769231, recall 0.555556 f1 0.645161\n",
            "2022-11-30T22:04:28.863934: step 325, loss 0.415679, acc 0.78125, precision 0.636364, recall 0.411765 f1 0.5\n",
            "2022-11-30T22:04:30.196024: step 326, loss 0.513067, acc 0.765625, precision 0.47619, recall 0.714286 f1 0.571429\n",
            "2022-11-30T22:04:31.551399: step 327, loss 0.261652, acc 0.890625, precision 0.727273, recall 0.666667 f1 0.695652\n",
            "2022-11-30T22:04:33.059750: step 328, loss 0.271544, acc 0.890625, precision 0.777778, recall 0.823529 f1 0.8\n",
            "2022-11-30T22:04:35.221724: step 329, loss 0.414569, acc 0.828125, precision 0.705882, recall 0.666667 f1 0.685714\n",
            "2022-11-30T22:04:37.392112: step 330, loss 0.394226, acc 0.875, precision 0.714286, recall 0.714286 f1 0.714286\n",
            "2022-11-30T22:04:39.099381: step 331, loss 0.396421, acc 0.859375, precision 0.722222, recall 0.764706 f1 0.742857\n",
            "2022-11-30T22:04:40.440565: step 332, loss 0.493615, acc 0.8125, precision 0.769231, recall 0.526316 f1 0.625\n",
            "2022-11-30T22:04:41.811176: step 333, loss 0.598862, acc 0.765625, precision 0.4375, recall 0.538462 f1 0.482759\n",
            "2022-11-30T22:04:43.141966: step 334, loss 0.374157, acc 0.84375, precision 0.571429, recall 0.666667 f1 0.615385\n",
            "2022-11-30T22:04:44.474061: step 335, loss 0.313459, acc 0.890625, precision 0.777778, recall 0.583333 f1 0.666667\n",
            "2022-11-30T22:04:45.827413: step 336, loss 0.461247, acc 0.84375, precision 0.666667, recall 0.571429 f1 0.615385\n",
            "2022-11-30T22:04:47.152751: step 337, loss 0.515349, acc 0.8125, precision 0.666667, recall 0.285714 f1 0.4\n",
            "2022-11-30T22:04:48.492185: step 338, loss 0.563073, acc 0.796875, precision 0.666667, recall 0.7 f1 0.682927\n",
            "2022-11-30T22:04:49.835280: step 339, loss 0.501174, acc 0.796875, precision 0.4, recall 0.6 f1 0.48\n",
            "2022-11-30T22:04:51.177095: step 340, loss 0.508626, acc 0.84375, precision 0.583333, recall 0.583333 f1 0.583333\n",
            "2022-11-30T22:04:52.536194: step 341, loss 0.437663, acc 0.859375, precision 1, recall 0.470588 f1 0.64\n",
            "2022-11-30T22:04:53.953827: step 342, loss 0.575672, acc 0.8125, precision 0.636364, recall 0.466667 f1 0.538462\n",
            "2022-11-30T22:04:55.310286: step 343, loss 0.40737, acc 0.875, precision 1, recall 0.5 f1 0.666667\n",
            "2022-11-30T22:04:56.625179: step 344, loss 0.455533, acc 0.84375, precision 0.555556, recall 0.454545 f1 0.5\n",
            "2022-11-30T22:04:57.950909: step 345, loss 0.559708, acc 0.859375, precision 0.857143, recall 0.428571 f1 0.571429\n",
            "2022-11-30T22:04:59.262669: step 346, loss 0.390424, acc 0.84375, precision 0.7, recall 0.5 f1 0.583333\n",
            "2022-11-30T22:05:00.584039: step 347, loss 0.734632, acc 0.75, precision 0.555556, recall 0.555556 f1 0.555556\n",
            "2022-11-30T22:05:01.908442: step 348, loss 0.505716, acc 0.8125, precision 0.684211, recall 0.684211 f1 0.684211\n",
            "2022-11-30T22:05:03.238300: step 349, loss 0.587123, acc 0.78125, precision 0.555556, recall 0.625 f1 0.588235\n",
            "2022-11-30T22:05:04.593721: step 350, loss 0.407598, acc 0.84375, precision 0.722222, recall 0.722222 f1 0.722222\n",
            "2022-11-30T22:05:05.935475: step 351, loss 0.433279, acc 0.84375, precision 0.6, recall 0.692308 f1 0.642857\n",
            "2022-11-30T22:05:07.271226: step 352, loss 0.303822, acc 0.859375, precision 0.882353, recall 0.681818 f1 0.769231\n",
            "2022-11-30T22:05:08.574674: step 353, loss 0.274902, acc 0.90625, precision 0.764706, recall 0.866667 f1 0.8125\n",
            "2022-11-30T22:05:09.892447: step 354, loss 0.256199, acc 0.90625, precision 0.75, recall 0.75 f1 0.75\n",
            "2022-11-30T22:05:11.231294: step 355, loss 0.47506, acc 0.859375, precision 0.875, recall 0.466667 f1 0.608696\n",
            "2022-11-30T22:05:12.551968: step 356, loss 0.381238, acc 0.890625, precision 0.818182, recall 0.642857 f1 0.72\n",
            "2022-11-30T22:05:13.918040: step 357, loss 0.496214, acc 0.859375, precision 0.833333, recall 0.588235 f1 0.689655\n",
            "2022-11-30T22:05:15.263078: step 358, loss 0.438793, acc 0.890625, precision 0.857143, recall 0.705882 f1 0.774194\n",
            "2022-11-30T22:05:16.585056: step 359, loss 0.419758, acc 0.859375, precision 0.75, recall 0.789474 f1 0.769231\n",
            "2022-11-30T22:05:17.912813: step 360, loss 0.410692, acc 0.765625, precision 0.571429, recall 0.470588 f1 0.516129\n",
            "2022-11-30T22:05:19.225342: step 361, loss 0.241816, acc 0.890625, precision 0.866667, recall 0.722222 f1 0.787879\n",
            "2022-11-30T22:05:20.549274: step 362, loss 0.389884, acc 0.84375, precision 0.833333, recall 0.555556 f1 0.666667\n",
            "2022-11-30T22:05:21.895801: step 363, loss 0.284038, acc 0.875, precision 0.923077, recall 0.631579 f1 0.75\n",
            "2022-11-30T22:05:23.219592: step 364, loss 0.412463, acc 0.84375, precision 0.647059, recall 0.733333 f1 0.6875\n",
            "2022-11-30T22:05:24.525563: step 365, loss 0.555382, acc 0.796875, precision 0.789474, recall 0.625 f1 0.697674\n",
            "2022-11-30T22:05:25.840993: step 366, loss 0.523963, acc 0.765625, precision 0.461538, recall 0.428571 f1 0.444444\n",
            "2022-11-30T22:05:27.174340: step 367, loss 0.496188, acc 0.765625, precision 0.47619, recall 0.714286 f1 0.571429\n",
            "2022-11-30T22:05:28.533525: step 368, loss 0.540378, acc 0.734375, precision 0.695652, recall 0.615385 f1 0.653061\n",
            "2022-11-30T22:05:29.877095: step 369, loss 0.511998, acc 0.796875, precision 0.722222, recall 0.619048 f1 0.666667\n",
            "2022-11-30T22:05:31.195855: step 370, loss 0.537196, acc 0.875, precision 0.65, recall 0.928571 f1 0.764706\n",
            "2022-11-30T22:05:32.538683: step 371, loss 0.308553, acc 0.8125, precision 0.692308, recall 0.529412 f1 0.6\n",
            "2022-11-30T22:05:33.886994: step 372, loss 0.37801, acc 0.828125, precision 0.769231, recall 0.555556 f1 0.645161\n",
            "2022-11-30T22:05:35.218834: step 373, loss 0.294332, acc 0.84375, precision 0.642857, recall 0.642857 f1 0.642857\n",
            "2022-11-30T22:05:36.559547: step 374, loss 0.391253, acc 0.828125, precision 1, recall 0.45 f1 0.62069\n",
            "2022-11-30T22:05:37.945373: step 375, loss 0.318351, acc 0.859375, precision 0.75, recall 0.6 f1 0.666667\n",
            "2022-11-30T22:05:39.300336: step 376, loss 0.332907, acc 0.875, precision 0.75, recall 0.642857 f1 0.692308\n",
            "2022-11-30T22:05:40.606498: step 377, loss 0.440216, acc 0.84375, precision 0.875, recall 0.636364 f1 0.736842\n",
            "2022-11-30T22:05:41.928278: step 378, loss 0.300985, acc 0.84375, precision 0.722222, recall 0.722222 f1 0.722222\n",
            "2022-11-30T22:05:43.236846: step 379, loss 0.349504, acc 0.875, precision 0.75, recall 0.833333 f1 0.789474\n",
            "2022-11-30T22:05:44.550176: step 380, loss 0.363564, acc 0.796875, precision 0.714286, recall 0.681818 f1 0.697674\n",
            "2022-11-30T22:05:45.892923: step 381, loss 0.530493, acc 0.796875, precision 0.666667, recall 0.7 f1 0.682927\n",
            "2022-11-30T22:05:47.191555: step 382, loss 0.242637, acc 0.90625, precision 0.769231, recall 0.769231 f1 0.769231\n",
            "2022-11-30T22:05:48.533685: step 383, loss 0.291474, acc 0.890625, precision 0.6875, recall 0.846154 f1 0.758621\n",
            "2022-11-30T22:05:49.849018: step 384, loss 0.694673, acc 0.609375, precision 0.285714, recall 0.375 f1 0.324324\n",
            "2022-11-30T22:05:51.148287: step 385, loss 0.262247, acc 0.90625, precision 0.842105, recall 0.842105 f1 0.842105\n",
            "2022-11-30T22:05:52.482854: step 386, loss 0.429867, acc 0.84375, precision 0.692308, recall 0.6 f1 0.642857\n",
            "2022-11-30T22:05:53.867446: step 387, loss 0.326196, acc 0.875, precision 0.777778, recall 0.538462 f1 0.636364\n",
            "2022-11-30T22:05:55.253739: step 388, loss 0.337872, acc 0.890625, precision 0.571429, recall 0.5 f1 0.533333\n",
            "2022-11-30T22:05:56.580149: step 389, loss 0.421059, acc 0.921875, precision 1, recall 0.444444 f1 0.615385\n",
            "2022-11-30T22:05:57.891235: step 390, loss 0.5292, acc 0.78125, precision 0.166667, recall 0.1 f1 0.125\n",
            "2022-11-30T22:05:59.218981: step 391, loss 0.400901, acc 0.796875, precision 0.846154, recall 0.5 f1 0.628571\n",
            "2022-11-30T22:06:00.548723: step 392, loss 0.46916, acc 0.84375, precision 0.75, recall 0.25 f1 0.375\n",
            "2022-11-30T22:06:01.860797: step 393, loss 0.307947, acc 0.890625, precision 0.777778, recall 0.823529 f1 0.8\n",
            "2022-11-30T22:06:03.193345: step 394, loss 0.318328, acc 0.84375, precision 0.9, recall 0.5 f1 0.642857\n",
            "2022-11-30T22:06:04.522168: step 395, loss 0.564998, acc 0.828125, precision 0.833333, recall 0.526316 f1 0.645161\n",
            "2022-11-30T22:06:05.853904: step 396, loss 0.598532, acc 0.828125, precision 0.666667, recall 0.428571 f1 0.521739\n",
            "2022-11-30T22:06:07.198706: step 397, loss 0.315887, acc 0.90625, precision 0.8, recall 0.666667 f1 0.727273\n",
            "2022-11-30T22:06:08.561674: step 398, loss 0.386019, acc 0.828125, precision 0.666667, recall 0.777778 f1 0.717949\n",
            "2022-11-30T22:06:09.878258: step 399, loss 0.481283, acc 0.828125, precision 0.68, recall 0.85 f1 0.755556\n",
            "2022-11-30T22:06:11.202206: step 400, loss 0.416378, acc 0.859375, precision 0.5625, recall 0.818182 f1 0.666667\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:06:14.934275: step 400, loss 0.397862, acc 0.834014, precision 0.777778, recall 0.510417 f1 0.616352\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-400\n",
            "\n",
            "2022-11-30T22:06:16.432983: step 401, loss 0.463754, acc 0.734375, precision 0.5, recall 0.764706 f1 0.604651\n",
            "2022-11-30T22:06:17.737443: step 402, loss 0.40795, acc 0.796875, precision 0.538462, recall 0.5 f1 0.518519\n",
            "2022-11-30T22:06:19.067816: step 403, loss 0.364616, acc 0.84375, precision 0.636364, recall 0.538462 f1 0.583333\n",
            "2022-11-30T22:06:20.418806: step 404, loss 0.460115, acc 0.828125, precision 0.888889, recall 0.444444 f1 0.592593\n",
            "2022-11-30T22:06:21.720393: step 405, loss 0.191457, acc 0.9375, precision 1, recall 0.733333 f1 0.846154\n",
            "2022-11-30T22:06:23.040735: step 406, loss 0.416322, acc 0.84375, precision 0.875, recall 0.4375 f1 0.583333\n",
            "2022-11-30T22:06:24.405144: step 407, loss 0.194677, acc 0.953125, precision 0.888889, recall 0.8 f1 0.842105\n",
            "2022-11-30T22:06:25.748538: step 408, loss 0.275951, acc 0.875, precision 0.444444, recall 0.571429 f1 0.5\n",
            "2022-11-30T22:06:27.072744: step 409, loss 0.217265, acc 0.9375, precision 0.818182, recall 0.818182 f1 0.818182\n",
            "2022-11-30T22:06:28.386797: step 410, loss 0.253154, acc 0.890625, precision 0.7, recall 0.636364 f1 0.666667\n",
            "2022-11-30T22:06:29.698495: step 411, loss 0.335941, acc 0.859375, precision 0.833333, recall 0.384615 f1 0.526316\n",
            "2022-11-30T22:06:30.992891: step 412, loss 0.361142, acc 0.859375, precision 0.928571, recall 0.619048 f1 0.742857\n",
            "2022-11-30T22:06:32.290640: step 413, loss 0.382329, acc 0.859375, precision 0.785714, recall 0.647059 f1 0.709677\n",
            "2022-11-30T22:06:33.589140: step 414, loss 0.479925, acc 0.828125, precision 0.833333, recall 0.652174 f1 0.731707\n",
            "2022-11-30T22:06:34.912324: step 415, loss 0.526177, acc 0.75, precision 0.619048, recall 0.619048 f1 0.619048\n",
            "2022-11-30T22:06:35.582800: step 416, loss 0.290816, acc 0.888889, precision 0.666667, recall 0.5 f1 0.571429\n",
            "2022-11-30T22:06:36.900639: step 417, loss 0.201385, acc 0.921875, precision 0.833333, recall 0.882353 f1 0.857143\n",
            "2022-11-30T22:06:38.214638: step 418, loss 0.406148, acc 0.859375, precision 0.647059, recall 0.785714 f1 0.709677\n",
            "2022-11-30T22:06:39.572217: step 419, loss 0.40347, acc 0.84375, precision 0.615385, recall 0.615385 f1 0.615385\n",
            "2022-11-30T22:06:40.889090: step 420, loss 0.357645, acc 0.8125, precision 0.416667, recall 0.5 f1 0.454545\n",
            "2022-11-30T22:06:42.191290: step 421, loss 0.384606, acc 0.859375, precision 0.875, recall 0.666667 f1 0.756757\n",
            "2022-11-30T22:06:43.493183: step 422, loss 0.320976, acc 0.859375, precision 0.647059, recall 0.785714 f1 0.709677\n",
            "2022-11-30T22:06:44.783923: step 423, loss 0.348253, acc 0.875, precision 0.8, recall 0.8 f1 0.8\n",
            "2022-11-30T22:06:46.085300: step 424, loss 0.213868, acc 0.921875, precision 0.888889, recall 0.842105 f1 0.864865\n",
            "2022-11-30T22:06:47.409006: step 425, loss 0.269648, acc 0.875, precision 0.75, recall 0.642857 f1 0.692308\n",
            "2022-11-30T22:06:48.712640: step 426, loss 0.334008, acc 0.84375, precision 0.6, recall 0.272727 f1 0.375\n",
            "2022-11-30T22:06:49.997843: step 427, loss 0.309945, acc 0.90625, precision 1, recall 0.5 f1 0.666667\n",
            "2022-11-30T22:06:51.316637: step 428, loss 0.251265, acc 0.875, precision 0.75, recall 0.5 f1 0.6\n",
            "2022-11-30T22:06:52.613237: step 429, loss 0.216487, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:06:53.945367: step 430, loss 0.306077, acc 0.890625, precision 0.833333, recall 0.666667 f1 0.740741\n",
            "2022-11-30T22:06:55.244184: step 431, loss 0.156734, acc 0.921875, precision 0.846154, recall 0.785714 f1 0.814815\n",
            "2022-11-30T22:06:56.542849: step 432, loss 0.295022, acc 0.859375, precision 0.642857, recall 0.692308 f1 0.666667\n",
            "2022-11-30T22:06:57.860151: step 433, loss 0.349109, acc 0.875, precision 0.789474, recall 0.789474 f1 0.789474\n",
            "2022-11-30T22:06:59.139229: step 434, loss 0.257959, acc 0.890625, precision 0.923077, recall 0.666667 f1 0.774194\n",
            "2022-11-30T22:07:00.448585: step 435, loss 0.502569, acc 0.765625, precision 0.636364, recall 0.388889 f1 0.482759\n",
            "2022-11-30T22:07:01.761405: step 436, loss 0.246501, acc 0.890625, precision 0.75, recall 0.8 f1 0.774194\n",
            "2022-11-30T22:07:03.070850: step 437, loss 0.442472, acc 0.8125, precision 0.611111, recall 0.6875 f1 0.647059\n",
            "2022-11-30T22:07:04.338089: step 438, loss 0.171983, acc 0.953125, precision 0.941176, recall 0.888889 f1 0.914286\n",
            "2022-11-30T22:07:05.631847: step 439, loss 0.319977, acc 0.921875, precision 0.777778, recall 0.7 f1 0.736842\n",
            "2022-11-30T22:07:06.930498: step 440, loss 0.351625, acc 0.859375, precision 0.857143, recall 0.631579 f1 0.727273\n",
            "2022-11-30T22:07:08.249497: step 441, loss 0.370632, acc 0.84375, precision 0.538462, recall 0.636364 f1 0.583333\n",
            "2022-11-30T22:07:09.603777: step 442, loss 0.261496, acc 0.890625, precision 0.823529, recall 0.777778 f1 0.8\n",
            "2022-11-30T22:07:10.933705: step 443, loss 0.530112, acc 0.796875, precision 0.866667, recall 0.541667 f1 0.666667\n",
            "2022-11-30T22:07:12.209639: step 444, loss 0.39205, acc 0.890625, precision 0.692308, recall 0.75 f1 0.72\n",
            "2022-11-30T22:07:13.488660: step 445, loss 0.334355, acc 0.859375, precision 0.818182, recall 0.5625 f1 0.666667\n",
            "2022-11-30T22:07:14.791787: step 446, loss 0.203643, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:07:16.105053: step 447, loss 0.669658, acc 0.765625, precision 0.8, recall 0.380952 f1 0.516129\n",
            "2022-11-30T22:07:17.403250: step 448, loss 0.392602, acc 0.84375, precision 0.714286, recall 0.625 f1 0.666667\n",
            "2022-11-30T22:07:18.697217: step 449, loss 0.242377, acc 0.921875, precision 0.833333, recall 0.882353 f1 0.857143\n",
            "2022-11-30T22:07:19.993940: step 450, loss 0.382151, acc 0.84375, precision 0.705882, recall 0.705882 f1 0.705882\n",
            "2022-11-30T22:07:21.284406: step 451, loss 0.231093, acc 0.90625, precision 0.85, recall 0.85 f1 0.85\n",
            "2022-11-30T22:07:22.603553: step 452, loss 0.517447, acc 0.796875, precision 0.6, recall 0.4 f1 0.48\n",
            "2022-11-30T22:07:23.908092: step 453, loss 0.272827, acc 0.875, precision 0.714286, recall 0.714286 f1 0.714286\n",
            "2022-11-30T22:07:25.192867: step 454, loss 0.301418, acc 0.875, precision 0.777778, recall 0.777778 f1 0.777778\n",
            "2022-11-30T22:07:26.483686: step 455, loss 0.291774, acc 0.90625, precision 0.75, recall 0.75 f1 0.75\n",
            "2022-11-30T22:07:27.813661: step 456, loss 0.395476, acc 0.859375, precision 0.625, recall 0.454545 f1 0.526316\n",
            "2022-11-30T22:07:29.115560: step 457, loss 0.434567, acc 0.859375, precision 0.866667, recall 0.65 f1 0.742857\n",
            "2022-11-30T22:07:30.428459: step 458, loss 0.254366, acc 0.890625, precision 0.714286, recall 0.769231 f1 0.740741\n",
            "2022-11-30T22:07:31.702764: step 459, loss 0.442554, acc 0.875, precision 0.764706, recall 0.764706 f1 0.764706\n",
            "2022-11-30T22:07:32.998220: step 460, loss 0.335338, acc 0.875, precision 0.875, recall 0.5 f1 0.636364\n",
            "2022-11-30T22:07:34.297400: step 461, loss 0.424722, acc 0.828125, precision 0.615385, recall 0.571429 f1 0.592593\n",
            "2022-11-30T22:07:35.776616: step 462, loss 0.31975, acc 0.84375, precision 0.909091, recall 0.526316 f1 0.666667\n",
            "2022-11-30T22:07:38.008946: step 463, loss 0.341452, acc 0.890625, precision 0.875, recall 0.736842 f1 0.8\n",
            "2022-11-30T22:07:39.981109: step 464, loss 0.210462, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:07:41.754917: step 465, loss 0.308612, acc 0.90625, precision 0.9, recall 0.642857 f1 0.75\n",
            "2022-11-30T22:07:43.040238: step 466, loss 0.174269, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:07:44.317257: step 467, loss 0.229178, acc 0.9375, precision 1, recall 0.764706 f1 0.866667\n",
            "2022-11-30T22:07:45.631438: step 468, loss 0.338702, acc 0.875, precision 0.764706, recall 0.764706 f1 0.764706\n",
            "2022-11-30T22:07:46.929670: step 469, loss 0.21589, acc 0.890625, precision 0.7, recall 0.933333 f1 0.8\n",
            "2022-11-30T22:07:48.246480: step 470, loss 0.106323, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:07:49.558380: step 471, loss 0.253031, acc 0.875, precision 0.928571, recall 0.65 f1 0.764706\n",
            "2022-11-30T22:07:50.876788: step 472, loss 0.27581, acc 0.890625, precision 0.916667, recall 0.647059 f1 0.758621\n",
            "2022-11-30T22:07:52.185221: step 473, loss 0.335094, acc 0.859375, precision 0.615385, recall 0.666667 f1 0.64\n",
            "2022-11-30T22:07:53.549334: step 474, loss 0.49706, acc 0.8125, precision 0.8125, recall 0.590909 f1 0.684211\n",
            "2022-11-30T22:07:54.857983: step 475, loss 0.20077, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:07:56.157295: step 476, loss 0.320342, acc 0.84375, precision 0.75, recall 0.5625 f1 0.642857\n",
            "2022-11-30T22:07:57.461778: step 477, loss 0.300451, acc 0.84375, precision 0.714286, recall 0.625 f1 0.666667\n",
            "2022-11-30T22:07:58.784202: step 478, loss 0.359521, acc 0.84375, precision 0.857143, recall 0.6 f1 0.705882\n",
            "2022-11-30T22:08:00.086277: step 479, loss 0.232703, acc 0.875, precision 0.705882, recall 0.8 f1 0.75\n",
            "2022-11-30T22:08:01.364092: step 480, loss 0.292495, acc 0.84375, precision 0.615385, recall 0.615385 f1 0.615385\n",
            "2022-11-30T22:08:02.644459: step 481, loss 0.380113, acc 0.859375, precision 0.75, recall 0.6 f1 0.666667\n",
            "2022-11-30T22:08:03.956089: step 482, loss 0.39704, acc 0.8125, precision 0.692308, recall 0.529412 f1 0.6\n",
            "2022-11-30T22:08:05.274177: step 483, loss 0.505819, acc 0.828125, precision 0.6, recall 0.642857 f1 0.62069\n",
            "2022-11-30T22:08:06.590004: step 484, loss 0.266401, acc 0.890625, precision 0.764706, recall 0.8125 f1 0.787879\n",
            "2022-11-30T22:08:07.888086: step 485, loss 0.297101, acc 0.875, precision 0.7, recall 0.875 f1 0.777778\n",
            "2022-11-30T22:08:09.202840: step 486, loss 0.320055, acc 0.890625, precision 0.769231, recall 0.714286 f1 0.740741\n",
            "2022-11-30T22:08:10.552025: step 487, loss 0.226505, acc 0.890625, precision 0.714286, recall 0.769231 f1 0.740741\n",
            "2022-11-30T22:08:11.855039: step 488, loss 0.187409, acc 0.90625, precision 0.857143, recall 0.545455 f1 0.666667\n",
            "2022-11-30T22:08:13.137598: step 489, loss 0.40364, acc 0.859375, precision 0.866667, recall 0.65 f1 0.742857\n",
            "2022-11-30T22:08:14.432052: step 490, loss 0.307723, acc 0.875, precision 1, recall 0.555556 f1 0.714286\n",
            "2022-11-30T22:08:15.728052: step 491, loss 0.392782, acc 0.84375, precision 0.916667, recall 0.55 f1 0.6875\n",
            "2022-11-30T22:08:17.058333: step 492, loss 0.264065, acc 0.859375, precision 0.538462, recall 0.7 f1 0.608696\n",
            "2022-11-30T22:08:18.351497: step 493, loss 0.437949, acc 0.875, precision 0.866667, recall 0.684211 f1 0.764706\n",
            "2022-11-30T22:08:19.689048: step 494, loss 0.214822, acc 0.90625, precision 0.769231, recall 0.769231 f1 0.769231\n",
            "2022-11-30T22:08:20.996372: step 495, loss 0.213797, acc 0.890625, precision 0.818182, recall 0.642857 f1 0.72\n",
            "2022-11-30T22:08:22.279818: step 496, loss 0.361794, acc 0.859375, precision 0.75, recall 0.6 f1 0.666667\n",
            "2022-11-30T22:08:23.568373: step 497, loss 0.329517, acc 0.890625, precision 0.714286, recall 0.769231 f1 0.740741\n",
            "2022-11-30T22:08:24.859521: step 498, loss 0.464096, acc 0.78125, precision 0.636364, recall 0.7 f1 0.666667\n",
            "2022-11-30T22:08:26.169794: step 499, loss 0.261512, acc 0.890625, precision 0.625, recall 0.555556 f1 0.588235\n",
            "2022-11-30T22:08:27.495372: step 500, loss 0.392084, acc 0.875, precision 0.888889, recall 0.727273 f1 0.8\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:08:31.157530: step 500, loss 0.438215, acc 0.842177, precision 0.904255, recall 0.442708 f1 0.594406\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-500\n",
            "\n",
            "2022-11-30T22:08:32.556772: step 501, loss 0.351469, acc 0.796875, precision 0.705882, recall 0.6 f1 0.648649\n",
            "2022-11-30T22:08:33.825730: step 502, loss 0.283784, acc 0.875, precision 0.882353, recall 0.714286 f1 0.789474\n",
            "2022-11-30T22:08:35.108008: step 503, loss 0.322649, acc 0.8125, precision 0.631579, recall 0.705882 f1 0.666667\n",
            "2022-11-30T22:08:36.381763: step 504, loss 0.255584, acc 0.890625, precision 0.818182, recall 0.857143 f1 0.837209\n",
            "2022-11-30T22:08:37.678524: step 505, loss 0.542008, acc 0.8125, precision 0.857143, recall 0.545455 f1 0.666667\n",
            "2022-11-30T22:08:39.009175: step 506, loss 0.283698, acc 0.859375, precision 0.846154, recall 0.611111 f1 0.709677\n",
            "2022-11-30T22:08:40.398123: step 507, loss 0.373397, acc 0.90625, precision 0.866667, recall 0.764706 f1 0.8125\n",
            "2022-11-30T22:08:41.712307: step 508, loss 0.193732, acc 0.90625, precision 0.923077, recall 0.705882 f1 0.8\n",
            "2022-11-30T22:08:42.991788: step 509, loss 0.230863, acc 0.921875, precision 0.846154, recall 0.785714 f1 0.814815\n",
            "2022-11-30T22:08:44.256411: step 510, loss 0.372001, acc 0.84375, precision 0.571429, recall 0.666667 f1 0.615385\n",
            "2022-11-30T22:08:45.550944: step 511, loss 0.251708, acc 0.890625, precision 0.8, recall 0.75 f1 0.774194\n",
            "2022-11-30T22:08:46.849152: step 512, loss 0.47702, acc 0.8125, precision 0.8125, recall 0.590909 f1 0.684211\n",
            "2022-11-30T22:08:48.132230: step 513, loss 0.317145, acc 0.875, precision 0.6, recall 0.6 f1 0.6\n",
            "2022-11-30T22:08:49.453675: step 514, loss 0.286434, acc 0.875, precision 0.823529, recall 0.736842 f1 0.777778\n",
            "2022-11-30T22:08:50.745135: step 515, loss 0.45473, acc 0.828125, precision 0.692308, recall 0.5625 f1 0.62069\n",
            "2022-11-30T22:08:52.036323: step 516, loss 0.439494, acc 0.78125, precision 0.588235, recall 0.588235 f1 0.588235\n",
            "2022-11-30T22:08:53.346121: step 517, loss 0.392635, acc 0.890625, precision 0.733333, recall 0.785714 f1 0.758621\n",
            "2022-11-30T22:08:54.637270: step 518, loss 0.33435, acc 0.875, precision 0.846154, recall 0.647059 f1 0.733333\n",
            "2022-11-30T22:08:55.927523: step 519, loss 0.356433, acc 0.84375, precision 0.846154, recall 0.578947 f1 0.6875\n",
            "2022-11-30T22:08:56.591199: step 520, loss 0.319619, acc 0.888889, precision 1, recall 0.4 f1 0.571429\n",
            "2022-11-30T22:08:57.879264: step 521, loss 0.327032, acc 0.84375, precision 0.692308, recall 0.6 f1 0.642857\n",
            "2022-11-30T22:08:59.176538: step 522, loss 0.298749, acc 0.890625, precision 0.666667, recall 0.727273 f1 0.695652\n",
            "2022-11-30T22:09:00.465256: step 523, loss 0.213614, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:09:01.758943: step 524, loss 0.300388, acc 0.890625, precision 0.857143, recall 0.705882 f1 0.774194\n",
            "2022-11-30T22:09:03.063778: step 525, loss 0.213961, acc 0.90625, precision 0.764706, recall 0.866667 f1 0.8125\n",
            "2022-11-30T22:09:04.332387: step 526, loss 0.265765, acc 0.84375, precision 0.647059, recall 0.733333 f1 0.6875\n",
            "2022-11-30T22:09:05.596267: step 527, loss 0.141208, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:09:06.887177: step 528, loss 0.217788, acc 0.890625, precision 0.769231, recall 0.714286 f1 0.740741\n",
            "2022-11-30T22:09:08.185295: step 529, loss 0.176467, acc 0.96875, precision 0.894737, recall 1 f1 0.944444\n",
            "2022-11-30T22:09:09.463872: step 530, loss 0.181948, acc 0.90625, precision 0.9, recall 0.642857 f1 0.75\n",
            "2022-11-30T22:09:10.827341: step 531, loss 0.312304, acc 0.859375, precision 0.857143, recall 0.428571 f1 0.571429\n",
            "2022-11-30T22:09:12.110887: step 532, loss 0.326698, acc 0.84375, precision 0.866667, recall 0.619048 f1 0.722222\n",
            "2022-11-30T22:09:13.399265: step 533, loss 0.389564, acc 0.84375, precision 0.75, recall 0.75 f1 0.75\n",
            "2022-11-30T22:09:14.676840: step 534, loss 0.227525, acc 0.890625, precision 0.916667, recall 0.647059 f1 0.758621\n",
            "2022-11-30T22:09:15.995062: step 535, loss 0.262472, acc 0.921875, precision 0.846154, recall 0.785714 f1 0.814815\n",
            "2022-11-30T22:09:17.342703: step 536, loss 0.29174, acc 0.875, precision 0.727273, recall 0.615385 f1 0.666667\n",
            "2022-11-30T22:09:18.655432: step 537, loss 0.510418, acc 0.828125, precision 0.692308, recall 0.5625 f1 0.62069\n",
            "2022-11-30T22:09:19.954998: step 538, loss 0.307006, acc 0.890625, precision 0.666667, recall 0.6 f1 0.631579\n",
            "2022-11-30T22:09:21.253528: step 539, loss 0.258679, acc 0.90625, precision 0.8, recall 0.8 f1 0.8\n",
            "2022-11-30T22:09:22.560433: step 540, loss 0.297408, acc 0.859375, precision 0.8, recall 0.533333 f1 0.64\n",
            "2022-11-30T22:09:23.845149: step 541, loss 0.336838, acc 0.890625, precision 0.928571, recall 0.684211 f1 0.787879\n",
            "2022-11-30T22:09:25.155832: step 542, loss 0.223219, acc 0.890625, precision 0.809524, recall 0.85 f1 0.829268\n",
            "2022-11-30T22:09:26.447198: step 543, loss 0.2751, acc 0.90625, precision 0.916667, recall 0.6875 f1 0.785714\n",
            "2022-11-30T22:09:27.761462: step 544, loss 0.280067, acc 0.875, precision 0.769231, recall 0.666667 f1 0.714286\n",
            "2022-11-30T22:09:29.091813: step 545, loss 0.367694, acc 0.84375, precision 0.818182, recall 0.529412 f1 0.642857\n",
            "2022-11-30T22:09:30.405217: step 546, loss 0.323342, acc 0.828125, precision 0.714286, recall 0.75 f1 0.731707\n",
            "2022-11-30T22:09:31.685535: step 547, loss 0.288584, acc 0.921875, precision 0.785714, recall 0.846154 f1 0.814815\n",
            "2022-11-30T22:09:32.980957: step 548, loss 0.294067, acc 0.890625, precision 0.625, recall 0.909091 f1 0.740741\n",
            "2022-11-30T22:09:34.249694: step 549, loss 0.281828, acc 0.890625, precision 0.727273, recall 0.666667 f1 0.695652\n",
            "2022-11-30T22:09:35.539287: step 550, loss 0.556751, acc 0.796875, precision 0.73913, recall 0.708333 f1 0.723404\n",
            "2022-11-30T22:09:36.819208: step 551, loss 0.23382, acc 0.9375, precision 0.846154, recall 0.846154 f1 0.846154\n",
            "2022-11-30T22:09:38.120175: step 552, loss 0.379714, acc 0.890625, precision 0.7, recall 0.636364 f1 0.666667\n",
            "2022-11-30T22:09:39.405603: step 553, loss 0.298827, acc 0.921875, precision 0.9375, recall 0.789474 f1 0.857143\n",
            "2022-11-30T22:09:40.727721: step 554, loss 0.227803, acc 0.90625, precision 0.818182, recall 0.692308 f1 0.75\n",
            "2022-11-30T22:09:42.082749: step 555, loss 0.381065, acc 0.84375, precision 0.8, recall 0.5 f1 0.615385\n",
            "2022-11-30T22:09:43.374487: step 556, loss 0.26447, acc 0.890625, precision 0.888889, recall 0.571429 f1 0.695652\n",
            "2022-11-30T22:09:44.667666: step 557, loss 0.203969, acc 0.90625, precision 0.916667, recall 0.6875 f1 0.785714\n",
            "2022-11-30T22:09:45.953777: step 558, loss 0.209172, acc 0.890625, precision 0.75, recall 0.545455 f1 0.631579\n",
            "2022-11-30T22:09:47.231010: step 559, loss 0.255484, acc 0.875, precision 0.888889, recall 0.727273 f1 0.8\n",
            "2022-11-30T22:09:48.545387: step 560, loss 0.14511, acc 0.9375, precision 0.923077, recall 0.8 f1 0.857143\n",
            "2022-11-30T22:09:49.903290: step 561, loss 0.234468, acc 0.875, precision 0.916667, recall 0.611111 f1 0.733333\n",
            "2022-11-30T22:09:51.211004: step 562, loss 0.33873, acc 0.859375, precision 0.8, recall 0.666667 f1 0.727273\n",
            "2022-11-30T22:09:52.561661: step 563, loss 0.294201, acc 0.953125, precision 0.95, recall 0.904762 f1 0.926829\n",
            "2022-11-30T22:09:53.867866: step 564, loss 0.318911, acc 0.859375, precision 0.529412, recall 0.9 f1 0.666667\n",
            "2022-11-30T22:09:55.130962: step 565, loss 0.195911, acc 0.921875, precision 0.833333, recall 0.882353 f1 0.857143\n",
            "2022-11-30T22:09:56.386933: step 566, loss 0.336556, acc 0.859375, precision 0.666667, recall 0.714286 f1 0.689655\n",
            "2022-11-30T22:09:57.636290: step 567, loss 0.459678, acc 0.828125, precision 0.714286, recall 0.588235 f1 0.645161\n",
            "2022-11-30T22:09:58.891958: step 568, loss 0.250306, acc 0.875, precision 0.875, recall 0.5 f1 0.636364\n",
            "2022-11-30T22:10:00.182299: step 569, loss 0.371047, acc 0.890625, precision 0.777778, recall 0.583333 f1 0.666667\n",
            "2022-11-30T22:10:01.472250: step 570, loss 0.131472, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:10:02.735803: step 571, loss 0.300353, acc 0.890625, precision 0.818182, recall 0.642857 f1 0.72\n",
            "2022-11-30T22:10:04.006463: step 572, loss 0.337276, acc 0.859375, precision 0.818182, recall 0.5625 f1 0.666667\n",
            "2022-11-30T22:10:05.266850: step 573, loss 0.212612, acc 0.921875, precision 0.916667, recall 0.733333 f1 0.814815\n",
            "2022-11-30T22:10:06.529750: step 574, loss 0.332305, acc 0.890625, precision 0.769231, recall 0.714286 f1 0.740741\n",
            "2022-11-30T22:10:07.792404: step 575, loss 0.24038, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:10:09.037424: step 576, loss 0.29219, acc 0.828125, precision 0.533333, recall 0.666667 f1 0.592593\n",
            "2022-11-30T22:10:10.301742: step 577, loss 0.186179, acc 0.875, precision 0.777778, recall 0.538462 f1 0.636364\n",
            "2022-11-30T22:10:11.686141: step 578, loss 0.352823, acc 0.875, precision 0.818182, recall 0.818182 f1 0.818182\n",
            "2022-11-30T22:10:12.947955: step 579, loss 0.183832, acc 0.921875, precision 0.75, recall 0.818182 f1 0.782609\n",
            "2022-11-30T22:10:14.273396: step 580, loss 0.653491, acc 0.8125, precision 0.764706, recall 0.619048 f1 0.684211\n",
            "2022-11-30T22:10:15.537590: step 581, loss 0.286188, acc 0.90625, precision 0.95, recall 0.791667 f1 0.863636\n",
            "2022-11-30T22:10:16.798252: step 582, loss 0.378933, acc 0.875, precision 0.846154, recall 0.647059 f1 0.733333\n",
            "2022-11-30T22:10:18.053055: step 583, loss 0.428861, acc 0.875, precision 0.944444, recall 0.708333 f1 0.809524\n",
            "2022-11-30T22:10:19.339529: step 584, loss 0.253437, acc 0.921875, precision 0.769231, recall 0.833333 f1 0.8\n",
            "2022-11-30T22:10:20.606468: step 585, loss 0.150729, acc 0.953125, precision 0.875, recall 0.777778 f1 0.823529\n",
            "2022-11-30T22:10:21.895059: step 586, loss 0.215195, acc 0.9375, precision 0.823529, recall 0.933333 f1 0.875\n",
            "2022-11-30T22:10:23.185042: step 587, loss 0.26428, acc 0.875, precision 0.714286, recall 0.882353 f1 0.789474\n",
            "2022-11-30T22:10:24.470111: step 588, loss 0.291185, acc 0.859375, precision 0.611111, recall 0.846154 f1 0.709677\n",
            "2022-11-30T22:10:25.733898: step 589, loss 0.259271, acc 0.921875, precision 0.933333, recall 0.777778 f1 0.848485\n",
            "2022-11-30T22:10:27.015012: step 590, loss 0.197181, acc 0.9375, precision 1, recall 0.733333 f1 0.846154\n",
            "2022-11-30T22:10:28.307270: step 591, loss 0.34547, acc 0.84375, precision 0.866667, recall 0.619048 f1 0.722222\n",
            "2022-11-30T22:10:29.646393: step 592, loss 0.231897, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:10:30.985966: step 593, loss 0.368506, acc 0.90625, precision 0.833333, recall 0.714286 f1 0.769231\n",
            "2022-11-30T22:10:32.264374: step 594, loss 0.291785, acc 0.875, precision 0.7, recall 0.583333 f1 0.636364\n",
            "2022-11-30T22:10:33.575961: step 595, loss 0.313035, acc 0.875, precision 0.875, recall 0.5 f1 0.636364\n",
            "2022-11-30T22:10:34.891696: step 596, loss 0.26924, acc 0.890625, precision 0.9375, recall 0.714286 f1 0.810811\n",
            "2022-11-30T22:10:36.208992: step 597, loss 0.2441, acc 0.921875, precision 1, recall 0.666667 f1 0.8\n",
            "2022-11-30T22:10:37.504963: step 598, loss 0.204676, acc 0.921875, precision 1, recall 0.642857 f1 0.782609\n",
            "2022-11-30T22:10:38.781292: step 599, loss 0.282381, acc 0.90625, precision 0.869565, recall 0.869565 f1 0.869565\n",
            "2022-11-30T22:10:40.237660: step 600, loss 0.225404, acc 0.921875, precision 0.9, recall 0.692308 f1 0.782609\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:10:45.758753: step 600, loss 0.395643, acc 0.847619, precision 0.844828, recall 0.510417 f1 0.636364\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-600\n",
            "\n",
            "2022-11-30T22:10:47.198592: step 601, loss 0.257682, acc 0.84375, precision 0.705882, recall 0.705882 f1 0.705882\n",
            "2022-11-30T22:10:48.531970: step 602, loss 0.313073, acc 0.859375, precision 0.5625, recall 0.818182 f1 0.666667\n",
            "2022-11-30T22:10:49.912964: step 603, loss 0.188872, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:10:51.178856: step 604, loss 0.38105, acc 0.8125, precision 0.615385, recall 0.533333 f1 0.571429\n",
            "2022-11-30T22:10:52.565133: step 605, loss 0.134966, acc 0.96875, precision 0.9, recall 0.9 f1 0.9\n",
            "2022-11-30T22:10:53.867136: step 606, loss 0.345388, acc 0.84375, precision 0.684211, recall 0.764706 f1 0.722222\n",
            "2022-11-30T22:10:55.152520: step 607, loss 0.328405, acc 0.875, precision 0.882353, recall 0.714286 f1 0.789474\n",
            "2022-11-30T22:10:56.426542: step 608, loss 0.217104, acc 0.890625, precision 0.909091, recall 0.625 f1 0.740741\n",
            "2022-11-30T22:10:57.696225: step 609, loss 0.23794, acc 0.921875, precision 0.846154, recall 0.785714 f1 0.814815\n",
            "2022-11-30T22:10:58.949435: step 610, loss 0.214769, acc 0.90625, precision 0.916667, recall 0.6875 f1 0.785714\n",
            "2022-11-30T22:11:00.216374: step 611, loss 0.25901, acc 0.90625, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:11:01.519195: step 612, loss 0.184328, acc 0.9375, precision 0.933333, recall 0.823529 f1 0.875\n",
            "2022-11-30T22:11:02.807582: step 613, loss 0.211095, acc 0.9375, precision 0.916667, recall 0.785714 f1 0.846154\n",
            "2022-11-30T22:11:04.069104: step 614, loss 0.341112, acc 0.875, precision 0.7, recall 0.583333 f1 0.636364\n",
            "2022-11-30T22:11:05.339923: step 615, loss 0.292658, acc 0.859375, precision 0.6875, recall 0.733333 f1 0.709677\n",
            "2022-11-30T22:11:06.612888: step 616, loss 0.324119, acc 0.890625, precision 0.888889, recall 0.761905 f1 0.820513\n",
            "2022-11-30T22:11:07.867817: step 617, loss 0.191493, acc 0.90625, precision 0.933333, recall 0.736842 f1 0.823529\n",
            "2022-11-30T22:11:09.119145: step 618, loss 0.291524, acc 0.875, precision 0.727273, recall 0.615385 f1 0.666667\n",
            "2022-11-30T22:11:10.407266: step 619, loss 0.254107, acc 0.875, precision 0.666667, recall 0.933333 f1 0.777778\n",
            "2022-11-30T22:11:11.718428: step 620, loss 0.232131, acc 0.875, precision 0.714286, recall 0.882353 f1 0.789474\n",
            "2022-11-30T22:11:13.008149: step 621, loss 0.317742, acc 0.84375, precision 0.84, recall 0.777778 f1 0.807692\n",
            "2022-11-30T22:11:14.302918: step 622, loss 0.465499, acc 0.796875, precision 0.615385, recall 0.5 f1 0.551724\n",
            "2022-11-30T22:11:15.670865: step 623, loss 0.42134, acc 0.84375, precision 0.809524, recall 0.73913 f1 0.772727\n",
            "2022-11-30T22:11:16.310794: step 624, loss 0.39384, acc 0.814815, precision 0.875, recall 0.636364 f1 0.736842\n",
            "2022-11-30T22:11:17.571319: step 625, loss 0.214518, acc 0.921875, precision 1, recall 0.705882 f1 0.827586\n",
            "2022-11-30T22:11:18.825874: step 626, loss 0.283146, acc 0.8125, precision 0.583333, recall 0.875 f1 0.7\n",
            "2022-11-30T22:11:20.115793: step 627, loss 0.158596, acc 0.953125, precision 0.952381, recall 0.909091 f1 0.930233\n",
            "2022-11-30T22:11:21.395392: step 628, loss 0.231097, acc 0.9375, precision 0.9, recall 0.9 f1 0.9\n",
            "2022-11-30T22:11:22.670334: step 629, loss 0.157426, acc 0.921875, precision 0.777778, recall 0.933333 f1 0.848485\n",
            "2022-11-30T22:11:23.918406: step 630, loss 0.367063, acc 0.90625, precision 0.894737, recall 0.809524 f1 0.85\n",
            "2022-11-30T22:11:25.182520: step 631, loss 0.137351, acc 0.953125, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T22:11:26.453883: step 632, loss 0.271914, acc 0.890625, precision 1, recall 0.363636 f1 0.533333\n",
            "2022-11-30T22:11:27.711252: step 633, loss 0.197234, acc 0.890625, precision 0.8, recall 0.75 f1 0.774194\n",
            "2022-11-30T22:11:28.975954: step 634, loss 0.42185, acc 0.859375, precision 0.785714, recall 0.647059 f1 0.709677\n",
            "2022-11-30T22:11:30.248672: step 635, loss 0.167416, acc 0.9375, precision 0.941176, recall 0.842105 f1 0.888889\n",
            "2022-11-30T22:11:31.535689: step 636, loss 0.353392, acc 0.828125, precision 0.636364, recall 0.5 f1 0.56\n",
            "2022-11-30T22:11:32.809116: step 637, loss 0.163569, acc 0.90625, precision 0.785714, recall 0.785714 f1 0.785714\n",
            "2022-11-30T22:11:34.068691: step 638, loss 0.283467, acc 0.890625, precision 0.785714, recall 0.733333 f1 0.758621\n",
            "2022-11-30T22:11:35.374746: step 639, loss 0.0739861, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:11:36.643347: step 640, loss 0.19485, acc 0.90625, precision 0.85, recall 0.85 f1 0.85\n",
            "2022-11-30T22:11:37.926807: step 641, loss 0.155874, acc 0.953125, precision 0.8125, recall 1 f1 0.896552\n",
            "2022-11-30T22:11:39.230657: step 642, loss 0.283789, acc 0.859375, precision 0.727273, recall 0.571429 f1 0.64\n",
            "2022-11-30T22:11:40.508462: step 643, loss 0.238788, acc 0.9375, precision 0.933333, recall 0.823529 f1 0.875\n",
            "2022-11-30T22:11:41.769928: step 644, loss 0.115821, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:11:43.030425: step 645, loss 0.275764, acc 0.921875, precision 0.933333, recall 0.777778 f1 0.848485\n",
            "2022-11-30T22:11:44.291595: step 646, loss 0.316539, acc 0.84375, precision 0.941176, recall 0.64 f1 0.761905\n",
            "2022-11-30T22:11:45.609538: step 647, loss 0.156237, acc 0.953125, precision 0.857143, recall 0.923077 f1 0.888889\n",
            "2022-11-30T22:11:46.916321: step 648, loss 0.195347, acc 0.921875, precision 0.733333, recall 0.916667 f1 0.814815\n",
            "2022-11-30T22:11:48.192722: step 649, loss 0.236107, acc 0.9375, precision 0.947368, recall 0.857143 f1 0.9\n",
            "2022-11-30T22:11:49.484234: step 650, loss 0.232588, acc 0.890625, precision 0.777778, recall 0.823529 f1 0.8\n",
            "2022-11-30T22:11:50.825576: step 651, loss 0.188229, acc 0.921875, precision 0.875, recall 0.823529 f1 0.848485\n",
            "2022-11-30T22:11:52.108457: step 652, loss 0.300905, acc 0.875, precision 0.666667, recall 0.666667 f1 0.666667\n",
            "2022-11-30T22:11:53.402484: step 653, loss 0.138258, acc 0.9375, precision 0.733333, recall 1 f1 0.846154\n",
            "2022-11-30T22:11:54.701848: step 654, loss 0.181584, acc 0.90625, precision 0.933333, recall 0.736842 f1 0.823529\n",
            "2022-11-30T22:11:56.003176: step 655, loss 0.302306, acc 0.859375, precision 0.727273, recall 0.571429 f1 0.64\n",
            "2022-11-30T22:11:57.295115: step 656, loss 0.319859, acc 0.875, precision 0.875, recall 0.5 f1 0.636364\n",
            "2022-11-30T22:11:58.577080: step 657, loss 0.206197, acc 0.921875, precision 0.909091, recall 0.714286 f1 0.8\n",
            "2022-11-30T22:11:59.869542: step 658, loss 0.162993, acc 0.921875, precision 0.777778, recall 0.7 f1 0.736842\n",
            "2022-11-30T22:12:01.154318: step 659, loss 0.205411, acc 0.890625, precision 0.833333, recall 0.666667 f1 0.740741\n",
            "2022-11-30T22:12:02.448883: step 660, loss 0.254214, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:12:03.769435: step 661, loss 0.247612, acc 0.859375, precision 0.866667, recall 0.65 f1 0.742857\n",
            "2022-11-30T22:12:05.062456: step 662, loss 0.209471, acc 0.921875, precision 0.846154, recall 0.785714 f1 0.814815\n",
            "2022-11-30T22:12:06.331110: step 663, loss 0.118962, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:12:07.601599: step 664, loss 0.199848, acc 0.890625, precision 0.684211, recall 0.928571 f1 0.787879\n",
            "2022-11-30T22:12:08.888569: step 665, loss 0.322904, acc 0.890625, precision 0.866667, recall 0.722222 f1 0.787879\n",
            "2022-11-30T22:12:10.172776: step 666, loss 0.240132, acc 0.9375, precision 0.923077, recall 0.8 f1 0.857143\n",
            "2022-11-30T22:12:11.480755: step 667, loss 0.328448, acc 0.84375, precision 0.692308, recall 0.6 f1 0.642857\n",
            "2022-11-30T22:12:12.761941: step 668, loss 0.270069, acc 0.890625, precision 0.722222, recall 0.866667 f1 0.787879\n",
            "2022-11-30T22:12:14.039607: step 669, loss 0.247669, acc 0.890625, precision 0.8, recall 0.75 f1 0.774194\n",
            "2022-11-30T22:12:15.315014: step 670, loss 0.188671, acc 0.90625, precision 0.666667, recall 0.8 f1 0.727273\n",
            "2022-11-30T22:12:16.683759: step 671, loss 0.272722, acc 0.84375, precision 0.9, recall 0.5 f1 0.642857\n",
            "2022-11-30T22:12:17.933646: step 672, loss 0.205296, acc 0.921875, precision 0.769231, recall 0.833333 f1 0.8\n",
            "2022-11-30T22:12:19.200937: step 673, loss 0.0641062, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:12:20.515509: step 674, loss 0.210185, acc 0.875, precision 0.923077, recall 0.631579 f1 0.75\n",
            "2022-11-30T22:12:21.862473: step 675, loss 0.147354, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:12:23.201787: step 676, loss 0.36958, acc 0.90625, precision 0.888889, recall 0.615385 f1 0.727273\n",
            "2022-11-30T22:12:24.464488: step 677, loss 0.263817, acc 0.859375, precision 0.923077, recall 0.6 f1 0.727273\n",
            "2022-11-30T22:12:25.740143: step 678, loss 0.189687, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:12:27.027416: step 679, loss 0.319832, acc 0.84375, precision 0.8, recall 0.631579 f1 0.705882\n",
            "2022-11-30T22:12:28.311660: step 680, loss 0.160433, acc 0.9375, precision 0.785714, recall 0.916667 f1 0.846154\n",
            "2022-11-30T22:12:29.600466: step 681, loss 0.374727, acc 0.859375, precision 0.75, recall 0.857143 f1 0.8\n",
            "2022-11-30T22:12:30.875230: step 682, loss 0.159923, acc 0.9375, precision 0.7, recall 0.875 f1 0.777778\n",
            "2022-11-30T22:12:32.156561: step 683, loss 0.184016, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:12:33.480076: step 684, loss 0.320272, acc 0.84375, precision 0.7, recall 0.777778 f1 0.736842\n",
            "2022-11-30T22:12:34.759284: step 685, loss 0.308483, acc 0.875, precision 0.684211, recall 0.866667 f1 0.764706\n",
            "2022-11-30T22:12:36.044963: step 686, loss 0.100551, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:12:37.366786: step 687, loss 0.144225, acc 0.921875, precision 0.833333, recall 0.769231 f1 0.8\n",
            "2022-11-30T22:12:38.654315: step 688, loss 0.223428, acc 0.921875, precision 0.909091, recall 0.714286 f1 0.8\n",
            "2022-11-30T22:12:39.956352: step 689, loss 0.191977, acc 0.90625, precision 0.9, recall 0.642857 f1 0.75\n",
            "2022-11-30T22:12:41.233361: step 690, loss 0.423812, acc 0.84375, precision 0.866667, recall 0.619048 f1 0.722222\n",
            "2022-11-30T22:12:42.528851: step 691, loss 0.404746, acc 0.78125, precision 0.875, recall 0.35 f1 0.5\n",
            "2022-11-30T22:12:43.833762: step 692, loss 0.27664, acc 0.890625, precision 0.8, recall 0.615385 f1 0.695652\n",
            "2022-11-30T22:12:45.159839: step 693, loss 0.186391, acc 0.90625, precision 0.916667, recall 0.6875 f1 0.785714\n",
            "2022-11-30T22:12:46.507548: step 694, loss 0.189532, acc 0.9375, precision 0.916667, recall 0.785714 f1 0.846154\n",
            "2022-11-30T22:12:47.825048: step 695, loss 0.242346, acc 0.890625, precision 0.857143, recall 0.705882 f1 0.774194\n",
            "2022-11-30T22:12:49.110526: step 696, loss 0.159259, acc 0.953125, precision 0.952381, recall 0.909091 f1 0.930233\n",
            "2022-11-30T22:12:50.398222: step 697, loss 0.29348, acc 0.84375, precision 0.578947, recall 0.846154 f1 0.6875\n",
            "2022-11-30T22:12:51.680833: step 698, loss 0.243347, acc 0.890625, precision 0.730769, recall 1 f1 0.844444\n",
            "2022-11-30T22:12:52.998919: step 699, loss 0.227999, acc 0.859375, precision 0.684211, recall 0.8125 f1 0.742857\n",
            "2022-11-30T22:12:54.300471: step 700, loss 0.204179, acc 0.90625, precision 0.625, recall 1 f1 0.769231\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:12:57.816894: step 700, loss 0.436473, acc 0.84898, precision 0.893204, recall 0.479167 f1 0.623729\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-700\n",
            "\n",
            "2022-11-30T22:12:59.246608: step 701, loss 0.211579, acc 0.9375, precision 0.923077, recall 0.8 f1 0.857143\n",
            "2022-11-30T22:13:00.551024: step 702, loss 0.135794, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:13:01.827247: step 703, loss 0.263075, acc 0.90625, precision 1, recall 0.571429 f1 0.727273\n",
            "2022-11-30T22:13:03.088067: step 704, loss 0.170259, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:13:04.349638: step 705, loss 0.258692, acc 0.875, precision 0.888889, recall 0.533333 f1 0.666667\n",
            "2022-11-30T22:13:05.606326: step 706, loss 0.457694, acc 0.859375, precision 1, recall 0.571429 f1 0.727273\n",
            "2022-11-30T22:13:06.871364: step 707, loss 0.278701, acc 0.921875, precision 1, recall 0.705882 f1 0.827586\n",
            "2022-11-30T22:13:08.162304: step 708, loss 0.253033, acc 0.921875, precision 0.8125, recall 0.866667 f1 0.83871\n",
            "2022-11-30T22:13:09.429124: step 709, loss 0.216499, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:13:10.716185: step 710, loss 0.0876908, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:13:12.002723: step 711, loss 0.285926, acc 0.921875, precision 0.833333, recall 0.769231 f1 0.8\n",
            "2022-11-30T22:13:13.290085: step 712, loss 0.18932, acc 0.921875, precision 0.769231, recall 0.833333 f1 0.8\n",
            "2022-11-30T22:13:14.585232: step 713, loss 0.220292, acc 0.921875, precision 0.8, recall 0.941176 f1 0.864865\n",
            "2022-11-30T22:13:15.852780: step 714, loss 0.155337, acc 0.921875, precision 0.857143, recall 0.9 f1 0.878049\n",
            "2022-11-30T22:13:17.155039: step 715, loss 0.212983, acc 0.90625, precision 0.75, recall 0.857143 f1 0.8\n",
            "2022-11-30T22:13:18.451256: step 716, loss 0.204917, acc 0.90625, precision 0.928571, recall 0.722222 f1 0.8125\n",
            "2022-11-30T22:13:19.722067: step 717, loss 0.200025, acc 0.9375, precision 0.9, recall 0.75 f1 0.818182\n",
            "2022-11-30T22:13:20.997351: step 718, loss 0.210689, acc 0.90625, precision 0.75, recall 0.857143 f1 0.8\n",
            "2022-11-30T22:13:22.277439: step 719, loss 0.290414, acc 0.859375, precision 0.571429, recall 0.727273 f1 0.64\n",
            "2022-11-30T22:13:23.569925: step 720, loss 0.135344, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:13:24.864524: step 721, loss 0.29581, acc 0.890625, precision 1, recall 0.681818 f1 0.810811\n",
            "2022-11-30T22:13:26.129132: step 722, loss 0.221018, acc 0.90625, precision 0.941176, recall 0.761905 f1 0.842105\n",
            "2022-11-30T22:13:27.407217: step 723, loss 0.221242, acc 0.890625, precision 0.888889, recall 0.571429 f1 0.695652\n",
            "2022-11-30T22:13:28.674814: step 724, loss 0.210178, acc 0.953125, precision 1, recall 0.769231 f1 0.869565\n",
            "2022-11-30T22:13:29.954822: step 725, loss 0.457345, acc 0.875, precision 1, recall 0.529412 f1 0.692308\n",
            "2022-11-30T22:13:31.224928: step 726, loss 0.268518, acc 0.890625, precision 0.916667, recall 0.647059 f1 0.758621\n",
            "2022-11-30T22:13:32.488413: step 727, loss 0.265486, acc 0.921875, precision 0.875, recall 0.823529 f1 0.848485\n",
            "2022-11-30T22:13:33.137186: step 728, loss 0.0649576, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:13:34.408409: step 729, loss 0.176302, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:13:35.688884: step 730, loss 0.197735, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:13:36.942752: step 731, loss 0.220962, acc 0.90625, precision 0.736842, recall 0.933333 f1 0.823529\n",
            "2022-11-30T22:13:38.243642: step 732, loss 0.169371, acc 0.9375, precision 0.769231, recall 0.909091 f1 0.833333\n",
            "2022-11-30T22:13:39.536233: step 733, loss 0.35147, acc 0.828125, precision 0.571429, recall 0.857143 f1 0.685714\n",
            "2022-11-30T22:13:40.834433: step 734, loss 0.310793, acc 0.859375, precision 0.708333, recall 0.894737 f1 0.790698\n",
            "2022-11-30T22:13:42.129859: step 735, loss 0.220541, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:13:44.116021: step 736, loss 0.126726, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:13:46.088750: step 737, loss 0.355294, acc 0.890625, precision 1, recall 0.588235 f1 0.740741\n",
            "2022-11-30T22:13:48.075244: step 738, loss 0.306941, acc 0.859375, precision 1, recall 0.571429 f1 0.727273\n",
            "2022-11-30T22:13:49.349509: step 739, loss 0.218429, acc 0.875, precision 1, recall 0.636364 f1 0.777778\n",
            "2022-11-30T22:13:50.660430: step 740, loss 0.142216, acc 0.90625, precision 1, recall 0.5 f1 0.666667\n",
            "2022-11-30T22:13:51.949396: step 741, loss 0.256777, acc 0.859375, precision 1, recall 0.470588 f1 0.64\n",
            "2022-11-30T22:13:53.253600: step 742, loss 0.305664, acc 0.875, precision 1, recall 0.652174 f1 0.789474\n",
            "2022-11-30T22:13:54.578816: step 743, loss 0.168448, acc 0.890625, precision 0.692308, recall 0.75 f1 0.72\n",
            "2022-11-30T22:13:55.868416: step 744, loss 0.115244, acc 0.953125, precision 1, recall 0.785714 f1 0.88\n",
            "2022-11-30T22:13:57.130176: step 745, loss 0.265427, acc 0.875, precision 0.625, recall 0.833333 f1 0.714286\n",
            "2022-11-30T22:13:58.423516: step 746, loss 0.365548, acc 0.890625, precision 0.714286, recall 0.9375 f1 0.810811\n",
            "2022-11-30T22:13:59.719420: step 747, loss 0.197826, acc 0.9375, precision 0.823529, recall 0.933333 f1 0.875\n",
            "2022-11-30T22:14:01.019449: step 748, loss 0.248024, acc 0.90625, precision 0.75, recall 0.9375 f1 0.833333\n",
            "2022-11-30T22:14:02.312404: step 749, loss 0.269416, acc 0.875, precision 0.625, recall 0.833333 f1 0.714286\n",
            "2022-11-30T22:14:03.613869: step 750, loss 0.171312, acc 0.9375, precision 0.833333, recall 0.833333 f1 0.833333\n",
            "2022-11-30T22:14:04.917425: step 751, loss 0.121465, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:14:06.212398: step 752, loss 0.230227, acc 0.890625, precision 0.857143, recall 0.705882 f1 0.774194\n",
            "2022-11-30T22:14:07.492080: step 753, loss 0.225763, acc 0.921875, precision 0.944444, recall 0.809524 f1 0.871795\n",
            "2022-11-30T22:14:08.751610: step 754, loss 0.170752, acc 0.9375, precision 1, recall 0.809524 f1 0.894737\n",
            "2022-11-30T22:14:10.032653: step 755, loss 0.229288, acc 0.90625, precision 0.928571, recall 0.722222 f1 0.8125\n",
            "2022-11-30T22:14:11.343134: step 756, loss 0.255531, acc 0.875, precision 1, recall 0.5 f1 0.666667\n",
            "2022-11-30T22:14:12.650439: step 757, loss 0.234031, acc 0.890625, precision 1, recall 0.631579 f1 0.774194\n",
            "2022-11-30T22:14:13.956142: step 758, loss 0.187939, acc 0.921875, precision 0.9375, recall 0.789474 f1 0.857143\n",
            "2022-11-30T22:14:15.261430: step 759, loss 0.24554, acc 0.9375, precision 1, recall 0.84 f1 0.913043\n",
            "2022-11-30T22:14:16.543507: step 760, loss 0.122578, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:14:17.929255: step 761, loss 0.193671, acc 0.953125, precision 0.769231, recall 1 f1 0.869565\n",
            "2022-11-30T22:14:19.254242: step 762, loss 0.196886, acc 0.859375, precision 0.73913, recall 0.85 f1 0.790698\n",
            "2022-11-30T22:14:20.562132: step 763, loss 0.205236, acc 0.9375, precision 0.85, recall 0.944444 f1 0.894737\n",
            "2022-11-30T22:14:21.846200: step 764, loss 0.271209, acc 0.890625, precision 0.733333, recall 0.785714 f1 0.758621\n",
            "2022-11-30T22:14:23.128531: step 765, loss 0.234999, acc 0.921875, precision 0.785714, recall 0.846154 f1 0.814815\n",
            "2022-11-30T22:14:24.462259: step 766, loss 0.142122, acc 0.953125, precision 0.95, recall 0.904762 f1 0.926829\n",
            "2022-11-30T22:14:25.750021: step 767, loss 0.174485, acc 0.90625, precision 0.857143, recall 0.75 f1 0.8\n",
            "2022-11-30T22:14:27.027197: step 768, loss 0.152056, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:14:28.309489: step 769, loss 0.194818, acc 0.90625, precision 0.947368, recall 0.782609 f1 0.857143\n",
            "2022-11-30T22:14:29.587917: step 770, loss 0.245146, acc 0.921875, precision 0.894737, recall 0.85 f1 0.871795\n",
            "2022-11-30T22:14:30.872145: step 771, loss 0.1966, acc 0.9375, precision 1, recall 0.733333 f1 0.846154\n",
            "2022-11-30T22:14:32.165706: step 772, loss 0.0706606, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:14:33.449274: step 773, loss 0.206026, acc 0.921875, precision 0.9375, recall 0.789474 f1 0.857143\n",
            "2022-11-30T22:14:34.762458: step 774, loss 0.11675, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:14:36.065827: step 775, loss 0.189735, acc 0.9375, precision 0.866667, recall 0.866667 f1 0.866667\n",
            "2022-11-30T22:14:37.346558: step 776, loss 0.169905, acc 0.9375, precision 0.941176, recall 0.842105 f1 0.888889\n",
            "2022-11-30T22:14:38.636200: step 777, loss 0.088249, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:14:39.943540: step 778, loss 0.155299, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:14:41.275112: step 779, loss 0.297652, acc 0.921875, precision 0.785714, recall 0.846154 f1 0.814815\n",
            "2022-11-30T22:14:42.586709: step 780, loss 0.111811, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:14:43.895162: step 781, loss 0.186886, acc 0.953125, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:14:45.156143: step 782, loss 0.219327, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:14:46.460661: step 783, loss 0.222987, acc 0.9375, precision 0.947368, recall 0.857143 f1 0.9\n",
            "2022-11-30T22:14:47.740998: step 784, loss 0.122174, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:14:49.093411: step 785, loss 0.238901, acc 0.921875, precision 0.8, recall 0.941176 f1 0.864865\n",
            "2022-11-30T22:14:50.383405: step 786, loss 0.165846, acc 0.953125, precision 0.846154, recall 0.916667 f1 0.88\n",
            "2022-11-30T22:14:51.670673: step 787, loss 0.172988, acc 0.9375, precision 0.9, recall 0.9 f1 0.9\n",
            "2022-11-30T22:14:52.995943: step 788, loss 0.178934, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:14:54.297919: step 789, loss 0.185003, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:14:55.599918: step 790, loss 0.247298, acc 0.90625, precision 0.923077, recall 0.705882 f1 0.8\n",
            "2022-11-30T22:14:56.890773: step 791, loss 0.248025, acc 0.90625, precision 0.909091, recall 0.666667 f1 0.769231\n",
            "2022-11-30T22:14:58.200196: step 792, loss 0.154597, acc 0.953125, precision 0.833333, recall 0.909091 f1 0.869565\n",
            "2022-11-30T22:14:59.472696: step 793, loss 0.219219, acc 0.90625, precision 0.857143, recall 0.75 f1 0.8\n",
            "2022-11-30T22:15:00.759734: step 794, loss 0.174098, acc 0.890625, precision 0.888889, recall 0.571429 f1 0.695652\n",
            "2022-11-30T22:15:02.059268: step 795, loss 0.16057, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:15:03.356228: step 796, loss 0.216633, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:15:04.655366: step 797, loss 0.144045, acc 0.9375, precision 0.857143, recall 0.857143 f1 0.857143\n",
            "2022-11-30T22:15:05.959665: step 798, loss 0.202376, acc 0.953125, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:15:07.226857: step 799, loss 0.135516, acc 0.921875, precision 0.818182, recall 0.75 f1 0.782609\n",
            "2022-11-30T22:15:08.496406: step 800, loss 0.156128, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:15:12.065447: step 800, loss 0.440734, acc 0.851701, precision 0.880734, recall 0.5 f1 0.637874\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-800\n",
            "\n",
            "2022-11-30T22:15:13.479914: step 801, loss 0.174195, acc 0.953125, precision 0.85, recall 1 f1 0.918919\n",
            "2022-11-30T22:15:14.775571: step 802, loss 0.261427, acc 0.890625, precision 0.933333, recall 0.7 f1 0.8\n",
            "2022-11-30T22:15:16.068444: step 803, loss 0.163917, acc 0.890625, precision 0.714286, recall 0.769231 f1 0.740741\n",
            "2022-11-30T22:15:17.332836: step 804, loss 0.123803, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:15:18.664796: step 805, loss 0.199521, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:15:19.972358: step 806, loss 0.145824, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:15:21.292525: step 807, loss 0.126958, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:15:22.594401: step 808, loss 0.363068, acc 0.875, precision 0.818182, recall 0.6 f1 0.692308\n",
            "2022-11-30T22:15:23.889689: step 809, loss 0.317857, acc 0.890625, precision 0.909091, recall 0.625 f1 0.740741\n",
            "2022-11-30T22:15:25.197817: step 810, loss 0.258523, acc 0.921875, precision 0.941176, recall 0.8 f1 0.864865\n",
            "2022-11-30T22:15:26.507499: step 811, loss 0.177543, acc 0.921875, precision 0.916667, recall 0.733333 f1 0.814815\n",
            "2022-11-30T22:15:27.801369: step 812, loss 0.0747278, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:15:29.086703: step 813, loss 0.125425, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:15:30.373019: step 814, loss 0.105193, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:15:31.653310: step 815, loss 0.131892, acc 0.953125, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:15:32.922994: step 816, loss 0.39084, acc 0.828125, precision 0.590909, recall 0.866667 f1 0.702703\n",
            "2022-11-30T22:15:34.198469: step 817, loss 0.160426, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:15:35.485850: step 818, loss 0.106554, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:15:36.805900: step 819, loss 0.14358, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:15:38.093916: step 820, loss 0.233263, acc 0.890625, precision 0.7, recall 0.636364 f1 0.666667\n",
            "2022-11-30T22:15:39.394115: step 821, loss 0.279727, acc 0.90625, precision 1, recall 0.625 f1 0.769231\n",
            "2022-11-30T22:15:40.698121: step 822, loss 0.137766, acc 0.9375, precision 0.888889, recall 0.888889 f1 0.888889\n",
            "2022-11-30T22:15:41.977983: step 823, loss 0.126859, acc 0.9375, precision 0.909091, recall 0.769231 f1 0.833333\n",
            "2022-11-30T22:15:43.242978: step 824, loss 0.216191, acc 0.921875, precision 0.818182, recall 0.75 f1 0.782609\n",
            "2022-11-30T22:15:44.540789: step 825, loss 0.0626536, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:15:45.834631: step 826, loss 0.120916, acc 0.9375, precision 1, recall 0.636364 f1 0.777778\n",
            "2022-11-30T22:15:47.134576: step 827, loss 0.246795, acc 0.90625, precision 1, recall 0.714286 f1 0.833333\n",
            "2022-11-30T22:15:48.389138: step 828, loss 0.256022, acc 0.90625, precision 0.8, recall 0.666667 f1 0.727273\n",
            "2022-11-30T22:15:49.740252: step 829, loss 0.207848, acc 0.9375, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:15:51.077668: step 830, loss 0.175839, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:15:52.390283: step 831, loss 0.289568, acc 0.875, precision 0.789474, recall 0.789474 f1 0.789474\n",
            "2022-11-30T22:15:53.051096: step 832, loss 0.255208, acc 0.925926, precision 0.75, recall 1 f1 0.857143\n",
            "2022-11-30T22:15:54.378827: step 833, loss 0.086589, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:15:55.681136: step 834, loss 0.153713, acc 0.90625, precision 0.75, recall 0.857143 f1 0.8\n",
            "2022-11-30T22:15:56.998722: step 835, loss 0.249329, acc 0.921875, precision 0.888889, recall 0.842105 f1 0.864865\n",
            "2022-11-30T22:15:58.293234: step 836, loss 0.0958309, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:15:59.607124: step 837, loss 0.171866, acc 0.90625, precision 0.733333, recall 0.846154 f1 0.785714\n",
            "2022-11-30T22:16:00.919183: step 838, loss 0.0688304, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:16:02.223097: step 839, loss 0.248945, acc 0.90625, precision 0.785714, recall 0.785714 f1 0.785714\n",
            "2022-11-30T22:16:03.508406: step 840, loss 0.0943025, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:16:04.788954: step 841, loss 0.231821, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:16:06.091616: step 842, loss 0.0992198, acc 0.953125, precision 1, recall 0.85 f1 0.918919\n",
            "2022-11-30T22:16:07.364642: step 843, loss 0.190566, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:16:08.640251: step 844, loss 0.176017, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:16:09.918747: step 845, loss 0.191369, acc 0.90625, precision 0.769231, recall 0.769231 f1 0.769231\n",
            "2022-11-30T22:16:11.199760: step 846, loss 0.137305, acc 0.921875, precision 1, recall 0.666667 f1 0.8\n",
            "2022-11-30T22:16:12.496983: step 847, loss 0.102369, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:16:13.787974: step 848, loss 0.0901173, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:16:15.078444: step 849, loss 0.165702, acc 0.953125, precision 0.95, recall 0.904762 f1 0.926829\n",
            "2022-11-30T22:16:16.406850: step 850, loss 0.280003, acc 0.921875, precision 0.8, recall 0.857143 f1 0.827586\n",
            "2022-11-30T22:16:17.700242: step 851, loss 0.11454, acc 0.953125, precision 0.842105, recall 1 f1 0.914286\n",
            "2022-11-30T22:16:19.015274: step 852, loss 0.260229, acc 0.890625, precision 0.826087, recall 0.863636 f1 0.844444\n",
            "2022-11-30T22:16:20.329627: step 853, loss 0.280576, acc 0.890625, precision 0.894737, recall 0.772727 f1 0.829268\n",
            "2022-11-30T22:16:21.621583: step 854, loss 0.0662276, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:16:22.917958: step 855, loss 0.173289, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:16:24.196768: step 856, loss 0.253224, acc 0.90625, precision 0.941176, recall 0.761905 f1 0.842105\n",
            "2022-11-30T22:16:25.492900: step 857, loss 0.152747, acc 0.953125, precision 1, recall 0.88 f1 0.93617\n",
            "2022-11-30T22:16:26.803857: step 858, loss 0.232966, acc 0.90625, precision 0.75, recall 0.9375 f1 0.833333\n",
            "2022-11-30T22:16:28.102869: step 859, loss 0.210714, acc 0.9375, precision 0.8, recall 0.923077 f1 0.857143\n",
            "2022-11-30T22:16:29.400772: step 860, loss 0.163428, acc 0.9375, precision 1, recall 0.692308 f1 0.818182\n",
            "2022-11-30T22:16:30.717004: step 861, loss 0.194044, acc 0.90625, precision 0.75, recall 0.75 f1 0.75\n",
            "2022-11-30T22:16:32.014134: step 862, loss 0.161156, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:16:33.311699: step 863, loss 0.104716, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:16:34.607048: step 864, loss 0.22836, acc 0.875, precision 0.842105, recall 0.761905 f1 0.8\n",
            "2022-11-30T22:16:35.923914: step 865, loss 0.0592438, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:16:37.202295: step 866, loss 0.0839004, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:16:38.520982: step 867, loss 0.130045, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:16:39.797368: step 868, loss 0.228124, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:16:41.097150: step 869, loss 0.142835, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:16:42.367451: step 870, loss 0.122418, acc 0.96875, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:16:43.647273: step 871, loss 0.0860907, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:16:44.971080: step 872, loss 0.122419, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:16:46.267357: step 873, loss 0.11853, acc 0.9375, precision 0.909091, recall 0.769231 f1 0.833333\n",
            "2022-11-30T22:16:47.597790: step 874, loss 0.154304, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:16:49.473913: step 875, loss 0.180433, acc 0.890625, precision 0.785714, recall 0.733333 f1 0.758621\n",
            "2022-11-30T22:16:51.589391: step 876, loss 0.17945, acc 0.921875, precision 0.916667, recall 0.733333 f1 0.814815\n",
            "2022-11-30T22:16:53.593540: step 877, loss 0.243827, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:16:54.954488: step 878, loss 0.17929, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:16:56.242341: step 879, loss 0.1034, acc 0.96875, precision 0.952381, recall 0.952381 f1 0.952381\n",
            "2022-11-30T22:16:57.544686: step 880, loss 0.175139, acc 0.921875, precision 0.857143, recall 0.8 f1 0.827586\n",
            "2022-11-30T22:16:58.826978: step 881, loss 0.137899, acc 0.9375, precision 0.916667, recall 0.785714 f1 0.846154\n",
            "2022-11-30T22:17:00.102171: step 882, loss 0.147542, acc 0.90625, precision 0.857143, recall 0.75 f1 0.8\n",
            "2022-11-30T22:17:01.405614: step 883, loss 0.123047, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:17:02.674611: step 884, loss 0.225021, acc 0.9375, precision 0.95, recall 0.863636 f1 0.904762\n",
            "2022-11-30T22:17:03.944944: step 885, loss 0.158782, acc 0.90625, precision 0.842105, recall 0.842105 f1 0.842105\n",
            "2022-11-30T22:17:05.220637: step 886, loss 0.131106, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:17:06.523475: step 887, loss 0.163033, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:17:07.822124: step 888, loss 0.30732, acc 0.890625, precision 0.733333, recall 0.785714 f1 0.758621\n",
            "2022-11-30T22:17:09.087929: step 889, loss 0.251213, acc 0.90625, precision 0.75, recall 0.857143 f1 0.8\n",
            "2022-11-30T22:17:10.379675: step 890, loss 0.109551, acc 0.9375, precision 0.777778, recall 1 f1 0.875\n",
            "2022-11-30T22:17:11.655105: step 891, loss 0.157129, acc 0.9375, precision 0.909091, recall 0.769231 f1 0.833333\n",
            "2022-11-30T22:17:12.975682: step 892, loss 0.167383, acc 0.9375, precision 0.947368, recall 0.857143 f1 0.9\n",
            "2022-11-30T22:17:14.277385: step 893, loss 0.0854609, acc 0.953125, precision 0.933333, recall 0.875 f1 0.903226\n",
            "2022-11-30T22:17:15.596775: step 894, loss 0.0732017, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:17:16.894419: step 895, loss 0.184143, acc 0.953125, precision 1, recall 0.769231 f1 0.869565\n",
            "2022-11-30T22:17:18.194600: step 896, loss 0.220462, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:17:19.492081: step 897, loss 0.0570647, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:17:20.866204: step 898, loss 0.140796, acc 0.9375, precision 0.928571, recall 0.8125 f1 0.866667\n",
            "2022-11-30T22:17:22.161461: step 899, loss 0.234666, acc 0.921875, precision 0.888889, recall 0.842105 f1 0.864865\n",
            "2022-11-30T22:17:23.487008: step 900, loss 0.202833, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:17:27.058634: step 900, loss 0.442982, acc 0.851701, precision 0.854701, recall 0.520833 f1 0.647249\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-900\n",
            "\n",
            "2022-11-30T22:17:28.525425: step 901, loss 0.164972, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:17:29.817446: step 902, loss 0.0958675, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:17:31.103585: step 903, loss 0.0671462, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:17:32.430681: step 904, loss 0.166352, acc 0.9375, precision 0.833333, recall 0.833333 f1 0.833333\n",
            "2022-11-30T22:17:33.709335: step 905, loss 0.140259, acc 0.9375, precision 0.894737, recall 0.894737 f1 0.894737\n",
            "2022-11-30T22:17:34.978827: step 906, loss 0.220551, acc 0.890625, precision 0.909091, recall 0.8 f1 0.851064\n",
            "2022-11-30T22:17:36.261039: step 907, loss 0.17931, acc 0.953125, precision 0.8125, recall 1 f1 0.896552\n",
            "2022-11-30T22:17:37.595446: step 908, loss 0.138053, acc 0.953125, precision 0.785714, recall 1 f1 0.88\n",
            "2022-11-30T22:17:38.916221: step 909, loss 0.112552, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:17:40.222672: step 910, loss 0.18233, acc 0.921875, precision 0.789474, recall 0.9375 f1 0.857143\n",
            "2022-11-30T22:17:41.504739: step 911, loss 0.11868, acc 0.9375, precision 0.916667, recall 0.785714 f1 0.846154\n",
            "2022-11-30T22:17:42.801529: step 912, loss 0.202637, acc 0.921875, precision 0.933333, recall 0.777778 f1 0.848485\n",
            "2022-11-30T22:17:44.092251: step 913, loss 0.184176, acc 0.9375, precision 1, recall 0.809524 f1 0.894737\n",
            "2022-11-30T22:17:45.403189: step 914, loss 0.14016, acc 0.9375, precision 0.923077, recall 0.8 f1 0.857143\n",
            "2022-11-30T22:17:46.728219: step 915, loss 0.170732, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:17:48.015809: step 916, loss 0.13445, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:17:49.289005: step 917, loss 0.0941007, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:17:50.608848: step 918, loss 0.190347, acc 0.90625, precision 0.818182, recall 0.692308 f1 0.75\n",
            "2022-11-30T22:17:51.945723: step 919, loss 0.2626, acc 0.921875, precision 0.916667, recall 0.733333 f1 0.814815\n",
            "2022-11-30T22:17:53.250694: step 920, loss 0.159472, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:17:54.568312: step 921, loss 0.31678, acc 0.859375, precision 0.842105, recall 0.727273 f1 0.780488\n",
            "2022-11-30T22:17:55.859544: step 922, loss 0.226158, acc 0.90625, precision 0.866667, recall 0.764706 f1 0.8125\n",
            "2022-11-30T22:17:57.149933: step 923, loss 0.15563, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:17:58.423007: step 924, loss 0.0840223, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:17:59.708149: step 925, loss 0.130012, acc 0.953125, precision 0.909091, recall 0.952381 f1 0.930233\n",
            "2022-11-30T22:18:01.001579: step 926, loss 0.123979, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:18:02.291673: step 927, loss 0.166921, acc 0.9375, precision 0.952381, recall 0.869565 f1 0.909091\n",
            "2022-11-30T22:18:03.580688: step 928, loss 0.11368, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:18:04.895366: step 929, loss 0.183627, acc 0.921875, precision 0.764706, recall 0.928571 f1 0.83871\n",
            "2022-11-30T22:18:06.206219: step 930, loss 0.07819, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:18:07.477609: step 931, loss 0.146075, acc 0.9375, precision 0.846154, recall 0.846154 f1 0.846154\n",
            "2022-11-30T22:18:08.782582: step 932, loss 0.106876, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:18:10.092678: step 933, loss 0.256571, acc 0.890625, precision 0.75, recall 0.692308 f1 0.72\n",
            "2022-11-30T22:18:11.380038: step 934, loss 0.130528, acc 0.953125, precision 1, recall 0.727273 f1 0.842105\n",
            "2022-11-30T22:18:12.676983: step 935, loss 0.104703, acc 0.9375, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:18:13.360153: step 936, loss 0.167861, acc 0.888889, precision 1, recall 0.571429 f1 0.727273\n",
            "2022-11-30T22:18:14.631341: step 937, loss 0.0877924, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:18:15.925060: step 938, loss 0.153576, acc 0.9375, precision 0.866667, recall 0.866667 f1 0.866667\n",
            "2022-11-30T22:18:17.174226: step 939, loss 0.107302, acc 0.953125, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T22:18:18.428950: step 940, loss 0.0883469, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:18:19.734441: step 941, loss 0.120514, acc 0.96875, precision 1, recall 0.92 f1 0.958333\n",
            "2022-11-30T22:18:21.118562: step 942, loss 0.140116, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:18:22.468254: step 943, loss 0.129196, acc 0.9375, precision 0.909091, recall 0.769231 f1 0.833333\n",
            "2022-11-30T22:18:23.773582: step 944, loss 0.166415, acc 0.9375, precision 0.75, recall 1 f1 0.857143\n",
            "2022-11-30T22:18:25.093355: step 945, loss 0.0755505, acc 0.953125, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:18:26.392510: step 946, loss 0.169619, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:18:27.719675: step 947, loss 0.149566, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:18:29.055018: step 948, loss 0.094511, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:18:30.355884: step 949, loss 0.0988474, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:18:31.653916: step 950, loss 0.163385, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:18:32.983548: step 951, loss 0.181414, acc 0.921875, precision 0.857143, recall 0.8 f1 0.827586\n",
            "2022-11-30T22:18:34.283357: step 952, loss 0.0978265, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:18:35.599248: step 953, loss 0.0996624, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:18:36.902655: step 954, loss 0.0919419, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:18:38.198799: step 955, loss 0.0713807, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:18:39.494701: step 956, loss 0.15732, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:18:40.818803: step 957, loss 0.130002, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:18:42.133889: step 958, loss 0.11018, acc 0.953125, precision 1, recall 0.869565 f1 0.930233\n",
            "2022-11-30T22:18:43.433945: step 959, loss 0.175076, acc 0.9375, precision 1, recall 0.764706 f1 0.866667\n",
            "2022-11-30T22:18:44.747988: step 960, loss 0.0701047, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:18:46.040496: step 961, loss 0.121699, acc 0.953125, precision 0.933333, recall 0.875 f1 0.903226\n",
            "2022-11-30T22:18:47.352539: step 962, loss 0.0758522, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:18:48.635815: step 963, loss 0.179761, acc 0.9375, precision 0.928571, recall 0.8125 f1 0.866667\n",
            "2022-11-30T22:18:49.980046: step 964, loss 0.174501, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:18:51.294568: step 965, loss 0.179012, acc 0.921875, precision 0.944444, recall 0.809524 f1 0.871795\n",
            "2022-11-30T22:18:52.630944: step 966, loss 0.147955, acc 0.9375, precision 0.777778, recall 1 f1 0.875\n",
            "2022-11-30T22:18:53.918511: step 967, loss 0.134122, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:18:55.224623: step 968, loss 0.0904432, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:18:56.573509: step 969, loss 0.181549, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:18:57.867166: step 970, loss 0.1029, acc 0.953125, precision 1, recall 0.863636 f1 0.926829\n",
            "2022-11-30T22:18:59.164204: step 971, loss 0.096042, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:19:00.477401: step 972, loss 0.142799, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:19:01.743613: step 973, loss 0.0764236, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:19:03.012963: step 974, loss 0.212051, acc 0.953125, precision 0.818182, recall 0.9 f1 0.857143\n",
            "2022-11-30T22:19:04.310464: step 975, loss 0.133757, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:19:05.613947: step 976, loss 0.0525677, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:19:06.926561: step 977, loss 0.0909933, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:19:08.227478: step 978, loss 0.166652, acc 0.9375, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:19:09.505662: step 979, loss 0.116251, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:19:10.813630: step 980, loss 0.172635, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:19:12.078643: step 981, loss 0.129754, acc 0.921875, precision 0.7, recall 0.777778 f1 0.736842\n",
            "2022-11-30T22:19:13.366193: step 982, loss 0.0673691, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:19:14.647044: step 983, loss 0.0961393, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:19:15.913936: step 984, loss 0.124811, acc 0.9375, precision 0.823529, recall 0.933333 f1 0.875\n",
            "2022-11-30T22:19:17.206693: step 985, loss 0.119836, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:19:18.509987: step 986, loss 0.160119, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:19:19.791516: step 987, loss 0.17431, acc 0.921875, precision 0.9375, recall 0.789474 f1 0.857143\n",
            "2022-11-30T22:19:21.123599: step 988, loss 0.0928307, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:19:22.460903: step 989, loss 0.257106, acc 0.953125, precision 0.956522, recall 0.916667 f1 0.93617\n",
            "2022-11-30T22:19:23.780753: step 990, loss 0.143143, acc 0.9375, precision 0.846154, recall 0.846154 f1 0.846154\n",
            "2022-11-30T22:19:25.036337: step 991, loss 0.169573, acc 0.9375, precision 0.928571, recall 0.8125 f1 0.866667\n",
            "2022-11-30T22:19:26.321297: step 992, loss 0.146183, acc 0.953125, precision 0.916667, recall 0.956522 f1 0.93617\n",
            "2022-11-30T22:19:27.585537: step 993, loss 0.105938, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:19:28.895706: step 994, loss 0.0715469, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:19:30.218105: step 995, loss 0.125253, acc 0.953125, precision 0.769231, recall 1 f1 0.869565\n",
            "2022-11-30T22:19:31.526300: step 996, loss 0.129652, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:19:32.808879: step 997, loss 0.217197, acc 0.921875, precision 0.913043, recall 0.875 f1 0.893617\n",
            "2022-11-30T22:19:34.106766: step 998, loss 0.131868, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:19:35.439738: step 999, loss 0.175224, acc 0.9375, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:19:36.745413: step 1000, loss 0.180308, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:19:40.269459: step 1000, loss 0.414497, acc 0.853061, precision 0.808824, recall 0.572917 f1 0.670732\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1000\n",
            "\n",
            "2022-11-30T22:19:41.784119: step 1001, loss 0.139195, acc 0.953125, precision 0.941176, recall 0.888889 f1 0.914286\n",
            "2022-11-30T22:19:43.078618: step 1002, loss 0.233201, acc 0.90625, precision 0.809524, recall 0.894737 f1 0.85\n",
            "2022-11-30T22:19:44.374870: step 1003, loss 0.121674, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:19:45.698485: step 1004, loss 0.266051, acc 0.90625, precision 0.714286, recall 0.833333 f1 0.769231\n",
            "2022-11-30T22:19:47.046478: step 1005, loss 0.177059, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:19:48.366778: step 1006, loss 0.161133, acc 0.9375, precision 0.833333, recall 0.833333 f1 0.833333\n",
            "2022-11-30T22:19:49.661277: step 1007, loss 0.166159, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:19:51.003962: step 1008, loss 0.107972, acc 0.953125, precision 0.769231, recall 1 f1 0.869565\n",
            "2022-11-30T22:19:52.612060: step 1009, loss 0.0907921, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:19:54.739779: step 1010, loss 0.0460159, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:19:56.715213: step 1011, loss 0.162651, acc 0.90625, precision 0.833333, recall 0.714286 f1 0.769231\n",
            "2022-11-30T22:19:58.380874: step 1012, loss 0.166617, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:19:59.682872: step 1013, loss 0.0787345, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:20:01.012962: step 1014, loss 0.0875542, acc 0.953125, precision 0.785714, recall 1 f1 0.88\n",
            "2022-11-30T22:20:02.330249: step 1015, loss 0.18666, acc 0.921875, precision 1, recall 0.736842 f1 0.848485\n",
            "2022-11-30T22:20:03.641342: step 1016, loss 0.147479, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:20:04.984804: step 1017, loss 0.128254, acc 0.921875, precision 0.916667, recall 0.733333 f1 0.814815\n",
            "2022-11-30T22:20:06.294205: step 1018, loss 0.182229, acc 0.921875, precision 0.928571, recall 0.764706 f1 0.83871\n",
            "2022-11-30T22:20:07.604819: step 1019, loss 0.0940601, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:20:08.905137: step 1020, loss 0.103637, acc 0.96875, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:20:10.186573: step 1021, loss 0.176711, acc 0.921875, precision 0.933333, recall 0.777778 f1 0.848485\n",
            "2022-11-30T22:20:11.487189: step 1022, loss 0.0792093, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:20:12.818976: step 1023, loss 0.098487, acc 0.9375, precision 0.833333, recall 0.833333 f1 0.833333\n",
            "2022-11-30T22:20:14.101166: step 1024, loss 0.134029, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:20:15.394980: step 1025, loss 0.196001, acc 0.9375, precision 0.842105, recall 0.941176 f1 0.888889\n",
            "2022-11-30T22:20:16.698129: step 1026, loss 0.146728, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:20:17.972587: step 1027, loss 0.166796, acc 0.9375, precision 0.8, recall 1 f1 0.888889\n",
            "2022-11-30T22:20:19.252078: step 1028, loss 0.128432, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:20:20.535188: step 1029, loss 0.0682255, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:20:21.855499: step 1030, loss 0.0313984, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:20:23.184888: step 1031, loss 0.337157, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:20:24.485317: step 1032, loss 0.100746, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:20:25.799622: step 1033, loss 0.224196, acc 0.90625, precision 0.8, recall 0.888889 f1 0.842105\n",
            "2022-11-30T22:20:27.085103: step 1034, loss 0.157592, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:20:28.389463: step 1035, loss 0.0821854, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:20:29.747197: step 1036, loss 0.0972271, acc 0.96875, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T22:20:31.144543: step 1037, loss 0.0944555, acc 0.96875, precision 1, recall 0.904762 f1 0.95\n",
            "2022-11-30T22:20:32.483739: step 1038, loss 0.147209, acc 0.9375, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:20:33.837548: step 1039, loss 0.185962, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:20:34.510757: step 1040, loss 0.241738, acc 0.962963, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:20:35.824494: step 1041, loss 0.0834837, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:20:37.154307: step 1042, loss 0.0808295, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:20:38.454214: step 1043, loss 0.162161, acc 0.921875, precision 0.928571, recall 0.764706 f1 0.83871\n",
            "2022-11-30T22:20:39.771700: step 1044, loss 0.023069, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:20:41.080315: step 1045, loss 0.22338, acc 0.9375, precision 0.75, recall 0.9 f1 0.818182\n",
            "2022-11-30T22:20:42.383288: step 1046, loss 0.0549442, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:20:43.744753: step 1047, loss 0.200615, acc 0.921875, precision 0.863636, recall 0.904762 f1 0.883721\n",
            "2022-11-30T22:20:45.098373: step 1048, loss 0.100965, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:20:46.471240: step 1049, loss 0.0624042, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:20:47.792502: step 1050, loss 0.127297, acc 0.921875, precision 0.882353, recall 0.833333 f1 0.857143\n",
            "2022-11-30T22:20:49.128549: step 1051, loss 0.265455, acc 0.921875, precision 0.928571, recall 0.764706 f1 0.83871\n",
            "2022-11-30T22:20:50.466081: step 1052, loss 0.111665, acc 0.9375, precision 0.866667, recall 0.866667 f1 0.866667\n",
            "2022-11-30T22:20:51.819483: step 1053, loss 0.141821, acc 0.953125, precision 0.9, recall 0.818182 f1 0.857143\n",
            "2022-11-30T22:20:53.125973: step 1054, loss 0.0731014, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:20:54.440682: step 1055, loss 0.110658, acc 0.953125, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:20:55.824796: step 1056, loss 0.169526, acc 0.921875, precision 0.875, recall 0.636364 f1 0.736842\n",
            "2022-11-30T22:20:57.212403: step 1057, loss 0.144855, acc 0.921875, precision 0.833333, recall 0.882353 f1 0.857143\n",
            "2022-11-30T22:20:58.512926: step 1058, loss 0.0848533, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:20:59.846852: step 1059, loss 0.129398, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:21:01.174656: step 1060, loss 0.121747, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:21:02.509343: step 1061, loss 0.123416, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:21:03.872875: step 1062, loss 0.108026, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:21:05.174100: step 1063, loss 0.161783, acc 0.9375, precision 1, recall 0.714286 f1 0.833333\n",
            "2022-11-30T22:21:06.554748: step 1064, loss 0.137325, acc 0.953125, precision 1, recall 0.863636 f1 0.926829\n",
            "2022-11-30T22:21:07.878853: step 1065, loss 0.144147, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:21:09.178158: step 1066, loss 0.107749, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:21:10.455298: step 1067, loss 0.0998221, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:21:11.764297: step 1068, loss 0.0753451, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:21:13.088500: step 1069, loss 0.125519, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:21:14.379356: step 1070, loss 0.102052, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:21:15.706148: step 1071, loss 0.130261, acc 0.96875, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:21:16.989248: step 1072, loss 0.0959135, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:21:18.310692: step 1073, loss 0.118693, acc 0.9375, precision 0.777778, recall 1 f1 0.875\n",
            "2022-11-30T22:21:19.625112: step 1074, loss 0.138291, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:21:20.932253: step 1075, loss 0.0391847, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:21:22.259159: step 1076, loss 0.0688062, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:21:23.578548: step 1077, loss 0.11398, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:21:24.904562: step 1078, loss 0.141155, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:21:26.254514: step 1079, loss 0.139042, acc 0.9375, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T22:21:27.531197: step 1080, loss 0.0656404, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:21:28.819409: step 1081, loss 0.174425, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:21:30.101072: step 1082, loss 0.0535128, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:21:31.386066: step 1083, loss 0.0857863, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:21:32.708592: step 1084, loss 0.155152, acc 0.921875, precision 0.894737, recall 0.85 f1 0.871795\n",
            "2022-11-30T22:21:33.999957: step 1085, loss 0.146829, acc 0.9375, precision 0.785714, recall 0.916667 f1 0.846154\n",
            "2022-11-30T22:21:35.281478: step 1086, loss 0.14421, acc 0.953125, precision 0.7, recall 1 f1 0.823529\n",
            "2022-11-30T22:21:36.580940: step 1087, loss 0.174673, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:21:37.862388: step 1088, loss 0.067794, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:21:39.143005: step 1089, loss 0.0699756, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:21:40.428308: step 1090, loss 0.171795, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:21:41.720946: step 1091, loss 0.306255, acc 0.9375, precision 0.9, recall 0.75 f1 0.818182\n",
            "2022-11-30T22:21:43.030061: step 1092, loss 0.0823027, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:21:44.318538: step 1093, loss 0.176352, acc 0.9375, precision 0.888889, recall 0.888889 f1 0.888889\n",
            "2022-11-30T22:21:45.634641: step 1094, loss 0.101683, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:21:46.942363: step 1095, loss 0.155705, acc 0.921875, precision 0.928571, recall 0.764706 f1 0.83871\n",
            "2022-11-30T22:21:48.236185: step 1096, loss 0.124956, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:21:49.530357: step 1097, loss 0.12373, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:21:50.841266: step 1098, loss 0.11072, acc 0.953125, precision 0.846154, recall 0.916667 f1 0.88\n",
            "2022-11-30T22:21:52.165503: step 1099, loss 0.146997, acc 0.921875, precision 0.823529, recall 0.875 f1 0.848485\n",
            "2022-11-30T22:21:53.522105: step 1100, loss 0.0281642, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:21:57.189812: step 1100, loss 0.418701, acc 0.847619, precision 0.785714, recall 0.572917 f1 0.662651\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1100\n",
            "\n",
            "2022-11-30T22:21:58.619175: step 1101, loss 0.110944, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:21:59.909192: step 1102, loss 0.106182, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:22:01.212978: step 1103, loss 0.139476, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:22:02.481959: step 1104, loss 0.0835553, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:22:03.781706: step 1105, loss 0.180756, acc 0.921875, precision 0.894737, recall 0.85 f1 0.871795\n",
            "2022-11-30T22:22:05.071383: step 1106, loss 0.0917158, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:22:06.349500: step 1107, loss 0.104078, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:22:07.646332: step 1108, loss 0.10386, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:22:08.946542: step 1109, loss 0.0341739, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:22:10.237406: step 1110, loss 0.156133, acc 0.953125, precision 1, recall 0.769231 f1 0.869565\n",
            "2022-11-30T22:22:11.545623: step 1111, loss 0.0703647, acc 0.96875, precision 1, recall 0.904762 f1 0.95\n",
            "2022-11-30T22:22:12.857431: step 1112, loss 0.0799348, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:22:14.151918: step 1113, loss 0.06857, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:22:15.465552: step 1114, loss 0.0807019, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:22:16.769052: step 1115, loss 0.0482711, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:22:18.056924: step 1116, loss 0.0947694, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:22:19.379064: step 1117, loss 0.101172, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:22:20.731149: step 1118, loss 0.022786, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:22:22.038085: step 1119, loss 0.0703557, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:22:23.364826: step 1120, loss 0.0543563, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:22:24.671122: step 1121, loss 0.209299, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:22:25.990381: step 1122, loss 0.076267, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:22:27.339067: step 1123, loss 0.195602, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:22:28.629692: step 1124, loss 0.124721, acc 0.953125, precision 1, recall 0.85 f1 0.918919\n",
            "2022-11-30T22:22:29.909026: step 1125, loss 0.162991, acc 0.9375, precision 0.85, recall 0.944444 f1 0.894737\n",
            "2022-11-30T22:22:31.171805: step 1126, loss 0.122492, acc 0.9375, precision 0.714286, recall 1 f1 0.833333\n",
            "2022-11-30T22:22:32.490889: step 1127, loss 0.0557369, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:22:33.856368: step 1128, loss 0.0309832, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:22:35.166743: step 1129, loss 0.140152, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:22:36.475342: step 1130, loss 0.185786, acc 0.90625, precision 0.863636, recall 0.863636 f1 0.863636\n",
            "2022-11-30T22:22:37.773709: step 1131, loss 0.157259, acc 0.9375, precision 0.9, recall 0.9 f1 0.9\n",
            "2022-11-30T22:22:39.082648: step 1132, loss 0.120049, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:22:40.400176: step 1133, loss 0.0722798, acc 0.96875, precision 0.866667, recall 1 f1 0.928571\n",
            "2022-11-30T22:22:41.710951: step 1134, loss 0.159256, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:22:43.001679: step 1135, loss 0.0950065, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:22:44.338740: step 1136, loss 0.206654, acc 0.9375, precision 0.857143, recall 0.947368 f1 0.9\n",
            "2022-11-30T22:22:45.666225: step 1137, loss 0.119193, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:22:46.986339: step 1138, loss 0.199488, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:22:48.313854: step 1139, loss 0.129043, acc 0.9375, precision 0.818182, recall 1 f1 0.9\n",
            "2022-11-30T22:22:49.624792: step 1140, loss 0.0631505, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:22:50.932210: step 1141, loss 0.185831, acc 0.953125, precision 0.863636, recall 1 f1 0.926829\n",
            "2022-11-30T22:22:52.249225: step 1142, loss 0.0372184, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:22:53.587553: step 1143, loss 0.0524879, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:22:54.268815: step 1144, loss 0.2162, acc 0.888889, precision 0.8, recall 0.666667 f1 0.727273\n",
            "2022-11-30T22:22:55.597961: step 1145, loss 0.0673314, acc 0.953125, precision 1, recall 0.7 f1 0.823529\n",
            "2022-11-30T22:22:56.895496: step 1146, loss 0.0893693, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:22:58.402643: step 1147, loss 0.0288843, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:00.503428: step 1148, loss 0.146445, acc 0.9375, precision 0.894737, recall 0.894737 f1 0.894737\n",
            "2022-11-30T22:23:02.536824: step 1149, loss 0.03508, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:23:04.157547: step 1150, loss 0.0827384, acc 0.953125, precision 0.846154, recall 0.916667 f1 0.88\n",
            "2022-11-30T22:23:05.471000: step 1151, loss 0.0750796, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:23:06.798766: step 1152, loss 0.171255, acc 0.890625, precision 0.888889, recall 0.571429 f1 0.695652\n",
            "2022-11-30T22:23:08.121728: step 1153, loss 0.0834489, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:23:09.403598: step 1154, loss 0.124398, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:23:10.698399: step 1155, loss 0.180514, acc 0.921875, precision 0.875, recall 0.823529 f1 0.848485\n",
            "2022-11-30T22:23:12.020514: step 1156, loss 0.130414, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:23:13.311912: step 1157, loss 0.0784598, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:23:14.589949: step 1158, loss 0.0504877, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:23:15.891937: step 1159, loss 0.241018, acc 0.96875, precision 0.913043, recall 1 f1 0.954545\n",
            "2022-11-30T22:23:17.200085: step 1160, loss 0.0856047, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:23:18.481696: step 1161, loss 0.099261, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:23:19.761399: step 1162, loss 0.104551, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:23:21.044806: step 1163, loss 0.149624, acc 0.9375, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:23:22.375313: step 1164, loss 0.145835, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:23:23.684012: step 1165, loss 0.103719, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:23:24.976677: step 1166, loss 0.0811366, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:23:26.259618: step 1167, loss 0.0539022, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:23:27.623904: step 1168, loss 0.0931486, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:23:28.931172: step 1169, loss 0.126791, acc 0.921875, precision 0.952381, recall 0.833333 f1 0.888889\n",
            "2022-11-30T22:23:30.245874: step 1170, loss 0.0962957, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:23:31.522737: step 1171, loss 0.138424, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:23:32.805975: step 1172, loss 0.0338432, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:23:34.169140: step 1173, loss 0.115697, acc 0.953125, precision 0.904762, recall 0.95 f1 0.926829\n",
            "2022-11-30T22:23:35.503444: step 1174, loss 0.0614814, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:23:36.830655: step 1175, loss 0.149908, acc 0.953125, precision 0.842105, recall 1 f1 0.914286\n",
            "2022-11-30T22:23:38.156211: step 1176, loss 0.0637292, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:23:39.474702: step 1177, loss 0.14649, acc 0.96875, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:23:40.801746: step 1178, loss 0.0706493, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:23:42.147920: step 1179, loss 0.0225495, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:43.475367: step 1180, loss 0.0738024, acc 0.953125, precision 0.9, recall 0.818182 f1 0.857143\n",
            "2022-11-30T22:23:44.754109: step 1181, loss 0.0726779, acc 0.96875, precision 0.866667, recall 1 f1 0.928571\n",
            "2022-11-30T22:23:46.091062: step 1182, loss 0.207875, acc 0.890625, precision 0.9, recall 0.782609 f1 0.837209\n",
            "2022-11-30T22:23:47.400708: step 1183, loss 0.0517113, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:48.705998: step 1184, loss 0.0664898, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:23:50.016624: step 1185, loss 0.107589, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:23:51.300480: step 1186, loss 0.0286568, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:52.645598: step 1187, loss 0.0514522, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:53.954902: step 1188, loss 0.0949828, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:23:55.299600: step 1189, loss 0.0743008, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:23:56.619867: step 1190, loss 0.0316891, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:23:57.951567: step 1191, loss 0.224643, acc 0.9375, precision 1, recall 0.733333 f1 0.846154\n",
            "2022-11-30T22:23:59.277292: step 1192, loss 0.0694664, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:24:00.595529: step 1193, loss 0.0560492, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:24:01.889362: step 1194, loss 0.11913, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:24:03.187741: step 1195, loss 0.114376, acc 0.9375, precision 0.733333, recall 1 f1 0.846154\n",
            "2022-11-30T22:24:04.494171: step 1196, loss 0.0870201, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:24:05.844505: step 1197, loss 0.0217909, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:24:07.198517: step 1198, loss 0.150528, acc 0.90625, precision 0.733333, recall 0.846154 f1 0.785714\n",
            "2022-11-30T22:24:08.532520: step 1199, loss 0.0482826, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:24:09.833131: step 1200, loss 0.0771585, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:24:13.430861: step 1200, loss 0.492099, acc 0.857143, precision 0.871795, recall 0.53125 f1 0.660194\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1200\n",
            "\n",
            "2022-11-30T22:24:14.876408: step 1201, loss 0.166944, acc 0.9375, precision 0.764706, recall 1 f1 0.866667\n",
            "2022-11-30T22:24:16.213041: step 1202, loss 0.0537948, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:24:17.518714: step 1203, loss 0.069795, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:24:18.813021: step 1204, loss 0.0643925, acc 0.953125, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:24:20.118670: step 1205, loss 0.0437655, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:24:21.448375: step 1206, loss 0.0672591, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:24:22.776817: step 1207, loss 0.0521135, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:24:24.068066: step 1208, loss 0.146256, acc 0.953125, precision 1, recall 0.85 f1 0.918919\n",
            "2022-11-30T22:24:25.337146: step 1209, loss 0.147997, acc 0.9375, precision 0.947368, recall 0.857143 f1 0.9\n",
            "2022-11-30T22:24:26.644367: step 1210, loss 0.0652954, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:24:27.953579: step 1211, loss 0.074205, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:24:29.305500: step 1212, loss 0.0218565, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:24:30.579860: step 1213, loss 0.197734, acc 0.90625, precision 0.846154, recall 0.916667 f1 0.88\n",
            "2022-11-30T22:24:31.877434: step 1214, loss 0.0639632, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:24:33.179677: step 1215, loss 0.166562, acc 0.953125, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:24:34.490353: step 1216, loss 0.0574076, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:24:35.811673: step 1217, loss 0.157595, acc 0.9375, precision 0.764706, recall 1 f1 0.866667\n",
            "2022-11-30T22:24:37.140368: step 1218, loss 0.116853, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:24:38.439693: step 1219, loss 0.088731, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:24:39.729469: step 1220, loss 0.0541785, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:24:41.006060: step 1221, loss 0.0888813, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:24:42.323444: step 1222, loss 0.0543781, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:24:43.633387: step 1223, loss 0.153105, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:24:44.930975: step 1224, loss 0.116507, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:24:46.242957: step 1225, loss 0.0653869, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:24:47.540235: step 1226, loss 0.0842543, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:24:48.832915: step 1227, loss 0.0662545, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:24:50.124925: step 1228, loss 0.0648391, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:24:51.441422: step 1229, loss 0.102262, acc 0.9375, precision 1, recall 0.733333 f1 0.846154\n",
            "2022-11-30T22:24:52.778510: step 1230, loss 0.0520332, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:24:54.088044: step 1231, loss 0.0255042, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:24:55.428867: step 1232, loss 0.081821, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:24:56.757692: step 1233, loss 0.115446, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:24:58.039360: step 1234, loss 0.0576331, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:24:59.415386: step 1235, loss 0.0735392, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:25:00.728835: step 1236, loss 0.0631762, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:25:02.034490: step 1237, loss 0.118518, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:25:03.322851: step 1238, loss 0.0477488, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:25:04.626127: step 1239, loss 0.111443, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:25:05.917507: step 1240, loss 0.0420281, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:25:07.253564: step 1241, loss 0.0737623, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:25:08.559456: step 1242, loss 0.109719, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:25:09.854351: step 1243, loss 0.0993298, acc 0.9375, precision 0.894737, recall 0.894737 f1 0.894737\n",
            "2022-11-30T22:25:11.153036: step 1244, loss 0.0644956, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:25:12.453005: step 1245, loss 0.127842, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:25:13.764738: step 1246, loss 0.0442693, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:25:15.049712: step 1247, loss 0.0377422, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:25:15.701314: step 1248, loss 0.14494, acc 0.962963, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:25:16.973729: step 1249, loss 0.136225, acc 0.953125, precision 0.857143, recall 0.923077 f1 0.888889\n",
            "2022-11-30T22:25:18.249010: step 1250, loss 0.164032, acc 0.90625, precision 0.833333, recall 0.833333 f1 0.833333\n",
            "2022-11-30T22:25:19.540409: step 1251, loss 0.0553143, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:25:20.846369: step 1252, loss 0.0890107, acc 0.953125, precision 0.823529, recall 1 f1 0.903226\n",
            "2022-11-30T22:25:22.163354: step 1253, loss 0.0419682, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:25:23.510040: step 1254, loss 0.0523025, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:25:24.821057: step 1255, loss 0.0498235, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:25:26.122538: step 1256, loss 0.175648, acc 0.9375, precision 0.884615, recall 0.958333 f1 0.92\n",
            "2022-11-30T22:25:27.440778: step 1257, loss 0.0481711, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:25:28.748911: step 1258, loss 0.117932, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:25:30.080694: step 1259, loss 0.16434, acc 0.9375, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:25:31.368790: step 1260, loss 0.0364224, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:25:32.644782: step 1261, loss 0.169394, acc 0.921875, precision 0.866667, recall 0.8125 f1 0.83871\n",
            "2022-11-30T22:25:33.962123: step 1262, loss 0.087537, acc 0.96875, precision 0.818182, recall 1 f1 0.9\n",
            "2022-11-30T22:25:35.270764: step 1263, loss 0.0840743, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:25:36.618647: step 1264, loss 0.0487382, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:25:37.947443: step 1265, loss 0.134933, acc 0.953125, precision 0.833333, recall 0.909091 f1 0.869565\n",
            "2022-11-30T22:25:39.241075: step 1266, loss 0.0818118, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:25:40.542712: step 1267, loss 0.0639118, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:25:41.864627: step 1268, loss 0.0616477, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:25:43.167853: step 1269, loss 0.135062, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:25:44.497571: step 1270, loss 0.048011, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:25:45.811927: step 1271, loss 0.134034, acc 0.9375, precision 1, recall 0.789474 f1 0.882353\n",
            "2022-11-30T22:25:47.107345: step 1272, loss 0.0430204, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:25:48.424361: step 1273, loss 0.0818219, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:25:49.716297: step 1274, loss 0.0751578, acc 0.96875, precision 0.954545, recall 0.954545 f1 0.954545\n",
            "2022-11-30T22:25:51.006212: step 1275, loss 0.0683331, acc 0.984375, precision 0.956522, recall 1 f1 0.977778\n",
            "2022-11-30T22:25:52.325508: step 1276, loss 0.0610485, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:25:53.633132: step 1277, loss 0.118483, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:25:54.950607: step 1278, loss 0.0609152, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:25:56.319437: step 1279, loss 0.0808765, acc 0.9375, precision 0.764706, recall 1 f1 0.866667\n",
            "2022-11-30T22:25:57.659919: step 1280, loss 0.109596, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:25:58.960870: step 1281, loss 0.0512002, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:26:00.314600: step 1282, loss 0.179251, acc 0.953125, precision 0.941176, recall 0.888889 f1 0.914286\n",
            "2022-11-30T22:26:01.591231: step 1283, loss 0.0489853, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:26:03.197525: step 1284, loss 0.0388121, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:26:05.307253: step 1285, loss 0.0212571, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:26:07.307352: step 1286, loss 0.0486537, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:26:08.878639: step 1287, loss 0.0826271, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:26:10.191971: step 1288, loss 0.0676276, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:26:11.499420: step 1289, loss 0.0403247, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:26:12.829511: step 1290, loss 0.0634829, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:26:14.135113: step 1291, loss 0.0407386, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:26:15.454633: step 1292, loss 0.0865614, acc 0.9375, precision 0.913043, recall 0.913043 f1 0.913043\n",
            "2022-11-30T22:26:16.742300: step 1293, loss 0.094422, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:26:18.027005: step 1294, loss 0.146653, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:26:19.311661: step 1295, loss 0.080709, acc 0.953125, precision 0.95, recall 0.904762 f1 0.926829\n",
            "2022-11-30T22:26:20.611847: step 1296, loss 0.0858481, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:26:21.898430: step 1297, loss 0.0674198, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:26:23.220464: step 1298, loss 0.0420576, acc 0.984375, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:26:24.494931: step 1299, loss 0.0886721, acc 0.953125, precision 0.916667, recall 0.846154 f1 0.88\n",
            "2022-11-30T22:26:25.762508: step 1300, loss 0.0403205, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:26:29.390655: step 1300, loss 0.493047, acc 0.854422, precision 0.84, recall 0.546875 f1 0.662461\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1300\n",
            "\n",
            "2022-11-30T22:26:30.907147: step 1301, loss 0.115791, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:26:32.224044: step 1302, loss 0.0805985, acc 0.96875, precision 0.818182, recall 1 f1 0.9\n",
            "2022-11-30T22:26:33.552799: step 1303, loss 0.107113, acc 0.96875, precision 0.888889, recall 0.888889 f1 0.888889\n",
            "2022-11-30T22:26:34.878112: step 1304, loss 0.10235, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:26:36.195943: step 1305, loss 0.0850598, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:26:37.510407: step 1306, loss 0.0600519, acc 0.96875, precision 0.952381, recall 0.952381 f1 0.952381\n",
            "2022-11-30T22:26:38.842110: step 1307, loss 0.0220852, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:26:40.158448: step 1308, loss 0.0314302, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:26:41.477383: step 1309, loss 0.0785631, acc 0.953125, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:26:42.803462: step 1310, loss 0.134481, acc 0.953125, precision 0.958333, recall 0.92 f1 0.938776\n",
            "2022-11-30T22:26:44.100354: step 1311, loss 0.0975908, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:26:45.410799: step 1312, loss 0.180276, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:26:46.721709: step 1313, loss 0.130191, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:26:48.020461: step 1314, loss 0.0374807, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:26:49.361017: step 1315, loss 0.0688073, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:26:50.697119: step 1316, loss 0.131105, acc 0.9375, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:26:52.022141: step 1317, loss 0.0819636, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:26:53.373119: step 1318, loss 0.244489, acc 0.9375, precision 0.8, recall 0.8 f1 0.8\n",
            "2022-11-30T22:26:54.671471: step 1319, loss 0.0754924, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:26:56.028248: step 1320, loss 0.118392, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:26:57.391982: step 1321, loss 0.163509, acc 0.9375, precision 0.857143, recall 0.666667 f1 0.75\n",
            "2022-11-30T22:26:58.709553: step 1322, loss 0.0969142, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:27:00.021368: step 1323, loss 0.0429876, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:27:01.352939: step 1324, loss 0.0770369, acc 0.953125, precision 0.8, recall 1 f1 0.888889\n",
            "2022-11-30T22:27:02.672024: step 1325, loss 0.134172, acc 0.96875, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:27:03.956087: step 1326, loss 0.0403601, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:27:05.287311: step 1327, loss 0.139924, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:27:06.616512: step 1328, loss 0.0509323, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:07.937579: step 1329, loss 0.0217665, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:09.245715: step 1330, loss 0.0756361, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:27:10.533507: step 1331, loss 0.0530328, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:27:11.833996: step 1332, loss 0.0175403, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:13.158801: step 1333, loss 0.0277989, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:14.455292: step 1334, loss 0.0797246, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:27:15.729869: step 1335, loss 0.063708, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:27:17.038608: step 1336, loss 0.0954304, acc 0.9375, precision 0.85, recall 0.944444 f1 0.894737\n",
            "2022-11-30T22:27:18.326802: step 1337, loss 0.158366, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:27:19.604824: step 1338, loss 0.128178, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:27:20.915891: step 1339, loss 0.0389931, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:27:22.203775: step 1340, loss 0.02466, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:23.543589: step 1341, loss 0.0271421, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:24.821539: step 1342, loss 0.121924, acc 0.953125, precision 0.9, recall 0.818182 f1 0.857143\n",
            "2022-11-30T22:27:26.104818: step 1343, loss 0.083292, acc 0.984375, precision 1, recall 0.961538 f1 0.980392\n",
            "2022-11-30T22:27:27.391624: step 1344, loss 0.0417682, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:27:28.677571: step 1345, loss 0.0288549, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:29.985934: step 1346, loss 0.0856349, acc 0.953125, precision 0.863636, recall 1 f1 0.926829\n",
            "2022-11-30T22:27:31.320972: step 1347, loss 0.0800372, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:27:32.653570: step 1348, loss 0.0401767, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:27:33.941024: step 1349, loss 0.107467, acc 0.9375, precision 0.888889, recall 0.888889 f1 0.888889\n",
            "2022-11-30T22:27:35.249879: step 1350, loss 0.0642653, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:27:36.545828: step 1351, loss 0.0973979, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:27:37.196696: step 1352, loss 0.228026, acc 0.925926, precision 1, recall 0.777778 f1 0.875\n",
            "2022-11-30T22:27:38.487921: step 1353, loss 0.096766, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:27:39.812701: step 1354, loss 0.00751416, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:41.155611: step 1355, loss 0.0726183, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:27:42.450362: step 1356, loss 0.139633, acc 0.953125, precision 1, recall 0.85 f1 0.918919\n",
            "2022-11-30T22:27:43.746043: step 1357, loss 0.0605189, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:27:45.030405: step 1358, loss 0.0355394, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:46.334596: step 1359, loss 0.0444197, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:27:47.624568: step 1360, loss 0.165848, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:27:48.937217: step 1361, loss 0.0606958, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:27:50.267019: step 1362, loss 0.0499001, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:51.575174: step 1363, loss 0.118666, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:27:52.954892: step 1364, loss 0.0660065, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:27:54.229627: step 1365, loss 0.0190462, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:55.550169: step 1366, loss 0.0995054, acc 0.96875, precision 0.866667, recall 1 f1 0.928571\n",
            "2022-11-30T22:27:56.834548: step 1367, loss 0.0312396, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:27:58.144859: step 1368, loss 0.0525138, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:27:59.457201: step 1369, loss 0.08719, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:28:00.757122: step 1370, loss 0.104965, acc 0.953125, precision 0.923077, recall 0.857143 f1 0.888889\n",
            "2022-11-30T22:28:02.094914: step 1371, loss 0.031625, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:28:03.397415: step 1372, loss 0.10665, acc 0.953125, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:28:04.706132: step 1373, loss 0.043588, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:28:06.016430: step 1374, loss 0.107075, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:28:07.315579: step 1375, loss 0.0503576, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:28:08.628084: step 1376, loss 0.134871, acc 0.953125, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T22:28:09.959632: step 1377, loss 0.0439739, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:28:11.258677: step 1378, loss 0.142749, acc 0.9375, precision 1, recall 0.764706 f1 0.866667\n",
            "2022-11-30T22:28:12.547482: step 1379, loss 0.134185, acc 0.953125, precision 1, recall 0.85 f1 0.918919\n",
            "2022-11-30T22:28:13.855855: step 1380, loss 0.0384926, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:28:15.141396: step 1381, loss 0.0238889, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:28:16.445695: step 1382, loss 0.107547, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:28:17.745168: step 1383, loss 0.0359271, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:28:19.060039: step 1384, loss 0.0736487, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:28:20.359884: step 1385, loss 0.0938284, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:28:21.657813: step 1386, loss 0.0585449, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:28:22.993431: step 1387, loss 0.169092, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:28:24.290463: step 1388, loss 0.0862729, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:28:25.564373: step 1389, loss 0.0727349, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:28:26.823156: step 1390, loss 0.125042, acc 0.9375, precision 0.9375, recall 0.833333 f1 0.882353\n",
            "2022-11-30T22:28:28.095577: step 1391, loss 0.0259507, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:28:29.378526: step 1392, loss 0.0380171, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:28:30.667548: step 1393, loss 0.0364056, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:28:31.953447: step 1394, loss 0.0909977, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:28:33.269510: step 1395, loss 0.0271745, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:28:34.552503: step 1396, loss 0.138986, acc 0.96875, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:28:35.837965: step 1397, loss 0.158881, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:28:37.128024: step 1398, loss 0.0977158, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:28:38.381733: step 1399, loss 0.156363, acc 0.953125, precision 0.916667, recall 0.846154 f1 0.88\n",
            "2022-11-30T22:28:39.697927: step 1400, loss 0.0373154, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:28:43.333955: step 1400, loss 0.585212, acc 0.85034, precision 0.894231, recall 0.484375 f1 0.628378\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1400\n",
            "\n",
            "2022-11-30T22:28:44.748494: step 1401, loss 0.0699891, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:28:46.037927: step 1402, loss 0.0838659, acc 0.953125, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:28:47.312573: step 1403, loss 0.0592324, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:28:48.612249: step 1404, loss 0.0582002, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:28:49.945020: step 1405, loss 0.0362818, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:28:51.242023: step 1406, loss 0.0512219, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:28:52.548991: step 1407, loss 0.0518102, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:28:53.875470: step 1408, loss 0.103287, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:28:55.180244: step 1409, loss 0.0997631, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:28:56.502217: step 1410, loss 0.0515707, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:28:57.794003: step 1411, loss 0.0544306, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:28:59.146372: step 1412, loss 0.0360383, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:00.418681: step 1413, loss 0.196328, acc 0.9375, precision 0.764706, recall 1 f1 0.866667\n",
            "2022-11-30T22:29:01.707013: step 1414, loss 0.0384842, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:03.112652: step 1415, loss 0.0442633, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:29:04.446702: step 1416, loss 0.101046, acc 0.953125, precision 0.904762, recall 0.95 f1 0.926829\n",
            "2022-11-30T22:29:05.721174: step 1417, loss 0.0383293, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:29:07.805089: step 1418, loss 0.154857, acc 0.953125, precision 1, recall 0.785714 f1 0.88\n",
            "2022-11-30T22:29:09.910781: step 1419, loss 0.0512341, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:11.774865: step 1420, loss 0.065052, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:29:13.086256: step 1421, loss 0.0620914, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:29:14.406207: step 1422, loss 0.0353957, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:29:15.679494: step 1423, loss 0.0609148, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:29:16.951089: step 1424, loss 0.0631099, acc 0.96875, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:29:18.241787: step 1425, loss 0.0635508, acc 0.96875, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:29:19.515759: step 1426, loss 0.12898, acc 0.9375, precision 0.894737, recall 0.894737 f1 0.894737\n",
            "2022-11-30T22:29:20.847820: step 1427, loss 0.0256545, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:29:22.154102: step 1428, loss 0.0446467, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:29:23.483868: step 1429, loss 0.115212, acc 0.953125, precision 0.95, recall 0.904762 f1 0.926829\n",
            "2022-11-30T22:29:24.778794: step 1430, loss 0.120116, acc 0.921875, precision 0.785714, recall 0.846154 f1 0.814815\n",
            "2022-11-30T22:29:26.061884: step 1431, loss 0.0793179, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:29:27.344480: step 1432, loss 0.0415627, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:29:28.636437: step 1433, loss 0.0660392, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:29:29.943401: step 1434, loss 0.111381, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:29:31.232689: step 1435, loss 0.0545215, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:29:32.503543: step 1436, loss 0.118542, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:29:33.881114: step 1437, loss 0.0770823, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:29:35.189595: step 1438, loss 0.0285757, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:36.520693: step 1439, loss 0.0829647, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:29:37.785016: step 1440, loss 0.0665854, acc 0.96875, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:29:39.093700: step 1441, loss 0.0581716, acc 0.96875, precision 0.818182, recall 1 f1 0.9\n",
            "2022-11-30T22:29:40.379532: step 1442, loss 0.104753, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:29:41.725341: step 1443, loss 0.137399, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:29:43.037120: step 1444, loss 0.0475606, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:44.353456: step 1445, loss 0.033379, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:29:45.625617: step 1446, loss 0.1717, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:29:46.932515: step 1447, loss 0.0447074, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:29:48.230190: step 1448, loss 0.107166, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:29:49.543143: step 1449, loss 0.107202, acc 0.953125, precision 0.875, recall 0.933333 f1 0.903226\n",
            "2022-11-30T22:29:50.894298: step 1450, loss 0.108728, acc 0.953125, precision 0.916667, recall 0.846154 f1 0.88\n",
            "2022-11-30T22:29:52.235971: step 1451, loss 0.0612732, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:29:53.597790: step 1452, loss 0.030439, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:29:54.894816: step 1453, loss 0.0588044, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:29:56.210301: step 1454, loss 0.0472616, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:29:57.516919: step 1455, loss 0.058141, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:29:58.163691: step 1456, loss 0.0574615, acc 0.962963, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:29:59.459311: step 1457, loss 0.108879, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:30:00.763053: step 1458, loss 0.0263606, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:30:02.044588: step 1459, loss 0.0664932, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:30:03.404870: step 1460, loss 0.0876797, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:30:04.717334: step 1461, loss 0.0675963, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:30:05.986519: step 1462, loss 0.0225844, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:07.283470: step 1463, loss 0.0415502, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:08.595519: step 1464, loss 0.0148814, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:09.913857: step 1465, loss 0.0689198, acc 0.953125, precision 0.928571, recall 0.866667 f1 0.896552\n",
            "2022-11-30T22:30:11.238742: step 1466, loss 0.053073, acc 0.96875, precision 1, recall 0.904762 f1 0.95\n",
            "2022-11-30T22:30:12.566232: step 1467, loss 0.0289147, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:30:13.881883: step 1468, loss 0.0638047, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:30:15.148270: step 1469, loss 0.0262551, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:16.427199: step 1470, loss 0.0389304, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:30:17.719462: step 1471, loss 0.0773318, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:30:19.029978: step 1472, loss 0.11428, acc 0.96875, precision 0.894737, recall 1 f1 0.944444\n",
            "2022-11-30T22:30:20.337417: step 1473, loss 0.0427882, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:21.647701: step 1474, loss 0.0434831, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:30:22.971021: step 1475, loss 0.100989, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:30:24.289046: step 1476, loss 0.0963041, acc 0.9375, precision 0.714286, recall 0.714286 f1 0.714286\n",
            "2022-11-30T22:30:25.568615: step 1477, loss 0.080546, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:30:26.856252: step 1478, loss 0.056062, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:30:28.153209: step 1479, loss 0.0480562, acc 0.984375, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:30:29.464095: step 1480, loss 0.0835136, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:30:30.760408: step 1481, loss 0.0633314, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:30:32.058301: step 1482, loss 0.0335102, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:30:33.352227: step 1483, loss 0.0674722, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:30:34.673751: step 1484, loss 0.0717346, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:30:36.015396: step 1485, loss 0.108076, acc 0.921875, precision 1, recall 0.821429 f1 0.901961\n",
            "2022-11-30T22:30:37.316026: step 1486, loss 0.0142269, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:38.630812: step 1487, loss 0.0335258, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:30:39.983558: step 1488, loss 0.0334352, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:41.331119: step 1489, loss 0.0159275, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:30:42.691203: step 1490, loss 0.0470299, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:30:44.006712: step 1491, loss 0.0551886, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T22:30:45.302882: step 1492, loss 0.116655, acc 0.984375, precision 0.958333, recall 1 f1 0.978723\n",
            "2022-11-30T22:30:46.596644: step 1493, loss 0.0515516, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:30:47.904011: step 1494, loss 0.0270864, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:30:49.197726: step 1495, loss 0.0391283, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:30:50.494024: step 1496, loss 0.0430866, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:30:51.817826: step 1497, loss 0.0863795, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:30:53.151285: step 1498, loss 0.051597, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:30:54.462275: step 1499, loss 0.0774856, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:30:55.784014: step 1500, loss 0.149172, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:30:59.445117: step 1500, loss 0.527043, acc 0.857143, precision 0.865546, recall 0.536458 f1 0.662379\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1500\n",
            "\n",
            "2022-11-30T22:31:00.886704: step 1501, loss 0.0571446, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:31:02.177098: step 1502, loss 0.0258658, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:31:03.450506: step 1503, loss 0.0731915, acc 0.953125, precision 0.866667, recall 0.928571 f1 0.896552\n",
            "2022-11-30T22:31:04.788976: step 1504, loss 0.0186821, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:06.087208: step 1505, loss 0.0367163, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:31:07.439091: step 1506, loss 0.0367591, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:08.742788: step 1507, loss 0.0381155, acc 0.984375, precision 0.958333, recall 1 f1 0.978723\n",
            "2022-11-30T22:31:10.056021: step 1508, loss 0.0444512, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:31:11.369925: step 1509, loss 0.0146873, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:12.684090: step 1510, loss 0.0516964, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:31:13.973968: step 1511, loss 0.0992365, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:31:15.326109: step 1512, loss 0.0592434, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:16.672351: step 1513, loss 0.0253453, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:17.961014: step 1514, loss 0.0149137, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:19.272751: step 1515, loss 0.0157111, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:20.596267: step 1516, loss 0.0804657, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:31:21.933023: step 1517, loss 0.0148037, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:23.255978: step 1518, loss 0.0461596, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:31:24.603822: step 1519, loss 0.0694072, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:31:25.893066: step 1520, loss 0.107916, acc 0.921875, precision 0.769231, recall 0.833333 f1 0.8\n",
            "2022-11-30T22:31:27.182195: step 1521, loss 0.0626498, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:31:28.461521: step 1522, loss 0.0335251, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:29.727583: step 1523, loss 0.150204, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:31:31.016415: step 1524, loss 0.0270025, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:32.307571: step 1525, loss 0.0301506, acc 0.984375, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:31:33.601366: step 1526, loss 0.102892, acc 0.9375, precision 0.833333, recall 0.9375 f1 0.882353\n",
            "2022-11-30T22:31:34.907335: step 1527, loss 0.0296884, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:31:36.216338: step 1528, loss 0.221048, acc 0.953125, precision 1, recall 0.863636 f1 0.926829\n",
            "2022-11-30T22:31:37.568879: step 1529, loss 0.0968347, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:31:38.907702: step 1530, loss 0.0137122, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:40.230753: step 1531, loss 0.0303018, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:41.531252: step 1532, loss 0.0392204, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:42.815572: step 1533, loss 0.0299404, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:44.161420: step 1534, loss 0.0359807, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:31:45.503762: step 1535, loss 0.0589233, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:31:46.820375: step 1536, loss 0.0495381, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:31:48.139512: step 1537, loss 0.0967339, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:31:49.471288: step 1538, loss 0.0324215, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:31:50.794320: step 1539, loss 0.058006, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:31:52.091175: step 1540, loss 0.0762106, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:31:53.398682: step 1541, loss 0.0301079, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:31:54.723655: step 1542, loss 0.0436428, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:31:56.095486: step 1543, loss 0.0619381, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:31:57.423543: step 1544, loss 0.122242, acc 0.96875, precision 1, recall 0.904762 f1 0.95\n",
            "2022-11-30T22:31:58.766776: step 1545, loss 0.0272923, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:32:00.083628: step 1546, loss 0.0201241, acc 0.984375, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:32:01.390894: step 1547, loss 0.0471587, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:32:02.687801: step 1548, loss 0.0194898, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:04.012993: step 1549, loss 0.0366798, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:32:05.302144: step 1550, loss 0.159886, acc 0.9375, precision 0.944444, recall 0.85 f1 0.894737\n",
            "2022-11-30T22:32:06.620617: step 1551, loss 0.0836063, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:32:07.989445: step 1552, loss 0.0638119, acc 0.953125, precision 0.894737, recall 0.944444 f1 0.918919\n",
            "2022-11-30T22:32:09.313532: step 1553, loss 0.00726721, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:10.705653: step 1554, loss 0.123937, acc 0.953125, precision 0.9375, recall 0.882353 f1 0.909091\n",
            "2022-11-30T22:32:12.981056: step 1555, loss 0.0659872, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:32:15.051767: step 1556, loss 0.0714305, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:32:16.665858: step 1557, loss 0.0585994, acc 0.96875, precision 0.8, recall 1 f1 0.888889\n",
            "2022-11-30T22:32:17.967775: step 1558, loss 0.0953067, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:32:19.280510: step 1559, loss 0.0757789, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:32:19.936840: step 1560, loss 0.0105857, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:21.248205: step 1561, loss 0.0205744, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:22.584730: step 1562, loss 0.0472471, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:32:23.909860: step 1563, loss 0.103012, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:32:25.198781: step 1564, loss 0.0166899, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:26.498046: step 1565, loss 0.072156, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:32:27.796272: step 1566, loss 0.0983673, acc 0.953125, precision 1, recall 0.769231 f1 0.869565\n",
            "2022-11-30T22:32:29.099883: step 1567, loss 0.0300513, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:32:30.398392: step 1568, loss 0.0135021, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:31.718237: step 1569, loss 0.01058, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:33.020075: step 1570, loss 0.0133785, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:34.345468: step 1571, loss 0.0189134, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:35.662362: step 1572, loss 0.0918337, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:32:37.011309: step 1573, loss 0.016061, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:38.324051: step 1574, loss 0.052016, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:32:39.683866: step 1575, loss 0.0186269, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:41.046513: step 1576, loss 0.0393302, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:32:42.369206: step 1577, loss 0.045693, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:32:43.689190: step 1578, loss 0.0272923, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:32:45.041923: step 1579, loss 0.0606623, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:32:46.395481: step 1580, loss 0.0091908, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:47.687548: step 1581, loss 0.0453386, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:32:49.008595: step 1582, loss 0.0455557, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:32:50.353815: step 1583, loss 0.0253695, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:51.675477: step 1584, loss 0.0236936, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:53.035427: step 1585, loss 0.0960351, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:32:54.369669: step 1586, loss 0.0261368, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:32:55.685660: step 1587, loss 0.0732043, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:32:57.056866: step 1588, loss 0.015686, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:32:58.412484: step 1589, loss 0.0406433, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:32:59.727042: step 1590, loss 0.0510609, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:33:01.069719: step 1591, loss 0.019022, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:02.383951: step 1592, loss 0.0432025, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:33:03.699132: step 1593, loss 0.0227025, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:05.005528: step 1594, loss 0.0887723, acc 0.953125, precision 0.941176, recall 0.888889 f1 0.914286\n",
            "2022-11-30T22:33:06.347328: step 1595, loss 0.127014, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:33:07.662280: step 1596, loss 0.0298437, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:09.025315: step 1597, loss 0.13861, acc 0.953125, precision 0.928571, recall 0.866667 f1 0.896552\n",
            "2022-11-30T22:33:10.331447: step 1598, loss 0.0471927, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:11.652119: step 1599, loss 0.105733, acc 0.953125, precision 0.842105, recall 1 f1 0.914286\n",
            "2022-11-30T22:33:12.961428: step 1600, loss 0.0398217, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:33:16.664304: step 1600, loss 0.515525, acc 0.851701, precision 0.81203, recall 0.5625 f1 0.664615\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1600\n",
            "\n",
            "2022-11-30T22:33:18.158175: step 1601, loss 0.0343283, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:33:19.479167: step 1602, loss 0.0956062, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:33:20.809312: step 1603, loss 0.0441734, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:33:22.145221: step 1604, loss 0.0363299, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:33:23.513735: step 1605, loss 0.0335174, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:33:24.848199: step 1606, loss 0.0321863, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:33:26.186233: step 1607, loss 0.087338, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:33:27.554888: step 1608, loss 0.0296762, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:28.884463: step 1609, loss 0.15502, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:33:30.225497: step 1610, loss 0.16372, acc 0.9375, precision 1, recall 0.764706 f1 0.866667\n",
            "2022-11-30T22:33:31.562742: step 1611, loss 0.0262973, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:32.893989: step 1612, loss 0.122017, acc 0.96875, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:33:34.249371: step 1613, loss 0.112808, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:33:35.589792: step 1614, loss 0.151815, acc 0.9375, precision 0.714286, recall 0.714286 f1 0.714286\n",
            "2022-11-30T22:33:36.909866: step 1615, loss 0.0751184, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:33:38.239463: step 1616, loss 0.0492481, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:33:39.610633: step 1617, loss 0.0292694, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:33:40.943815: step 1618, loss 0.0687321, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:33:42.281918: step 1619, loss 0.0737616, acc 0.953125, precision 0.882353, recall 0.9375 f1 0.909091\n",
            "2022-11-30T22:33:43.612046: step 1620, loss 0.0429894, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:33:44.914739: step 1621, loss 0.0237523, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:46.240140: step 1622, loss 0.119888, acc 0.9375, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T22:33:47.576694: step 1623, loss 0.0367478, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:48.926395: step 1624, loss 0.0479674, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:33:50.281722: step 1625, loss 0.0738599, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:33:51.591669: step 1626, loss 0.0139283, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:52.916125: step 1627, loss 0.0637478, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:33:54.237147: step 1628, loss 0.0106021, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:55.539162: step 1629, loss 0.0347332, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:56.884871: step 1630, loss 0.0271349, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:33:58.207936: step 1631, loss 0.0485505, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:33:59.488705: step 1632, loss 0.057182, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:34:00.789055: step 1633, loss 0.0830472, acc 0.953125, precision 1, recall 0.863636 f1 0.926829\n",
            "2022-11-30T22:34:02.108352: step 1634, loss 0.0564027, acc 0.96875, precision 1, recall 0.846154 f1 0.916667\n",
            "2022-11-30T22:34:03.408795: step 1635, loss 0.0125471, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:04.695626: step 1636, loss 0.103846, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:34:05.957065: step 1637, loss 0.0440628, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:34:07.257116: step 1638, loss 0.0199074, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:08.540447: step 1639, loss 0.0836558, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:34:09.900637: step 1640, loss 0.0804797, acc 0.96875, precision 0.95, recall 0.95 f1 0.95\n",
            "2022-11-30T22:34:11.230390: step 1641, loss 0.0371624, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:34:12.531458: step 1642, loss 0.0557887, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:34:13.821707: step 1643, loss 0.0618403, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T22:34:15.126641: step 1644, loss 0.0653705, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:34:16.448871: step 1645, loss 0.0869951, acc 0.96875, precision 0.956522, recall 0.956522 f1 0.956522\n",
            "2022-11-30T22:34:17.740782: step 1646, loss 0.00994525, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:19.017599: step 1647, loss 0.051931, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:34:20.361756: step 1648, loss 0.0205978, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:21.642147: step 1649, loss 0.0852573, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:34:22.986812: step 1650, loss 0.00979524, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:24.297657: step 1651, loss 0.132223, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:34:25.586332: step 1652, loss 0.0190054, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:26.896012: step 1653, loss 0.0319852, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:34:28.215319: step 1654, loss 0.0567863, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:34:29.502915: step 1655, loss 0.0289198, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:34:30.799228: step 1656, loss 0.0533738, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:34:32.062346: step 1657, loss 0.0678794, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:34:33.341799: step 1658, loss 0.098941, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:34:34.635216: step 1659, loss 0.0713978, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:34:35.953759: step 1660, loss 0.0584434, acc 0.96875, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:34:37.260318: step 1661, loss 0.0289977, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:34:38.585521: step 1662, loss 0.0495344, acc 0.96875, precision 0.95, recall 0.95 f1 0.95\n",
            "2022-11-30T22:34:39.919416: step 1663, loss 0.0884151, acc 0.953125, precision 0.842105, recall 1 f1 0.914286\n",
            "2022-11-30T22:34:40.611728: step 1664, loss 0.0453993, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:41.945402: step 1665, loss 0.0325247, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:34:43.270634: step 1666, loss 0.0387521, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:34:44.589162: step 1667, loss 0.0931663, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:34:45.916812: step 1668, loss 0.102719, acc 0.9375, precision 0.882353, recall 0.882353 f1 0.882353\n",
            "2022-11-30T22:34:47.273728: step 1669, loss 0.078533, acc 0.953125, precision 0.909091, recall 0.952381 f1 0.930233\n",
            "2022-11-30T22:34:48.626603: step 1670, loss 0.0996555, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:34:49.976396: step 1671, loss 0.0731155, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:34:51.306362: step 1672, loss 0.021609, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:52.628108: step 1673, loss 0.0357831, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:34:53.973732: step 1674, loss 0.0100357, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:55.287453: step 1675, loss 0.0191313, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:34:56.627527: step 1676, loss 0.0864299, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:34:57.950713: step 1677, loss 0.00628087, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:34:59.291752: step 1678, loss 0.0294183, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:35:00.580019: step 1679, loss 0.0394911, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:35:01.866173: step 1680, loss 0.0389505, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:35:03.154300: step 1681, loss 0.0500745, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:35:04.442115: step 1682, loss 0.0259966, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:05.748503: step 1683, loss 0.0444212, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:35:07.055201: step 1684, loss 0.025841, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:35:08.365779: step 1685, loss 0.0810894, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:35:09.676208: step 1686, loss 0.0749962, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:35:11.060905: step 1687, loss 0.0754362, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:35:12.366505: step 1688, loss 0.0271575, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:13.681985: step 1689, loss 0.0317822, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:35:15.324801: step 1690, loss 0.0310043, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:35:17.480406: step 1691, loss 0.0699898, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:35:19.520968: step 1692, loss 0.0537812, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:35:20.948269: step 1693, loss 0.0140163, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:22.234244: step 1694, loss 0.062888, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:35:23.561782: step 1695, loss 0.0381594, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:24.885021: step 1696, loss 0.0511776, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:35:26.185810: step 1697, loss 0.0403266, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:35:27.465429: step 1698, loss 0.016072, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:28.746500: step 1699, loss 0.138722, acc 0.953125, precision 0.916667, recall 0.846154 f1 0.88\n",
            "2022-11-30T22:35:30.056073: step 1700, loss 0.013921, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:35:33.596980: step 1700, loss 0.612638, acc 0.851701, precision 0.837398, recall 0.536458 f1 0.653968\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1700\n",
            "\n",
            "2022-11-30T22:35:35.026019: step 1701, loss 0.00754153, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:36.325087: step 1702, loss 0.0333352, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:35:37.642202: step 1703, loss 0.0130762, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:38.957843: step 1704, loss 0.0431119, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:35:40.285973: step 1705, loss 0.0677659, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:35:41.669229: step 1706, loss 0.111478, acc 0.953125, precision 0.909091, recall 0.833333 f1 0.869565\n",
            "2022-11-30T22:35:43.068920: step 1707, loss 0.0808442, acc 0.96875, precision 1, recall 0.818182 f1 0.9\n",
            "2022-11-30T22:35:44.405049: step 1708, loss 0.00900859, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:45.745958: step 1709, loss 0.0701543, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:35:47.100016: step 1710, loss 0.0279088, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:48.506053: step 1711, loss 0.0315615, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:49.841270: step 1712, loss 0.0211779, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:51.180808: step 1713, loss 0.0799357, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:35:52.470459: step 1714, loss 0.0302207, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:53.774182: step 1715, loss 0.117884, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:35:55.084827: step 1716, loss 0.043367, acc 0.96875, precision 0.866667, recall 1 f1 0.928571\n",
            "2022-11-30T22:35:56.435140: step 1717, loss 0.0304531, acc 0.984375, precision 0.958333, recall 1 f1 0.978723\n",
            "2022-11-30T22:35:57.748010: step 1718, loss 0.0187991, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:35:59.076402: step 1719, loss 0.00606819, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:00.396866: step 1720, loss 0.0186127, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:01.706923: step 1721, loss 0.0817898, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:36:02.994995: step 1722, loss 0.0126389, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:04.306737: step 1723, loss 0.0656096, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:36:05.606966: step 1724, loss 0.00933221, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:06.928412: step 1725, loss 0.0184387, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:08.221478: step 1726, loss 0.0902751, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:36:09.533563: step 1727, loss 0.0637972, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:36:10.862293: step 1728, loss 0.201125, acc 0.96875, precision 0.923077, recall 0.923077 f1 0.923077\n",
            "2022-11-30T22:36:12.208835: step 1729, loss 0.0438158, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:36:13.515589: step 1730, loss 0.0574503, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:36:14.831765: step 1731, loss 0.0673606, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T22:36:16.197012: step 1732, loss 0.0453345, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:36:17.498644: step 1733, loss 0.0184565, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:18.825513: step 1734, loss 0.0310826, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:36:20.163448: step 1735, loss 0.0717545, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:36:21.460668: step 1736, loss 0.0138885, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:22.748706: step 1737, loss 0.0312691, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:24.045089: step 1738, loss 0.0343375, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:36:25.326901: step 1739, loss 0.0356169, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:36:26.597833: step 1740, loss 0.0786928, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:36:27.880304: step 1741, loss 0.024655, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:29.204804: step 1742, loss 0.0555218, acc 0.96875, precision 0.956522, recall 0.956522 f1 0.956522\n",
            "2022-11-30T22:36:30.494468: step 1743, loss 0.0409316, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:36:31.774282: step 1744, loss 0.0609817, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:36:33.087799: step 1745, loss 0.0414769, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:36:34.387066: step 1746, loss 0.0461048, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:36:35.680319: step 1747, loss 0.0939421, acc 0.953125, precision 0.928571, recall 0.866667 f1 0.896552\n",
            "2022-11-30T22:36:36.976103: step 1748, loss 0.111833, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:36:38.281553: step 1749, loss 0.0497845, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:36:39.568646: step 1750, loss 0.0194559, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:40.882824: step 1751, loss 0.0282332, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:36:42.196197: step 1752, loss 0.0181617, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:36:43.578014: step 1753, loss 0.0348351, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:36:44.895903: step 1754, loss 0.0370723, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:36:46.229260: step 1755, loss 0.0389821, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:36:47.524302: step 1756, loss 0.0335178, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:36:48.852849: step 1757, loss 0.010072, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:50.177736: step 1758, loss 0.00965003, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:51.524676: step 1759, loss 0.0698493, acc 0.953125, precision 0.833333, recall 0.909091 f1 0.869565\n",
            "2022-11-30T22:36:52.861565: step 1760, loss 0.0236009, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:54.163448: step 1761, loss 0.0220306, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:55.490131: step 1762, loss 0.109459, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:36:56.815450: step 1763, loss 0.0109318, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:36:58.099045: step 1764, loss 0.182581, acc 0.9375, precision 1, recall 0.75 f1 0.857143\n",
            "2022-11-30T22:36:59.373876: step 1765, loss 0.0260737, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:37:00.698617: step 1766, loss 0.0326201, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:37:02.010276: step 1767, loss 0.0473883, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:37:02.669707: step 1768, loss 0.174091, acc 0.925926, precision 0.857143, recall 0.857143 f1 0.857143\n",
            "2022-11-30T22:37:03.974617: step 1769, loss 0.0241662, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:37:05.275102: step 1770, loss 0.14661, acc 0.953125, precision 1, recall 0.8125 f1 0.896552\n",
            "2022-11-30T22:37:06.613444: step 1771, loss 0.0312264, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:07.888423: step 1772, loss 0.0713667, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:37:09.198747: step 1773, loss 0.0560174, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:37:10.510723: step 1774, loss 0.0381945, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:37:11.852221: step 1775, loss 0.0401942, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:13.218660: step 1776, loss 0.0741997, acc 0.953125, precision 0.9, recall 0.947368 f1 0.923077\n",
            "2022-11-30T22:37:14.547554: step 1777, loss 0.0429599, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:37:15.838722: step 1778, loss 0.0237486, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:17.152226: step 1779, loss 0.02122, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:18.438211: step 1780, loss 0.00963307, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:19.721253: step 1781, loss 0.0197325, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:21.052502: step 1782, loss 0.00990366, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:22.353179: step 1783, loss 0.0259501, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:37:23.656103: step 1784, loss 0.0536033, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:37:24.950865: step 1785, loss 0.0582506, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:37:26.259797: step 1786, loss 0.113573, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:37:27.620327: step 1787, loss 0.0614336, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:37:28.955753: step 1788, loss 0.00796238, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:30.247449: step 1789, loss 0.0533083, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:37:31.570398: step 1790, loss 0.00954025, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:32.908220: step 1791, loss 0.0284268, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:34.209486: step 1792, loss 0.0928162, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:37:35.525072: step 1793, loss 0.0496925, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:37:36.822443: step 1794, loss 0.046958, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:37:38.131565: step 1795, loss 0.0601852, acc 0.984375, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:37:39.443194: step 1796, loss 0.0154946, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:40.753691: step 1797, loss 0.094351, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:37:42.071991: step 1798, loss 0.0128062, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:43.420681: step 1799, loss 0.0181889, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:37:44.818663: step 1800, loss 0.0116525, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:37:48.425782: step 1800, loss 0.519732, acc 0.844898, precision 0.770833, recall 0.578125 f1 0.660714\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1800\n",
            "\n",
            "2022-11-30T22:37:49.903708: step 1801, loss 0.0512394, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:37:51.248763: step 1802, loss 0.0429721, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:52.575874: step 1803, loss 0.0529986, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:37:53.883020: step 1804, loss 0.0139993, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:37:55.225670: step 1805, loss 0.0682362, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:37:56.548410: step 1806, loss 0.037258, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:37:57.860942: step 1807, loss 0.0421696, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:37:59.165266: step 1808, loss 0.0270245, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:00.485528: step 1809, loss 0.0213872, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:01.776751: step 1810, loss 0.0244246, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:03.086395: step 1811, loss 0.00732832, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:04.440060: step 1812, loss 0.0421886, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:38:05.731220: step 1813, loss 0.164095, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:38:07.062054: step 1814, loss 0.0310445, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:38:08.349996: step 1815, loss 0.0145916, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:09.662091: step 1816, loss 0.0932253, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:38:10.974880: step 1817, loss 0.0769232, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:38:12.314612: step 1818, loss 0.0255081, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:13.706382: step 1819, loss 0.0320376, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:38:15.029819: step 1820, loss 0.150516, acc 0.921875, precision 0.8125, recall 0.866667 f1 0.83871\n",
            "2022-11-30T22:38:16.339457: step 1821, loss 0.0319101, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:38:18.271020: step 1822, loss 0.0254246, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:20.304918: step 1823, loss 0.0562324, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:38:22.325704: step 1824, loss 0.0175425, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:23.624796: step 1825, loss 0.0620722, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:38:24.957215: step 1826, loss 0.00787108, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:26.268273: step 1827, loss 0.0277655, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:38:27.581926: step 1828, loss 0.121685, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:38:28.881942: step 1829, loss 0.0334742, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:38:30.185568: step 1830, loss 0.0944957, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:38:31.476130: step 1831, loss 0.0175366, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:32.784244: step 1832, loss 0.0564227, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:38:34.101560: step 1833, loss 0.109044, acc 0.96875, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:38:35.410244: step 1834, loss 0.0685277, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:38:36.822832: step 1835, loss 0.0115101, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:38.126276: step 1836, loss 0.00802545, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:39.421142: step 1837, loss 0.00542001, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:40.745502: step 1838, loss 0.148867, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:38:42.054287: step 1839, loss 0.0103718, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:43.391649: step 1840, loss 0.026215, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:38:44.754721: step 1841, loss 0.0962922, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:38:46.058550: step 1842, loss 0.0150128, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:47.385205: step 1843, loss 0.0340695, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:38:48.720383: step 1844, loss 0.198001, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:38:50.035373: step 1845, loss 0.0370916, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:38:51.358758: step 1846, loss 0.0253126, acc 0.984375, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:38:52.721645: step 1847, loss 0.0236417, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:54.045713: step 1848, loss 0.0967175, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:38:55.368712: step 1849, loss 0.00663538, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:38:56.745892: step 1850, loss 0.0281923, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:38:58.085916: step 1851, loss 0.0542718, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:38:59.412738: step 1852, loss 0.0105789, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:00.713132: step 1853, loss 0.0650803, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:39:02.005128: step 1854, loss 0.0200619, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:03.337608: step 1855, loss 0.0214578, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:04.636025: step 1856, loss 0.0104167, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:05.927075: step 1857, loss 0.0255098, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:39:07.220704: step 1858, loss 0.0519295, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:39:08.504099: step 1859, loss 0.052442, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:39:09.788288: step 1860, loss 0.0187467, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:39:11.089539: step 1861, loss 0.0764901, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:39:12.383412: step 1862, loss 0.0895225, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:39:13.692545: step 1863, loss 0.035058, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:39:15.063220: step 1864, loss 0.0304132, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:39:16.348154: step 1865, loss 0.0131907, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:17.645561: step 1866, loss 0.0058926, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:18.946769: step 1867, loss 0.00970402, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:20.238731: step 1868, loss 0.0275081, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:39:21.558618: step 1869, loss 0.0190132, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:22.863146: step 1870, loss 0.0119203, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:24.165931: step 1871, loss 0.0253606, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:24.831174: step 1872, loss 0.0130308, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:26.123987: step 1873, loss 0.0639546, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:39:27.396178: step 1874, loss 0.00601836, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:28.705089: step 1875, loss 0.0159536, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:29.987658: step 1876, loss 0.0203427, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:31.252504: step 1877, loss 0.00423766, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:32.542124: step 1878, loss 0.0121714, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:33.849511: step 1879, loss 0.0106639, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:35.174310: step 1880, loss 0.0138983, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:36.491365: step 1881, loss 0.011222, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:37.816567: step 1882, loss 0.0191939, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:39.109710: step 1883, loss 0.0263908, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:40.413063: step 1884, loss 0.0265738, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:41.722502: step 1885, loss 0.0102132, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:43.032303: step 1886, loss 0.138599, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:39:44.359821: step 1887, loss 0.0158412, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:45.736537: step 1888, loss 0.0109074, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:47.067660: step 1889, loss 0.0145448, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:48.388780: step 1890, loss 0.0621237, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:39:49.716062: step 1891, loss 0.0333947, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:39:51.071129: step 1892, loss 0.0432858, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:39:52.387455: step 1893, loss 0.0629766, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:39:53.755312: step 1894, loss 0.0445343, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:39:55.106918: step 1895, loss 0.00911155, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:56.440352: step 1896, loss 0.0606825, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:39:57.765629: step 1897, loss 0.0074812, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:39:59.071394: step 1898, loss 0.0240781, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:40:00.402737: step 1899, loss 0.013099, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:01.694580: step 1900, loss 0.00780059, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:40:05.334849: step 1900, loss 0.611612, acc 0.853061, precision 0.833333, recall 0.546875 f1 0.660377\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-1900\n",
            "\n",
            "2022-11-30T22:40:06.795090: step 1901, loss 0.0298503, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:40:08.109550: step 1902, loss 0.031347, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:40:09.393234: step 1903, loss 0.0325081, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:40:10.704137: step 1904, loss 0.038557, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:40:12.032911: step 1905, loss 0.00836919, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:13.349521: step 1906, loss 0.0448462, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:40:14.696542: step 1907, loss 0.047167, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:40:16.037393: step 1908, loss 0.0166773, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:17.392307: step 1909, loss 0.0602446, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:40:18.720454: step 1910, loss 0.0425072, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:40:20.005789: step 1911, loss 0.0198465, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:21.306098: step 1912, loss 0.0198751, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:40:22.650284: step 1913, loss 0.0858163, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:40:23.951425: step 1914, loss 0.0246001, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:40:25.257436: step 1915, loss 0.0134298, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:26.567534: step 1916, loss 0.0111563, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:27.902865: step 1917, loss 0.0370781, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:40:29.214313: step 1918, loss 0.0254245, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:30.514874: step 1919, loss 0.0272118, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:40:31.809321: step 1920, loss 0.0230515, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T22:40:33.102241: step 1921, loss 0.109517, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:40:34.441175: step 1922, loss 0.0197017, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:40:35.747506: step 1923, loss 0.0744097, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:40:37.081379: step 1924, loss 0.03684, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:40:38.385204: step 1925, loss 0.0339066, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:40:39.691234: step 1926, loss 0.0121059, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:40.999233: step 1927, loss 0.0482572, acc 0.96875, precision 0.894737, recall 1 f1 0.944444\n",
            "2022-11-30T22:40:42.288523: step 1928, loss 0.0264097, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:40:43.594112: step 1929, loss 0.17335, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:40:44.911788: step 1930, loss 0.0489252, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:40:46.199841: step 1931, loss 0.0208889, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:47.505653: step 1932, loss 0.0375305, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:40:48.869341: step 1933, loss 0.0250561, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:50.228716: step 1934, loss 0.0118688, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:51.549443: step 1935, loss 0.0198582, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:52.883376: step 1936, loss 0.057442, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:40:54.220442: step 1937, loss 0.140459, acc 0.96875, precision 0.888889, recall 0.888889 f1 0.888889\n",
            "2022-11-30T22:40:55.573057: step 1938, loss 0.0621905, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:40:56.892250: step 1939, loss 0.015057, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:40:58.176299: step 1940, loss 0.0784859, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:40:59.487589: step 1941, loss 0.049036, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:41:00.768383: step 1942, loss 0.168399, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:41:02.068962: step 1943, loss 0.0497543, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:41:03.383361: step 1944, loss 0.0288782, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:41:04.671377: step 1945, loss 0.0131499, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:05.989435: step 1946, loss 0.0375709, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:41:07.271365: step 1947, loss 0.00794901, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:08.566180: step 1948, loss 0.0645683, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:41:09.871859: step 1949, loss 0.0926602, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:41:11.175706: step 1950, loss 0.0305292, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:12.479310: step 1951, loss 0.06141, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:41:13.786023: step 1952, loss 0.0301865, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:15.094481: step 1953, loss 0.0685256, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:41:16.479220: step 1954, loss 0.012205, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:17.805351: step 1955, loss 0.0585201, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:41:19.419561: step 1956, loss 0.0216556, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:21.609410: step 1957, loss 0.0355721, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:41:23.721183: step 1958, loss 0.00245471, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:25.252414: step 1959, loss 0.0521222, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:41:26.531366: step 1960, loss 0.0584956, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:41:27.851350: step 1961, loss 0.127704, acc 0.9375, precision 1, recall 0.714286 f1 0.833333\n",
            "2022-11-30T22:41:29.153750: step 1962, loss 0.131183, acc 0.953125, precision 0.933333, recall 0.875 f1 0.903226\n",
            "2022-11-30T22:41:30.457145: step 1963, loss 0.081728, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:41:31.761983: step 1964, loss 0.0368572, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:41:33.069665: step 1965, loss 0.0110531, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:34.380947: step 1966, loss 0.0168247, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:41:35.725792: step 1967, loss 0.0485741, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:41:37.046718: step 1968, loss 0.0510984, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:41:38.348653: step 1969, loss 0.0279544, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:39.637766: step 1970, loss 0.0592035, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:41:40.933942: step 1971, loss 0.16139, acc 0.953125, precision 0.88, recall 1 f1 0.93617\n",
            "2022-11-30T22:41:42.248589: step 1972, loss 0.00805101, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:43.551412: step 1973, loss 0.0278983, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:41:44.854054: step 1974, loss 0.0538969, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:41:46.176570: step 1975, loss 0.0440732, acc 0.984375, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:41:46.824866: step 1976, loss 0.00993616, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:48.138120: step 1977, loss 0.0172133, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:49.498198: step 1978, loss 0.0214798, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:41:50.853713: step 1979, loss 0.112263, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:41:52.192347: step 1980, loss 0.0324181, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:41:53.489914: step 1981, loss 0.00456133, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:54.828046: step 1982, loss 0.050321, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:41:56.198457: step 1983, loss 0.0381891, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:41:57.546551: step 1984, loss 0.00621306, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:41:58.830049: step 1985, loss 0.00290168, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:00.143368: step 1986, loss 0.00901244, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:01.480180: step 1987, loss 0.00495324, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:02.784375: step 1988, loss 0.0163157, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:04.092176: step 1989, loss 0.0234691, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:42:05.403642: step 1990, loss 0.0280829, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:06.729608: step 1991, loss 0.0122018, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:08.018237: step 1992, loss 0.0112463, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:09.308698: step 1993, loss 0.0133433, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:10.609345: step 1994, loss 0.0394698, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:42:11.949020: step 1995, loss 0.0458157, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:42:13.273504: step 1996, loss 0.0514731, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:42:14.581449: step 1997, loss 0.0579011, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:42:15.883435: step 1998, loss 0.0831776, acc 0.96875, precision 0.9, recall 0.9 f1 0.9\n",
            "2022-11-30T22:42:17.185785: step 1999, loss 0.00437952, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:18.503398: step 2000, loss 0.0108609, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:42:22.183992: step 2000, loss 0.577707, acc 0.85034, precision 0.810606, recall 0.557292 f1 0.660494\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2000\n",
            "\n",
            "2022-11-30T22:42:23.597496: step 2001, loss 0.0078087, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:24.904318: step 2002, loss 0.0470579, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:42:26.220130: step 2003, loss 0.0473801, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:42:27.534722: step 2004, loss 0.0354039, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:42:28.819410: step 2005, loss 0.0252718, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:42:30.100952: step 2006, loss 0.0832384, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:42:31.422695: step 2007, loss 0.0452961, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:42:32.740121: step 2008, loss 0.0687556, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:42:34.060171: step 2009, loss 0.0815699, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:42:35.334697: step 2010, loss 0.0115603, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:36.653398: step 2011, loss 0.00954338, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:37.983985: step 2012, loss 0.0293504, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:39.359750: step 2013, loss 0.0559858, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:42:40.671561: step 2014, loss 0.0593202, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:42:41.977627: step 2015, loss 0.114011, acc 0.96875, precision 0.894737, recall 1 f1 0.944444\n",
            "2022-11-30T22:42:43.322868: step 2016, loss 0.046923, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:42:44.644567: step 2017, loss 0.0365298, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:42:45.955001: step 2018, loss 0.0120918, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:47.275494: step 2019, loss 0.0537855, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:42:48.569548: step 2020, loss 0.0394116, acc 0.984375, precision 1, recall 0.96 f1 0.979592\n",
            "2022-11-30T22:42:49.935155: step 2021, loss 0.00526678, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:42:51.248181: step 2022, loss 0.10906, acc 0.953125, precision 0.941176, recall 0.888889 f1 0.914286\n",
            "2022-11-30T22:42:52.567775: step 2023, loss 0.0813161, acc 0.953125, precision 1, recall 0.842105 f1 0.914286\n",
            "2022-11-30T22:42:53.887138: step 2024, loss 0.0207616, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:42:55.191424: step 2025, loss 0.0238714, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:42:56.537533: step 2026, loss 0.0582731, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:42:57.864968: step 2027, loss 0.0518426, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:42:59.195225: step 2028, loss 0.0531755, acc 0.984375, precision 1, recall 0.964286 f1 0.981818\n",
            "2022-11-30T22:43:00.511315: step 2029, loss 0.0151645, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:01.786450: step 2030, loss 0.0320508, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:43:03.091774: step 2031, loss 0.0488786, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:43:04.376602: step 2032, loss 0.103142, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:43:05.666516: step 2033, loss 0.0935634, acc 0.953125, precision 0.909091, recall 0.952381 f1 0.930233\n",
            "2022-11-30T22:43:06.951132: step 2034, loss 0.00862653, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:08.235116: step 2035, loss 0.0458807, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:43:09.521378: step 2036, loss 0.0212563, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:10.829696: step 2037, loss 0.0491463, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:43:12.084655: step 2038, loss 0.00694484, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:13.372669: step 2039, loss 0.0106589, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:14.672002: step 2040, loss 0.0261916, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:15.944923: step 2041, loss 0.0170927, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:43:17.239301: step 2042, loss 0.145186, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:43:18.540711: step 2043, loss 0.125237, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:43:19.820647: step 2044, loss 0.00609644, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:21.250058: step 2045, loss 0.0346885, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:43:22.574300: step 2046, loss 0.0326933, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:43:23.866244: step 2047, loss 0.0314199, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:43:25.165064: step 2048, loss 0.00147489, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:26.477449: step 2049, loss 0.013865, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:27.811984: step 2050, loss 0.0642474, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:43:29.107932: step 2051, loss 0.0459488, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:43:30.380965: step 2052, loss 0.0120767, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:31.673508: step 2053, loss 0.00694432, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:32.982989: step 2054, loss 0.0102471, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:34.271982: step 2055, loss 0.0388855, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:43:35.572149: step 2056, loss 0.010771, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:36.899787: step 2057, loss 0.0147786, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:43:38.229709: step 2058, loss 0.0978661, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:43:39.542967: step 2059, loss 0.0106084, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:40.832463: step 2060, loss 0.0231784, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:43:42.156531: step 2061, loss 0.0051726, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:43.442180: step 2062, loss 0.0598398, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:43:44.746013: step 2063, loss 0.00565343, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:46.041630: step 2064, loss 0.0252798, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:47.319390: step 2065, loss 0.0184133, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:48.642470: step 2066, loss 0.0616544, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:43:49.989709: step 2067, loss 0.00776961, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:51.369718: step 2068, loss 0.0218471, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:52.690073: step 2069, loss 0.0197612, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:43:54.005355: step 2070, loss 0.00480157, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:55.340894: step 2071, loss 0.0251138, acc 0.984375, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:43:56.681762: step 2072, loss 0.0280163, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:43:58.086450: step 2073, loss 0.00478292, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:43:59.416832: step 2074, loss 0.0188852, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:00.749121: step 2075, loss 0.0648936, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:44:02.081989: step 2076, loss 0.0139331, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:03.395130: step 2077, loss 0.0162713, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:04.710641: step 2078, loss 0.0180539, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:06.031240: step 2079, loss 0.0125508, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:06.689544: step 2080, loss 0.259582, acc 0.962963, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:44:08.016561: step 2081, loss 0.0230148, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:09.293963: step 2082, loss 0.00497662, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:10.609615: step 2083, loss 0.0164914, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:11.952478: step 2084, loss 0.0211516, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:13.292639: step 2085, loss 0.0331819, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:44:14.595391: step 2086, loss 0.0215676, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:44:15.908136: step 2087, loss 0.0206382, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:17.269317: step 2088, loss 0.112134, acc 0.953125, precision 0.8, recall 1 f1 0.888889\n",
            "2022-11-30T22:44:18.605038: step 2089, loss 0.0169212, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:19.899287: step 2090, loss 0.0125734, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:21.317183: step 2091, loss 0.019334, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:44:23.535595: step 2092, loss 0.00544375, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:25.639189: step 2093, loss 0.0110715, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:27.421880: step 2094, loss 0.00734772, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:28.731353: step 2095, loss 0.0228896, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:30.036823: step 2096, loss 0.0238817, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:44:31.333243: step 2097, loss 0.0302358, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:44:32.652559: step 2098, loss 0.00276243, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:33.957160: step 2099, loss 0.145801, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:44:35.269317: step 2100, loss 0.0460159, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:44:38.950279: step 2100, loss 0.71076, acc 0.85034, precision 0.847458, recall 0.520833 f1 0.645161\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2100\n",
            "\n",
            "2022-11-30T22:44:40.410530: step 2101, loss 0.0531041, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:44:41.761158: step 2102, loss 0.00806417, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:43.086409: step 2103, loss 0.0354655, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:44:44.407594: step 2104, loss 0.098909, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:44:45.715370: step 2105, loss 0.0177403, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:44:47.100882: step 2106, loss 0.00485174, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:48.435164: step 2107, loss 0.0633862, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:44:49.793628: step 2108, loss 0.0218332, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:51.161727: step 2109, loss 0.0103805, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:52.569097: step 2110, loss 0.016037, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:53.922213: step 2111, loss 0.00635905, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:55.243872: step 2112, loss 0.0492354, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:44:56.576253: step 2113, loss 0.00572581, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:57.920738: step 2114, loss 0.00677135, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:44:59.284340: step 2115, loss 0.00529855, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:00.580023: step 2116, loss 0.00834424, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:01.901064: step 2117, loss 0.0186162, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:45:03.210184: step 2118, loss 0.0296868, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:45:04.558968: step 2119, loss 0.00706543, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:05.880837: step 2120, loss 0.0933163, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:45:07.225258: step 2121, loss 0.0175139, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:08.553200: step 2122, loss 0.0123186, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:09.905825: step 2123, loss 0.0183653, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:11.269012: step 2124, loss 0.033721, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:45:12.567355: step 2125, loss 0.0105671, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:13.882911: step 2126, loss 0.0132487, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:15.193933: step 2127, loss 0.0200971, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:45:16.531412: step 2128, loss 0.0485656, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:45:17.862078: step 2129, loss 0.158267, acc 0.921875, precision 0.923077, recall 0.75 f1 0.827586\n",
            "2022-11-30T22:45:19.198348: step 2130, loss 0.00705735, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:20.538071: step 2131, loss 0.00751405, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:21.886742: step 2132, loss 0.00800608, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:23.283234: step 2133, loss 0.00925317, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:24.596226: step 2134, loss 0.0292934, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:45:25.920153: step 2135, loss 0.00536387, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:27.221011: step 2136, loss 0.0142276, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:28.565465: step 2137, loss 0.0347328, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:45:29.914495: step 2138, loss 0.0588323, acc 0.953125, precision 0.904762, recall 0.95 f1 0.926829\n",
            "2022-11-30T22:45:31.263262: step 2139, loss 0.0139254, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:32.589212: step 2140, loss 0.0113824, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:33.886358: step 2141, loss 0.0242897, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:35.191990: step 2142, loss 0.00911935, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:36.517295: step 2143, loss 0.131122, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:45:37.848137: step 2144, loss 0.0317658, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:45:39.151479: step 2145, loss 0.0317313, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:45:40.488539: step 2146, loss 0.0309294, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:45:41.859888: step 2147, loss 0.0347215, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:45:43.185678: step 2148, loss 0.00777301, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:44.523282: step 2149, loss 0.137015, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:45:45.829858: step 2150, loss 0.0532677, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:45:47.153481: step 2151, loss 0.00870311, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:48.499456: step 2152, loss 0.0067675, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:49.832933: step 2153, loss 0.0139083, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:51.179920: step 2154, loss 0.0508646, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:45:52.542405: step 2155, loss 0.00803536, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:53.902975: step 2156, loss 0.030787, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:45:55.241211: step 2157, loss 0.0330894, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:45:56.586274: step 2158, loss 0.00772717, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:57.953226: step 2159, loss 0.00176467, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:45:59.275236: step 2160, loss 0.0178714, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:00.636343: step 2161, loss 0.00316155, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:01.985838: step 2162, loss 0.0889771, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:46:03.302623: step 2163, loss 0.00680969, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:04.619311: step 2164, loss 0.0759617, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:46:05.954656: step 2165, loss 0.16521, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:46:07.282593: step 2166, loss 0.00536154, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:08.612747: step 2167, loss 0.0422981, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:46:09.949209: step 2168, loss 0.0164929, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:11.265823: step 2169, loss 0.0534593, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:46:12.609345: step 2170, loss 0.00434768, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:13.926238: step 2171, loss 0.0784225, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:46:15.263629: step 2172, loss 0.0320082, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:46:16.621196: step 2173, loss 0.0164874, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:17.933340: step 2174, loss 0.0391627, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:19.238686: step 2175, loss 0.0252116, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:46:20.590076: step 2176, loss 0.0045838, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:21.906120: step 2177, loss 0.0193777, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:23.286595: step 2178, loss 0.00959576, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:24.616239: step 2179, loss 0.0229201, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:25.923428: step 2180, loss 0.0436946, acc 0.96875, precision 0.954545, recall 0.954545 f1 0.954545\n",
            "2022-11-30T22:46:27.263721: step 2181, loss 0.00271167, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:28.587274: step 2182, loss 0.0843273, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:46:29.888763: step 2183, loss 0.0400216, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:46:30.568703: step 2184, loss 0.0259047, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:31.890428: step 2185, loss 0.0951072, acc 0.96875, precision 1, recall 0.894737 f1 0.944444\n",
            "2022-11-30T22:46:33.222492: step 2186, loss 0.0111292, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:34.542556: step 2187, loss 0.0208789, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:46:35.863299: step 2188, loss 0.00343868, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:37.151507: step 2189, loss 0.00463339, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:38.491022: step 2190, loss 0.0310082, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:39.826368: step 2191, loss 0.0095888, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:41.206447: step 2192, loss 0.0336075, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:46:42.522423: step 2193, loss 0.0455413, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:46:43.879647: step 2194, loss 0.00164556, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:45.232944: step 2195, loss 0.143335, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:46:46.563007: step 2196, loss 0.0135466, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:47.872987: step 2197, loss 0.003493, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:49.193358: step 2198, loss 0.0171111, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:50.550391: step 2199, loss 0.102133, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:46:51.912996: step 2200, loss 0.0101207, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:46:55.662181: step 2200, loss 0.586953, acc 0.851701, precision 0.798561, recall 0.578125 f1 0.670695\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2200\n",
            "\n",
            "2022-11-30T22:46:57.148909: step 2201, loss 0.00845837, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:46:58.524142: step 2202, loss 0.0573833, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:46:59.862688: step 2203, loss 0.0114548, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:01.241420: step 2204, loss 0.0141966, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:02.579310: step 2205, loss 0.025543, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:47:03.925994: step 2206, loss 0.0257755, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:47:05.247285: step 2207, loss 0.0160197, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:06.580372: step 2208, loss 0.0266892, acc 0.984375, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T22:47:07.886694: step 2209, loss 0.0120615, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:09.224502: step 2210, loss 0.012576, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:10.554463: step 2211, loss 0.0192945, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:47:11.891218: step 2212, loss 0.0389865, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:47:13.215621: step 2213, loss 0.0226572, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:14.553065: step 2214, loss 0.00540671, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:15.900619: step 2215, loss 0.0203809, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:17.219232: step 2216, loss 0.00353196, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:18.543804: step 2217, loss 0.060357, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:47:19.866552: step 2218, loss 0.00961474, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:21.192157: step 2219, loss 0.00561864, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:22.539065: step 2220, loss 0.0795146, acc 0.96875, precision 0.933333, recall 0.933333 f1 0.933333\n",
            "2022-11-30T22:47:24.122537: step 2221, loss 0.010214, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:26.321931: step 2222, loss 0.0470955, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:47:28.401882: step 2223, loss 0.00456566, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:29.937482: step 2224, loss 0.121213, acc 0.953125, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:47:31.242581: step 2225, loss 0.00641302, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:32.562797: step 2226, loss 0.00152927, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:33.885766: step 2227, loss 0.0419499, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:47:35.193447: step 2228, loss 0.141394, acc 0.953125, precision 0.857143, recall 0.923077 f1 0.888889\n",
            "2022-11-30T22:47:36.528558: step 2229, loss 0.00462831, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:37.842405: step 2230, loss 0.00520282, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:39.159269: step 2231, loss 0.116505, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:47:40.540011: step 2232, loss 0.0581637, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:47:41.908594: step 2233, loss 0.0130819, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:43.237471: step 2234, loss 0.0128984, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:44.547170: step 2235, loss 0.0078026, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:45.882003: step 2236, loss 0.0158942, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:47.210908: step 2237, loss 0.00677281, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:48.534479: step 2238, loss 0.0079228, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:49.862649: step 2239, loss 0.0532944, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:47:51.209892: step 2240, loss 0.0188842, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:52.600622: step 2241, loss 0.00756824, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:53.905702: step 2242, loss 0.00523628, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:55.284542: step 2243, loss 0.16749, acc 0.953125, precision 0.928571, recall 0.866667 f1 0.896552\n",
            "2022-11-30T22:47:56.614357: step 2244, loss 0.0208788, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:47:57.963497: step 2245, loss 0.0113077, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:47:59.271237: step 2246, loss 0.00582546, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:00.556270: step 2247, loss 0.0758714, acc 0.96875, precision 0.909091, recall 0.909091 f1 0.909091\n",
            "2022-11-30T22:48:01.870114: step 2248, loss 0.0404617, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:48:03.213248: step 2249, loss 0.0331116, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:04.544564: step 2250, loss 0.102679, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:48:05.862042: step 2251, loss 0.0055898, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:07.192959: step 2252, loss 0.023636, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:08.477309: step 2253, loss 0.0272758, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:48:09.771562: step 2254, loss 0.0744057, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:48:11.075453: step 2255, loss 0.0693012, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:48:12.393846: step 2256, loss 0.0039257, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:13.723025: step 2257, loss 0.0780632, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:48:15.020171: step 2258, loss 0.0207602, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:16.320740: step 2259, loss 0.011146, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:17.649485: step 2260, loss 0.011198, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:18.943050: step 2261, loss 0.00843449, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:20.284950: step 2262, loss 0.00966727, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:21.598265: step 2263, loss 0.0173602, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:48:22.977987: step 2264, loss 0.0251826, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:24.299431: step 2265, loss 0.00496975, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:25.655858: step 2266, loss 0.0452736, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:48:26.994128: step 2267, loss 0.0102168, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:28.293427: step 2268, loss 0.0351984, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:48:29.569299: step 2269, loss 0.0678617, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T22:48:30.845794: step 2270, loss 0.0136977, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:32.137306: step 2271, loss 0.00486436, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:33.454593: step 2272, loss 0.00445836, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:34.759160: step 2273, loss 0.0736776, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:48:36.047412: step 2274, loss 0.0205387, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:37.359540: step 2275, loss 0.0108553, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:38.672748: step 2276, loss 0.0297517, acc 0.984375, precision 0.961538, recall 1 f1 0.980392\n",
            "2022-11-30T22:48:40.015943: step 2277, loss 0.0461636, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:48:41.310376: step 2278, loss 0.0777962, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:48:42.628375: step 2279, loss 0.0704725, acc 0.96875, precision 1, recall 0.833333 f1 0.909091\n",
            "2022-11-30T22:48:43.980495: step 2280, loss 0.0471846, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:48:45.280833: step 2281, loss 0.0121207, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:46.591623: step 2282, loss 0.0628695, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:48:47.911594: step 2283, loss 0.0126094, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:49.225121: step 2284, loss 0.00975643, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:50.558965: step 2285, loss 0.0466705, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:48:51.855270: step 2286, loss 0.0789752, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:48:53.199515: step 2287, loss 0.00738254, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:53.870682: step 2288, loss 0.132862, acc 0.962963, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:48:55.201067: step 2289, loss 0.00158209, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:56.527274: step 2290, loss 0.0644752, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:48:57.906735: step 2291, loss 0.00199475, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:48:59.241472: step 2292, loss 0.00931846, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:00.564661: step 2293, loss 0.0125933, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:01.882918: step 2294, loss 0.0179085, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:49:03.218162: step 2295, loss 0.017593, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:04.573596: step 2296, loss 0.00836866, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:05.889671: step 2297, loss 0.00371705, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:07.213524: step 2298, loss 0.0684097, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:49:08.510395: step 2299, loss 0.0359456, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:49:09.809276: step 2300, loss 0.00615352, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:49:13.425206: step 2300, loss 0.594107, acc 0.851701, precision 0.798561, recall 0.578125 f1 0.670695\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2300\n",
            "\n",
            "2022-11-30T22:49:14.880822: step 2301, loss 0.00305419, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:16.172085: step 2302, loss 0.0608809, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:49:17.481485: step 2303, loss 0.0521218, acc 0.96875, precision 0.952381, recall 0.952381 f1 0.952381\n",
            "2022-11-30T22:49:18.820518: step 2304, loss 0.0231165, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:49:20.122352: step 2305, loss 0.0290496, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:49:21.422284: step 2306, loss 0.00478521, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:22.719827: step 2307, loss 0.0170431, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:24.052646: step 2308, loss 0.017461, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:25.394543: step 2309, loss 0.0211965, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:49:26.768516: step 2310, loss 0.00331209, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:28.069222: step 2311, loss 0.00809273, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:29.404482: step 2312, loss 0.0106166, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:30.709868: step 2313, loss 0.00491718, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:31.990097: step 2314, loss 0.115944, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:49:33.283577: step 2315, loss 0.0419119, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:49:34.616753: step 2316, loss 0.0631019, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:49:35.917636: step 2317, loss 0.0211902, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:49:37.222370: step 2318, loss 0.0320392, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:49:38.530150: step 2319, loss 0.00568533, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:39.852818: step 2320, loss 0.0583119, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:49:41.162538: step 2321, loss 0.0199043, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:42.501857: step 2322, loss 0.00882753, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:43.815426: step 2323, loss 0.022632, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:49:45.109432: step 2324, loss 0.00203581, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:46.434377: step 2325, loss 0.00830838, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:47.741376: step 2326, loss 0.0270325, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:49:49.034814: step 2327, loss 0.00518874, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:50.373643: step 2328, loss 0.0878754, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:49:51.694445: step 2329, loss 0.100271, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:49:53.000011: step 2330, loss 0.0234493, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:54.290187: step 2331, loss 0.0389529, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:49:55.591703: step 2332, loss 0.0641085, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:49:56.946055: step 2333, loss 0.00295789, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:49:58.298054: step 2334, loss 0.0453735, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:49:59.598296: step 2335, loss 0.0349753, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:50:00.878572: step 2336, loss 0.0360184, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:50:02.176397: step 2337, loss 0.0103551, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:03.467718: step 2338, loss 0.0165276, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:04.814484: step 2339, loss 0.0780766, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T22:50:06.164508: step 2340, loss 0.0536917, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:50:07.462065: step 2341, loss 0.0237166, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:50:08.756790: step 2342, loss 0.00910151, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:10.037052: step 2343, loss 0.0375296, acc 0.96875, precision 1, recall 0.846154 f1 0.916667\n",
            "2022-11-30T22:50:11.357924: step 2344, loss 0.0218151, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:12.662842: step 2345, loss 0.00221187, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:13.980738: step 2346, loss 0.0877484, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:50:15.305373: step 2347, loss 0.0231981, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:50:16.632599: step 2348, loss 0.00463076, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:17.948655: step 2349, loss 0.0535031, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:50:19.234185: step 2350, loss 0.00248482, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:20.571475: step 2351, loss 0.00828801, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:21.876379: step 2352, loss 0.0278666, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:50:23.191877: step 2353, loss 0.203905, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:50:24.480837: step 2354, loss 0.0168111, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:26.583704: step 2355, loss 0.0125847, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:28.621829: step 2356, loss 0.00186802, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:30.430544: step 2357, loss 0.0021505, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:31.718539: step 2358, loss 0.0116784, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:33.009376: step 2359, loss 0.0199904, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:34.349967: step 2360, loss 0.00264287, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:35.680136: step 2361, loss 0.0194402, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:50:36.977959: step 2362, loss 0.0077086, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:38.257902: step 2363, loss 0.0044998, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:39.560474: step 2364, loss 0.0370156, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:50:40.891373: step 2365, loss 0.0045491, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:42.207423: step 2366, loss 0.0172968, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:43.519235: step 2367, loss 0.0197495, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:50:44.802894: step 2368, loss 0.0363924, acc 0.984375, precision 1, recall 0.961538 f1 0.980392\n",
            "2022-11-30T22:50:46.114934: step 2369, loss 0.00727634, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:47.407508: step 2370, loss 0.0202734, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:50:48.707133: step 2371, loss 0.0175829, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:50.005662: step 2372, loss 0.0262237, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:51.307286: step 2373, loss 0.00920689, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:52.647502: step 2374, loss 0.00472213, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:53.957539: step 2375, loss 0.0355603, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:50:55.253234: step 2376, loss 0.011819, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:56.570788: step 2377, loss 0.00180819, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:50:57.899980: step 2378, loss 0.0246505, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:50:59.240620: step 2379, loss 0.236315, acc 0.953125, precision 0.785714, recall 1 f1 0.88\n",
            "2022-11-30T22:51:00.560444: step 2380, loss 0.0241414, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:51:01.851868: step 2381, loss 0.00628226, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:03.161633: step 2382, loss 0.0219131, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:04.470094: step 2383, loss 0.00448253, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:05.840925: step 2384, loss 0.0276007, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:51:07.122820: step 2385, loss 0.0318698, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:51:08.466664: step 2386, loss 0.00814425, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:09.774693: step 2387, loss 0.00289446, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:11.080592: step 2388, loss 0.0221935, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:51:12.358326: step 2389, loss 0.121586, acc 0.953125, precision 0.947368, recall 0.9 f1 0.923077\n",
            "2022-11-30T22:51:13.623958: step 2390, loss 0.00281822, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:14.911189: step 2391, loss 0.101203, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:51:15.555185: step 2392, loss 0.00136366, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:16.927363: step 2393, loss 0.0674861, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:51:18.229823: step 2394, loss 0.0215579, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:19.537897: step 2395, loss 0.0258841, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:51:20.857840: step 2396, loss 0.00270929, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:22.170870: step 2397, loss 0.00593047, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:23.493355: step 2398, loss 0.00487264, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:24.789425: step 2399, loss 0.00915793, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:26.109744: step 2400, loss 0.0370903, acc 0.953125, precision 0.928571, recall 0.866667 f1 0.896552\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:51:29.769617: step 2400, loss 0.682482, acc 0.85034, precision 0.815385, recall 0.552083 f1 0.658385\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2400\n",
            "\n",
            "2022-11-30T22:51:31.227728: step 2401, loss 0.0544301, acc 0.984375, precision 1, recall 0.958333 f1 0.978723\n",
            "2022-11-30T22:51:32.553910: step 2402, loss 0.0142268, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:33.864012: step 2403, loss 0.0117246, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:35.130439: step 2404, loss 0.00707309, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:36.452704: step 2405, loss 0.00990181, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:37.729349: step 2406, loss 0.0082288, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:39.021433: step 2407, loss 0.00697351, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:40.304127: step 2408, loss 0.00183021, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:41.632745: step 2409, loss 0.0177487, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:51:42.939768: step 2410, loss 0.00416266, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:44.247181: step 2411, loss 0.00629348, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:45.530198: step 2412, loss 0.0179009, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:46.874071: step 2413, loss 0.0213135, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:48.166511: step 2414, loss 0.00178872, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:49.480153: step 2415, loss 0.0547883, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:51:50.813554: step 2416, loss 0.0247319, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:51:52.099278: step 2417, loss 0.0110337, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:53.413222: step 2418, loss 0.00714468, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:54.729076: step 2419, loss 0.0290184, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:51:56.050061: step 2420, loss 0.0101604, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:51:57.391079: step 2421, loss 0.0184493, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:51:58.747682: step 2422, loss 0.036658, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:52:00.091373: step 2423, loss 0.00516599, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:01.429294: step 2424, loss 0.00243983, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:02.737928: step 2425, loss 0.0061569, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:04.053218: step 2426, loss 0.0231938, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:52:05.386519: step 2427, loss 0.0046203, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:06.756624: step 2428, loss 0.00458205, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:08.060995: step 2429, loss 0.000755556, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:09.381664: step 2430, loss 0.0194785, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:10.667746: step 2431, loss 0.00744357, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:11.994772: step 2432, loss 0.0403781, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:52:13.306411: step 2433, loss 0.0128744, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:14.603012: step 2434, loss 0.0260333, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T22:52:15.914102: step 2435, loss 0.00782032, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:17.252902: step 2436, loss 0.00622291, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:18.556924: step 2437, loss 0.00746894, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:19.872677: step 2438, loss 0.00587233, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:21.236181: step 2439, loss 0.00434837, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:22.561145: step 2440, loss 0.0167614, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:23.871558: step 2441, loss 0.0777607, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:52:25.214049: step 2442, loss 0.00941576, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:26.511128: step 2443, loss 0.0953695, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:52:27.873359: step 2444, loss 0.0152357, acc 0.984375, precision 1, recall 0.961538 f1 0.980392\n",
            "2022-11-30T22:52:29.179878: step 2445, loss 0.00140877, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:30.483222: step 2446, loss 0.00107153, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:31.867830: step 2447, loss 0.05574, acc 0.96875, precision 0.857143, recall 0.857143 f1 0.857143\n",
            "2022-11-30T22:52:33.169171: step 2448, loss 0.00984083, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:34.449280: step 2449, loss 0.0338401, acc 0.984375, precision 0.961538, recall 1 f1 0.980392\n",
            "2022-11-30T22:52:35.754369: step 2450, loss 0.0235465, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:52:37.074537: step 2451, loss 0.136065, acc 0.953125, precision 0.857143, recall 0.923077 f1 0.888889\n",
            "2022-11-30T22:52:38.389011: step 2452, loss 0.019145, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T22:52:39.685309: step 2453, loss 0.0175816, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:41.003105: step 2454, loss 0.00402013, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:42.317430: step 2455, loss 0.0453321, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:52:43.598917: step 2456, loss 0.00997633, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:44.872671: step 2457, loss 0.00426985, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:46.144407: step 2458, loss 0.00890101, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:47.464203: step 2459, loss 0.00635925, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:48.781154: step 2460, loss 0.020909, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:50.088599: step 2461, loss 0.00130898, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:51.439665: step 2462, loss 0.00596395, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:52.765481: step 2463, loss 0.0236705, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:52:54.046591: step 2464, loss 0.015353, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:55.315376: step 2465, loss 0.0121024, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:56.611937: step 2466, loss 0.0699355, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:52:58.039109: step 2467, loss 0.00869621, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:52:59.323327: step 2468, loss 0.0381072, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:53:00.626967: step 2469, loss 0.00355573, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:01.993623: step 2470, loss 0.0197346, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:03.329095: step 2471, loss 0.0897553, acc 0.96875, precision 0.95, recall 0.95 f1 0.95\n",
            "2022-11-30T22:53:04.644853: step 2472, loss 0.0213733, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:05.968891: step 2473, loss 0.00202506, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:07.303787: step 2474, loss 0.09836, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:53:08.628619: step 2475, loss 0.0072962, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:09.930020: step 2476, loss 0.00773804, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:11.250391: step 2477, loss 0.00702773, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:12.553467: step 2478, loss 0.0178275, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:13.865352: step 2479, loss 0.062645, acc 0.96875, precision 0.944444, recall 0.944444 f1 0.944444\n",
            "2022-11-30T22:53:15.203273: step 2480, loss 0.11223, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:53:16.481090: step 2481, loss 0.0237278, acc 0.984375, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T22:53:17.784602: step 2482, loss 0.0416559, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:53:19.123797: step 2483, loss 0.0143743, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:53:20.442921: step 2484, loss 0.0134217, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:21.750637: step 2485, loss 0.0277894, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:23.070518: step 2486, loss 0.00760071, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:24.351563: step 2487, loss 0.023441, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:53:25.702279: step 2488, loss 0.0155985, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:27.489805: step 2489, loss 0.0018019, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:29.620242: step 2490, loss 0.0174063, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:31.645616: step 2491, loss 0.0153317, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:32.989788: step 2492, loss 0.00306562, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:34.291188: step 2493, loss 0.00898101, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:35.591413: step 2494, loss 0.0318249, acc 0.984375, precision 0.956522, recall 1 f1 0.977778\n",
            "2022-11-30T22:53:36.896952: step 2495, loss 0.0117496, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:37.536730: step 2496, loss 0.0590408, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:38.867146: step 2497, loss 0.0118327, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:40.175098: step 2498, loss 0.00175255, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:41.455938: step 2499, loss 0.0137707, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:42.759920: step 2500, loss 0.000536914, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:53:46.412116: step 2500, loss 0.710077, acc 0.846259, precision 0.801527, recall 0.546875 f1 0.650155\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2500\n",
            "\n",
            "2022-11-30T22:53:47.862932: step 2501, loss 0.0174019, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:49.160428: step 2502, loss 0.0312222, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:53:50.462242: step 2503, loss 0.0114427, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:51.765645: step 2504, loss 0.0357015, acc 0.96875, precision 0.941176, recall 0.941176 f1 0.941176\n",
            "2022-11-30T22:53:53.059437: step 2505, loss 0.0145087, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:54.368486: step 2506, loss 0.0248084, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:53:55.656972: step 2507, loss 0.0402304, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:53:56.972526: step 2508, loss 0.00218047, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:58.341577: step 2509, loss 0.0118429, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:53:59.640516: step 2510, loss 0.0481482, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:54:00.920648: step 2511, loss 0.0481255, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:54:02.261019: step 2512, loss 0.00365025, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:03.571292: step 2513, loss 0.0295146, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:54:04.906921: step 2514, loss 0.00490638, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:06.205610: step 2515, loss 0.00877735, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:07.514874: step 2516, loss 0.00701439, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:08.858225: step 2517, loss 0.0095111, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:10.196963: step 2518, loss 0.00389036, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:11.485127: step 2519, loss 0.0638449, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:54:12.800906: step 2520, loss 0.0140527, acc 0.984375, precision 0.962963, recall 1 f1 0.981132\n",
            "2022-11-30T22:54:14.086748: step 2521, loss 0.0164383, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:54:15.407584: step 2522, loss 0.0540506, acc 0.96875, precision 0.857143, recall 1 f1 0.923077\n",
            "2022-11-30T22:54:16.697648: step 2523, loss 0.0135662, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:18.016453: step 2524, loss 0.0207509, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:19.329414: step 2525, loss 0.00898711, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:20.614587: step 2526, loss 0.0023718, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:21.923914: step 2527, loss 0.00320012, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:23.241194: step 2528, loss 0.00568444, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:24.544553: step 2529, loss 0.0127331, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:25.881282: step 2530, loss 0.0081719, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:27.220984: step 2531, loss 0.00776411, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:28.523867: step 2532, loss 0.0241702, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:54:29.841286: step 2533, loss 0.00537789, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:31.130903: step 2534, loss 0.0114703, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:32.435536: step 2535, loss 0.00335998, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:33.804900: step 2536, loss 0.0109065, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:35.130419: step 2537, loss 0.00215076, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:36.426775: step 2538, loss 0.00664878, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:37.746169: step 2539, loss 0.00145965, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:39.047401: step 2540, loss 0.0130834, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:40.385114: step 2541, loss 0.0103831, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:41.694659: step 2542, loss 0.00487291, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:43.013219: step 2543, loss 0.0236884, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:54:44.329430: step 2544, loss 0.16638, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:54:45.673210: step 2545, loss 0.046922, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:54:46.985731: step 2546, loss 0.0111974, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:48.277086: step 2547, loss 0.00779126, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:49.599528: step 2548, loss 0.00876763, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:50.909124: step 2549, loss 0.00528959, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:52.221351: step 2550, loss 0.00447766, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:53.592379: step 2551, loss 0.00508356, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:54.956654: step 2552, loss 0.107517, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:54:56.288311: step 2553, loss 0.0219436, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:57.645115: step 2554, loss 0.0117887, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:54:59.031707: step 2555, loss 0.0241873, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:55:00.400137: step 2556, loss 0.0288682, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:55:01.719787: step 2557, loss 0.0208178, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:55:03.073384: step 2558, loss 0.023016, acc 0.984375, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T22:55:04.438324: step 2559, loss 0.0359412, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T22:55:05.802066: step 2560, loss 0.105232, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:55:07.172449: step 2561, loss 0.00248887, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:08.474543: step 2562, loss 0.0113111, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:09.799316: step 2563, loss 0.0717314, acc 0.96875, precision 0.947368, recall 0.947368 f1 0.947368\n",
            "2022-11-30T22:55:11.140901: step 2564, loss 0.0157689, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:55:12.460156: step 2565, loss 0.0267108, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:13.777458: step 2566, loss 0.0307797, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T22:55:15.098615: step 2567, loss 0.00868213, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:16.421896: step 2568, loss 0.00293868, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:17.732082: step 2569, loss 0.00612437, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:19.040167: step 2570, loss 0.13118, acc 0.984375, precision 1, recall 0.9 f1 0.947368\n",
            "2022-11-30T22:55:20.385277: step 2571, loss 0.00780068, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:21.727118: step 2572, loss 0.00184394, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:23.052762: step 2573, loss 0.0297437, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:24.339871: step 2574, loss 0.00523902, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:25.625573: step 2575, loss 0.0247389, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:26.949135: step 2576, loss 0.010615, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:28.251851: step 2577, loss 0.018993, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:29.527137: step 2578, loss 0.00614459, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:30.834523: step 2579, loss 0.0155901, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:32.131004: step 2580, loss 0.00513008, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:33.449523: step 2581, loss 0.0533432, acc 0.984375, precision 0.954545, recall 1 f1 0.976744\n",
            "2022-11-30T22:55:34.785532: step 2582, loss 0.0323879, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T22:55:36.067509: step 2583, loss 0.0327759, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:55:37.382058: step 2584, loss 0.037276, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:55:38.662779: step 2585, loss 0.00593765, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:39.957239: step 2586, loss 0.0332886, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:55:41.284142: step 2587, loss 0.00637006, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:42.605938: step 2588, loss 0.001994, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:43.934863: step 2589, loss 0.00333571, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:45.252043: step 2590, loss 0.0499329, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:55:46.552499: step 2591, loss 0.0334461, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:55:47.846650: step 2592, loss 0.00089631, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:49.144784: step 2593, loss 0.0094568, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:50.439078: step 2594, loss 0.00445535, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:51.757340: step 2595, loss 0.00439264, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:53.083631: step 2596, loss 0.0243221, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:55:54.426535: step 2597, loss 0.00116899, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:55.753311: step 2598, loss 0.0157584, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:55:57.066427: step 2599, loss 0.0163937, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:55:57.720289: step 2600, loss 0.0030982, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:56:01.447296: step 2600, loss 0.76442, acc 0.846259, precision 0.816, recall 0.53125 f1 0.643533\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2600\n",
            "\n",
            "2022-11-30T22:56:02.894485: step 2601, loss 0.00393925, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:04.293293: step 2602, loss 0.0209572, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:05.602910: step 2603, loss 0.00134825, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:06.918727: step 2604, loss 0.0226091, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:08.227289: step 2605, loss 0.00946817, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:09.548963: step 2606, loss 0.00754771, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:10.897243: step 2607, loss 0.100567, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:56:12.241727: step 2608, loss 0.00350692, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:13.576331: step 2609, loss 0.101863, acc 0.96875, precision 0.916667, recall 0.916667 f1 0.916667\n",
            "2022-11-30T22:56:14.940675: step 2610, loss 0.00313317, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:16.310430: step 2611, loss 0.0157491, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:17.637235: step 2612, loss 0.00443228, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:18.940935: step 2613, loss 0.00949395, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:20.269399: step 2614, loss 0.0179026, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:56:21.594416: step 2615, loss 0.00182162, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:22.962545: step 2616, loss 0.0116545, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:24.293106: step 2617, loss 0.000636746, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:25.599717: step 2618, loss 0.00252195, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:26.915409: step 2619, loss 0.078804, acc 0.953125, precision 1, recall 0.823529 f1 0.903226\n",
            "2022-11-30T22:56:28.505720: step 2620, loss 0.0188075, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:30.662916: step 2621, loss 0.0120385, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:32.642956: step 2622, loss 0.0817309, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T22:56:34.192946: step 2623, loss 0.0123325, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:35.574522: step 2624, loss 0.0427392, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:56:36.887613: step 2625, loss 0.0203344, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:38.187597: step 2626, loss 0.00580617, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:39.471149: step 2627, loss 0.0046856, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:40.760212: step 2628, loss 0.00878038, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:42.062658: step 2629, loss 0.0111411, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:43.366537: step 2630, loss 0.0045829, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:44.660276: step 2631, loss 0.00527683, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:45.979911: step 2632, loss 0.00464607, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:47.254101: step 2633, loss 0.0200184, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:56:48.568867: step 2634, loss 0.017679, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:56:49.881778: step 2635, loss 0.0325713, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T22:56:51.216246: step 2636, loss 0.0016914, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:52.549583: step 2637, loss 0.00359869, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:53.880562: step 2638, loss 0.00451227, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:55.203786: step 2639, loss 0.00344559, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:56:56.516920: step 2640, loss 0.128455, acc 0.96875, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:56:57.828077: step 2641, loss 0.0353822, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:56:59.170332: step 2642, loss 0.0112664, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:00.476507: step 2643, loss 0.003917, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:01.758595: step 2644, loss 0.0361192, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:57:03.076474: step 2645, loss 0.0332797, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:57:04.365301: step 2646, loss 0.0132647, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:05.706905: step 2647, loss 0.0543865, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:57:07.015660: step 2648, loss 0.00443714, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:08.335521: step 2649, loss 0.002419, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:09.647074: step 2650, loss 0.00803481, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:10.950563: step 2651, loss 0.0146266, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:57:12.280527: step 2652, loss 0.060681, acc 0.96875, precision 1, recall 0.882353 f1 0.9375\n",
            "2022-11-30T22:57:13.619679: step 2653, loss 0.00314439, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:14.911250: step 2654, loss 0.0226418, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:16.223683: step 2655, loss 0.0294973, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T22:57:17.547552: step 2656, loss 0.00763035, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:18.848803: step 2657, loss 0.00502813, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:20.152155: step 2658, loss 0.0108844, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:21.462863: step 2659, loss 0.0252382, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:57:22.789287: step 2660, loss 0.00106815, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:24.074270: step 2661, loss 0.00966329, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:25.355929: step 2662, loss 0.00937689, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:26.660833: step 2663, loss 0.11652, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:57:27.934054: step 2664, loss 0.0021766, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:29.249016: step 2665, loss 0.04959, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:57:30.532001: step 2666, loss 0.00214986, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:31.813913: step 2667, loss 0.00818768, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:33.114946: step 2668, loss 0.000873707, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:34.392084: step 2669, loss 0.00703124, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:35.751068: step 2670, loss 0.0243243, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:57:37.034468: step 2671, loss 0.0243584, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:38.369340: step 2672, loss 0.00236261, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:39.668037: step 2673, loss 0.041912, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T22:57:40.984131: step 2674, loss 0.0255618, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T22:57:42.261955: step 2675, loss 0.083935, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:57:43.571103: step 2676, loss 0.00426653, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:44.849112: step 2677, loss 0.0129228, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:46.176864: step 2678, loss 0.0231159, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T22:57:47.489172: step 2679, loss 0.00633812, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:48.804936: step 2680, loss 0.00379673, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:50.131868: step 2681, loss 0.013764, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:51.449582: step 2682, loss 0.211995, acc 0.953125, precision 0.888889, recall 0.941176 f1 0.914286\n",
            "2022-11-30T22:57:52.817802: step 2683, loss 0.002661, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:54.117340: step 2684, loss 0.00843372, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:55.409115: step 2685, loss 0.00296454, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:56.687618: step 2686, loss 0.0254586, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T22:57:57.989592: step 2687, loss 0.00414641, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:57:59.322802: step 2688, loss 0.00798015, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:00.623588: step 2689, loss 0.0195355, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:58:01.910950: step 2690, loss 0.0303774, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n",
            "2022-11-30T22:58:03.189464: step 2691, loss 0.000929296, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:04.537146: step 2692, loss 0.0053366, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:05.899644: step 2693, loss 0.00996414, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:07.233686: step 2694, loss 0.014356, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:08.565585: step 2695, loss 0.00398397, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:09.863286: step 2696, loss 0.0325899, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T22:58:11.171948: step 2697, loss 0.037151, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T22:58:12.474221: step 2698, loss 0.0185403, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:13.866763: step 2699, loss 0.0144082, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:15.163191: step 2700, loss 0.0433414, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T22:58:18.809204: step 2700, loss 0.7952, acc 0.851701, precision 0.848739, recall 0.526042 f1 0.649518\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2700\n",
            "\n",
            "2022-11-30T22:58:20.268823: step 2701, loss 0.0170406, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:21.586562: step 2702, loss 0.0192557, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:58:22.915677: step 2703, loss 0.00265472, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:23.575646: step 2704, loss 0.00756141, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:24.886396: step 2705, loss 0.00711418, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:26.177968: step 2706, loss 0.00430143, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:27.493666: step 2707, loss 0.00584965, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:28.789465: step 2708, loss 0.00216869, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:30.130845: step 2709, loss 0.0070961, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:31.408104: step 2710, loss 0.00511868, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:32.690169: step 2711, loss 0.012367, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:34.002743: step 2712, loss 0.00269021, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:35.307684: step 2713, loss 0.0194991, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:36.658094: step 2714, loss 0.0352809, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:58:37.988827: step 2715, loss 0.00130679, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:39.260554: step 2716, loss 0.0125096, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:40.548292: step 2717, loss 0.0472553, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T22:58:41.833822: step 2718, loss 0.0287439, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:58:43.107369: step 2719, loss 0.1687, acc 0.96875, precision 0.956522, recall 0.956522 f1 0.956522\n",
            "2022-11-30T22:58:44.412519: step 2720, loss 0.00606388, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:45.722384: step 2721, loss 0.00097468, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:46.990657: step 2722, loss 0.00195596, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:48.318070: step 2723, loss 0.0777485, acc 0.984375, precision 1, recall 0.888889 f1 0.941176\n",
            "2022-11-30T22:58:49.634939: step 2724, loss 0.0508404, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:58:50.949192: step 2725, loss 0.0415114, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:58:52.270617: step 2726, loss 0.0154262, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T22:58:53.582507: step 2727, loss 0.00970216, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:54.911743: step 2728, loss 0.00712215, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:56.206517: step 2729, loss 0.000929158, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:57.518014: step 2730, loss 0.0267499, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:58:58.890840: step 2731, loss 0.0175452, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T22:59:00.219598: step 2732, loss 0.00390224, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:01.538694: step 2733, loss 0.00313465, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:02.833089: step 2734, loss 0.00842785, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:04.153431: step 2735, loss 0.033448, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T22:59:05.487007: step 2736, loss 0.0428054, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T22:59:06.848124: step 2737, loss 0.0128874, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:08.183780: step 2738, loss 0.0934276, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:59:09.504074: step 2739, loss 0.0289111, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T22:59:10.822705: step 2740, loss 0.00955206, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:12.146329: step 2741, loss 0.0110741, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:13.461058: step 2742, loss 0.00168623, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:14.832049: step 2743, loss 0.00341432, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:16.126145: step 2744, loss 0.0901077, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T22:59:17.429592: step 2745, loss 0.00725264, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:18.730963: step 2746, loss 0.00592381, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:20.048775: step 2747, loss 0.0151266, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T22:59:21.352012: step 2748, loss 0.014372, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:22.682633: step 2749, loss 0.0487289, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:59:23.971967: step 2750, loss 0.00556535, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:25.266826: step 2751, loss 0.00245009, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:26.554708: step 2752, loss 0.00528527, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:27.848682: step 2753, loss 0.000909869, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:29.803754: step 2754, loss 0.138127, acc 0.96875, precision 0.818182, recall 1 f1 0.9\n",
            "2022-11-30T22:59:31.907195: step 2755, loss 0.00154309, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:33.792828: step 2756, loss 0.0440923, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T22:59:35.104188: step 2757, loss 0.0841939, acc 0.953125, precision 0.846154, recall 0.916667 f1 0.88\n",
            "2022-11-30T22:59:36.426940: step 2758, loss 0.000194524, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:37.768541: step 2759, loss 0.0313156, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:39.094599: step 2760, loss 0.0524608, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T22:59:40.415285: step 2761, loss 0.00551067, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:41.721544: step 2762, loss 0.0633746, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T22:59:43.052196: step 2763, loss 0.00906647, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:44.324183: step 2764, loss 0.0104929, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:45.661770: step 2765, loss 0.00364194, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:46.964040: step 2766, loss 0.0143098, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:48.290857: step 2767, loss 0.0362325, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T22:59:49.581585: step 2768, loss 0.000932594, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:50.907017: step 2769, loss 0.0941414, acc 0.96875, precision 0.882353, recall 1 f1 0.9375\n",
            "2022-11-30T22:59:52.197270: step 2770, loss 0.0652896, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T22:59:53.497044: step 2771, loss 0.0126034, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:54.804968: step 2772, loss 0.0587063, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T22:59:56.096148: step 2773, loss 0.119009, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T22:59:57.387513: step 2774, loss 0.00793009, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T22:59:58.737896: step 2775, loss 0.00745995, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:00.025629: step 2776, loss 0.0180164, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:01.323861: step 2777, loss 0.0155612, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:02.623178: step 2778, loss 0.0497754, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:00:03.905108: step 2779, loss 0.000923693, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:05.186221: step 2780, loss 0.00353397, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:06.520546: step 2781, loss 0.0201322, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:07.899661: step 2782, loss 0.00339778, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:09.266449: step 2783, loss 0.00138058, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:10.593365: step 2784, loss 0.0053516, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:11.890109: step 2785, loss 0.00113489, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:13.239575: step 2786, loss 0.0280551, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:00:14.534528: step 2787, loss 0.00316818, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:15.887357: step 2788, loss 0.070735, acc 0.953125, precision 0.904762, recall 0.95 f1 0.926829\n",
            "2022-11-30T23:00:17.173916: step 2789, loss 0.00498831, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:18.438764: step 2790, loss 0.00593235, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:19.717841: step 2791, loss 0.013764, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T23:00:21.056345: step 2792, loss 0.00294237, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:22.371508: step 2793, loss 0.011459, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:23.701643: step 2794, loss 0.00200794, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:25.003406: step 2795, loss 0.000538781, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:26.305821: step 2796, loss 0.0301564, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:00:27.605243: step 2797, loss 0.0197065, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:00:28.932164: step 2798, loss 0.00080803, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:30.219527: step 2799, loss 0.0048497, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:31.501096: step 2800, loss 0.0540644, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T23:00:35.094202: step 2800, loss 0.779465, acc 0.84898, precision 0.818898, recall 0.541667 f1 0.652038\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2800\n",
            "\n",
            "2022-11-30T23:00:36.549883: step 2801, loss 0.024514, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T23:00:37.824781: step 2802, loss 0.00605145, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:39.103279: step 2803, loss 0.00472889, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:40.398802: step 2804, loss 0.0353097, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T23:00:41.712676: step 2805, loss 0.0136181, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:43.005464: step 2806, loss 0.00163345, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:44.293435: step 2807, loss 0.0392937, acc 0.96875, precision 1, recall 0.913043 f1 0.954545\n",
            "2022-11-30T23:00:44.959348: step 2808, loss 0.00353265, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:46.287864: step 2809, loss 0.00636364, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:47.602136: step 2810, loss 0.0226067, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T23:00:48.915091: step 2811, loss 0.0166161, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T23:00:50.228502: step 2812, loss 0.0224065, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:00:51.540728: step 2813, loss 0.00322959, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:52.833619: step 2814, loss 0.0409845, acc 0.984375, precision 0.956522, recall 1 f1 0.977778\n",
            "2022-11-30T23:00:54.113183: step 2815, loss 0.0109081, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:55.397481: step 2816, loss 0.00416289, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:56.690598: step 2817, loss 0.0127998, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:57.993303: step 2818, loss 0.0034022, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:00:59.301405: step 2819, loss 0.00198134, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:00.599462: step 2820, loss 0.00703841, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:01.874539: step 2821, loss 0.0149162, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:03.167744: step 2822, loss 0.0037205, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:04.472915: step 2823, loss 0.00806772, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:05.766082: step 2824, loss 0.000900924, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:07.085502: step 2825, loss 0.000288553, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:08.403102: step 2826, loss 0.0761549, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T23:01:09.693115: step 2827, loss 0.00420582, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:11.065827: step 2828, loss 0.000750822, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:12.406038: step 2829, loss 0.00812806, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:13.667908: step 2830, loss 0.00467971, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:14.976379: step 2831, loss 0.00344378, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:16.308922: step 2832, loss 0.0274776, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:01:17.651619: step 2833, loss 0.00574013, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:18.945515: step 2834, loss 0.0377069, acc 0.96875, precision 1, recall 0.8 f1 0.888889\n",
            "2022-11-30T23:01:20.253310: step 2835, loss 0.0047358, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:21.575737: step 2836, loss 0.0283976, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T23:01:22.943422: step 2837, loss 0.00641159, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:24.296308: step 2838, loss 0.0194826, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:01:25.577483: step 2839, loss 0.0066881, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:26.883409: step 2840, loss 0.0514132, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T23:01:28.181226: step 2841, loss 0.00395886, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:29.479526: step 2842, loss 0.00284373, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:30.749853: step 2843, loss 0.00486738, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:32.045694: step 2844, loss 0.0431076, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T23:01:33.302753: step 2845, loss 0.0166554, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:34.615715: step 2846, loss 0.00362469, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:35.904486: step 2847, loss 0.0441474, acc 0.96875, precision 0.875, recall 1 f1 0.933333\n",
            "2022-11-30T23:01:37.185041: step 2848, loss 0.00417786, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:38.456567: step 2849, loss 0.00132998, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:39.771389: step 2850, loss 0.00978944, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:41.052105: step 2851, loss 0.00572758, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:42.373879: step 2852, loss 0.00463624, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:43.692116: step 2853, loss 0.00330337, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:45.004862: step 2854, loss 0.0113256, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:46.287286: step 2855, loss 0.0471379, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T23:01:47.585549: step 2856, loss 0.00618811, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:48.893703: step 2857, loss 0.00252522, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:50.213428: step 2858, loss 0.0130068, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:51.541137: step 2859, loss 0.00250013, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:52.850725: step 2860, loss 0.0341816, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T23:01:54.144272: step 2861, loss 0.0049265, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:55.414104: step 2862, loss 0.00192918, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:56.691523: step 2863, loss 0.00814969, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:57.986129: step 2864, loss 0.0103465, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:01:59.316771: step 2865, loss 0.000598597, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:00.607267: step 2866, loss 0.000306025, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:01.879204: step 2867, loss 0.0804508, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T23:02:03.187389: step 2868, loss 0.00182421, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:04.493881: step 2869, loss 0.00384017, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:05.761069: step 2870, loss 0.00257082, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:07.062500: step 2871, loss 0.0591815, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T23:02:08.353037: step 2872, loss 0.00107812, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:09.634926: step 2873, loss 0.00590153, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:10.949421: step 2874, loss 0.0215313, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T23:02:12.315521: step 2875, loss 0.00110692, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:13.624870: step 2876, loss 0.00988573, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:14.924611: step 2877, loss 0.00268613, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:16.220402: step 2878, loss 0.00869771, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:17.534321: step 2879, loss 0.0107781, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:18.835381: step 2880, loss 0.0102903, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:20.117300: step 2881, loss 0.00134676, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:21.466088: step 2882, loss 0.0238336, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T23:02:22.778277: step 2883, loss 0.00518996, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:24.040958: step 2884, loss 0.00103918, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:25.360054: step 2885, loss 0.0042862, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:26.612093: step 2886, loss 0.0212935, acc 0.984375, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T23:02:27.982799: step 2887, loss 0.00450505, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:30.063376: step 2888, loss 0.0045366, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:32.124316: step 2889, loss 0.00920937, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:33.873767: step 2890, loss 0.0183767, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:02:35.159395: step 2891, loss 0.0101987, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:36.443307: step 2892, loss 0.004338, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:37.730743: step 2893, loss 0.00348163, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:39.040383: step 2894, loss 0.00282812, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:40.307461: step 2895, loss 0.00563537, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:41.589737: step 2896, loss 0.0659119, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T23:02:42.983040: step 2897, loss 0.000394165, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:44.281801: step 2898, loss 0.0087627, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:45.556327: step 2899, loss 0.00118063, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:46.878104: step 2900, loss 0.010238, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T23:02:50.490753: step 2900, loss 0.75527, acc 0.85034, precision 0.810606, recall 0.557292 f1 0.660494\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-2900\n",
            "\n",
            "2022-11-30T23:02:52.046424: step 2901, loss 0.00463776, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:53.385681: step 2902, loss 0.00152587, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:54.681711: step 2903, loss 0.00948245, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:55.970813: step 2904, loss 0.00293469, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:57.286902: step 2905, loss 0.00324174, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:02:58.625768: step 2906, loss 0.260776, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:03:00.007721: step 2907, loss 0.0506525, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:03:01.292710: step 2908, loss 0.0144588, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:02.603441: step 2909, loss 0.018947, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:03.927011: step 2910, loss 0.049268, acc 0.984375, precision 1, recall 0.958333 f1 0.978723\n",
            "2022-11-30T23:03:05.217987: step 2911, loss 0.0425264, acc 0.96875, precision 0.894737, recall 1 f1 0.944444\n",
            "2022-11-30T23:03:05.892906: step 2912, loss 0.000153452, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:07.209450: step 2913, loss 0.0113795, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:08.560951: step 2914, loss 0.0121335, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:09.895997: step 2915, loss 0.00792319, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:11.203914: step 2916, loss 0.00216468, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:12.545685: step 2917, loss 0.00191192, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:13.899919: step 2918, loss 0.0153187, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:15.208225: step 2919, loss 0.00420328, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:16.471375: step 2920, loss 0.00541561, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:17.797520: step 2921, loss 0.00400425, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:19.166671: step 2922, loss 0.00126001, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:20.488688: step 2923, loss 0.0106946, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:21.782183: step 2924, loss 0.0106589, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:23.113881: step 2925, loss 0.0100263, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:24.457697: step 2926, loss 0.0233806, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T23:03:25.771401: step 2927, loss 0.0114987, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:27.067061: step 2928, loss 0.00643771, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:28.342896: step 2929, loss 0.0239343, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:03:29.662394: step 2930, loss 0.000876799, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:30.940098: step 2931, loss 0.0925824, acc 0.96875, precision 0.9375, recall 0.9375 f1 0.9375\n",
            "2022-11-30T23:03:32.224434: step 2932, loss 0.012962, acc 0.984375, precision 1, recall 0.95 f1 0.974359\n",
            "2022-11-30T23:03:33.493206: step 2933, loss 0.0268782, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:03:34.783942: step 2934, loss 0.0129974, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:36.067578: step 2935, loss 0.00601127, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:37.368693: step 2936, loss 0.00153838, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:38.640578: step 2937, loss 0.0103712, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:39.943926: step 2938, loss 0.00134897, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:41.243302: step 2939, loss 0.00651566, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:42.551952: step 2940, loss 0.0200869, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T23:03:43.896140: step 2941, loss 0.0191324, acc 0.984375, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T23:03:45.186599: step 2942, loss 0.00304636, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:46.477929: step 2943, loss 0.00308666, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:47.767345: step 2944, loss 0.0655892, acc 0.9375, precision 0.8, recall 0.923077 f1 0.857143\n",
            "2022-11-30T23:03:49.094224: step 2945, loss 0.00989406, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:50.430701: step 2946, loss 0.00569736, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:51.764477: step 2947, loss 0.0218399, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:03:53.083632: step 2948, loss 0.00631349, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:54.386867: step 2949, loss 0.0289007, acc 0.984375, precision 1, recall 0.909091 f1 0.952381\n",
            "2022-11-30T23:03:55.684380: step 2950, loss 0.00432555, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:56.980615: step 2951, loss 0.00290831, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:58.275087: step 2952, loss 0.00010874, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:03:59.616051: step 2953, loss 0.0321485, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:04:00.940560: step 2954, loss 0.0143349, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:02.214778: step 2955, loss 0.00278552, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:03.510210: step 2956, loss 0.00277036, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:04.802923: step 2957, loss 0.0600406, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:04:06.075438: step 2958, loss 0.0156208, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T23:04:07.333371: step 2959, loss 0.00766931, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:08.612943: step 2960, loss 0.000569453, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:09.890794: step 2961, loss 0.0371122, acc 0.984375, precision 0.916667, recall 1 f1 0.956522\n",
            "2022-11-30T23:04:11.211078: step 2962, loss 0.00423514, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:12.544835: step 2963, loss 0.0242691, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T23:04:13.900683: step 2964, loss 0.00150767, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:15.221115: step 2965, loss 0.00110305, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:16.490262: step 2966, loss 0.00943433, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:17.735965: step 2967, loss 0.0202297, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T23:04:19.027478: step 2968, loss 0.00882845, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:20.354924: step 2969, loss 0.00307605, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:21.648987: step 2970, loss 0.00795837, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:22.962976: step 2971, loss 0.00116094, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:24.260868: step 2972, loss 0.0279654, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:04:25.537931: step 2973, loss 0.00194396, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:26.809733: step 2974, loss 0.00176635, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:28.103414: step 2975, loss 0.00154607, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:29.414255: step 2976, loss 0.00762946, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:30.701640: step 2977, loss 0.013253, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:31.989963: step 2978, loss 0.066253, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:04:33.294291: step 2979, loss 0.0379838, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:04:34.580348: step 2980, loss 0.000771115, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:35.845206: step 2981, loss 0.00553276, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:37.105719: step 2982, loss 0.0171352, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T23:04:38.381830: step 2983, loss 0.0592442, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:04:39.639201: step 2984, loss 0.104973, acc 0.953125, precision 0.944444, recall 0.894737 f1 0.918919\n",
            "2022-11-30T23:04:40.913464: step 2985, loss 0.00185875, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:42.174769: step 2986, loss 0.00448597, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:43.465248: step 2987, loss 0.0570311, acc 0.96875, precision 0.833333, recall 1 f1 0.909091\n",
            "2022-11-30T23:04:44.812164: step 2988, loss 0.0287884, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T23:04:46.086953: step 2989, loss 0.00804036, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:47.382572: step 2990, loss 0.00825387, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:48.643664: step 2991, loss 0.00862241, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:49.934261: step 2992, loss 0.0229524, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:04:51.223890: step 2993, loss 0.0107434, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:52.496716: step 2994, loss 0.0200234, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T23:04:53.781635: step 2995, loss 0.000788791, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:55.054170: step 2996, loss 0.0395534, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:04:56.323618: step 2997, loss 0.00628627, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:04:57.605981: step 2998, loss 0.0341936, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:04:58.924287: step 2999, loss 0.0129276, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T23:05:00.214804: step 3000, loss 0.00457829, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T23:05:03.783658: step 3000, loss 0.720812, acc 0.851701, precision 0.798561, recall 0.578125 f1 0.670695\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-3000\n",
            "\n",
            "2022-11-30T23:05:05.208924: step 3001, loss 0.00205788, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:06.521615: step 3002, loss 0.00663508, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:07.814265: step 3003, loss 0.00053299, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:09.104333: step 3004, loss 0.0164443, acc 0.984375, precision 0.947368, recall 1 f1 0.972973\n",
            "2022-11-30T23:05:10.366495: step 3005, loss 0.00940503, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:11.708688: step 3006, loss 0.0059511, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:13.005875: step 3007, loss 0.0029877, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:14.309257: step 3008, loss 0.0108329, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:15.616654: step 3009, loss 0.0038485, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:16.894115: step 3010, loss 0.0114112, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:18.180792: step 3011, loss 0.0126912, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:19.448653: step 3012, loss 0.042676, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T23:05:20.754130: step 3013, loss 0.0178606, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:05:22.113932: step 3014, loss 0.00220937, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:23.443098: step 3015, loss 0.00449495, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:24.094360: step 3016, loss 0.00075599, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:25.394235: step 3017, loss 0.00362441, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:27.044172: step 3018, loss 0.00207388, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:29.149450: step 3019, loss 0.00291885, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:31.121788: step 3020, loss 0.104676, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:05:32.612885: step 3021, loss 0.0343221, acc 0.984375, precision 0.933333, recall 1 f1 0.965517\n",
            "2022-11-30T23:05:33.904483: step 3022, loss 0.00038647, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:35.190218: step 3023, loss 0.01673, acc 0.984375, precision 1, recall 0.916667 f1 0.956522\n",
            "2022-11-30T23:05:36.474071: step 3024, loss 0.00869892, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:37.741606: step 3025, loss 0.00735437, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:39.030884: step 3026, loss 0.00921059, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:40.319205: step 3027, loss 0.0106548, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:41.670546: step 3028, loss 0.00189653, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:42.982425: step 3029, loss 0.00708735, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:44.264417: step 3030, loss 0.00257736, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:45.612752: step 3031, loss 0.00308208, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:46.945263: step 3032, loss 0.000496362, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:48.216555: step 3033, loss 0.00367738, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:49.513526: step 3034, loss 0.0220917, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T23:05:50.819627: step 3035, loss 0.0018141, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:52.163922: step 3036, loss 0.0143509, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:53.495422: step 3037, loss 0.0127602, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:54.836684: step 3038, loss 0.00287619, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:05:56.122800: step 3039, loss 0.0189076, acc 0.984375, precision 1, recall 0.954545 f1 0.976744\n",
            "2022-11-30T23:05:57.405409: step 3040, loss 0.0176772, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T23:05:58.708810: step 3041, loss 0.0128776, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:00.064730: step 3042, loss 0.0969797, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:06:01.359387: step 3043, loss 0.00275528, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:02.667869: step 3044, loss 0.00250087, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:03.985688: step 3045, loss 0.000762282, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:05.277943: step 3046, loss 0.000827171, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:06.582135: step 3047, loss 0.00893692, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:07.860897: step 3048, loss 0.00675625, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:09.181953: step 3049, loss 0.0057832, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:10.470747: step 3050, loss 0.023193, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:06:11.776976: step 3051, loss 0.133883, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:06:13.116060: step 3052, loss 0.0195795, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T23:06:14.443324: step 3053, loss 0.0295753, acc 0.96875, precision 0.888889, recall 1 f1 0.941176\n",
            "2022-11-30T23:06:15.817260: step 3054, loss 0.00242695, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:17.134285: step 3055, loss 0.00734369, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:18.418877: step 3056, loss 0.000783488, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:19.705256: step 3057, loss 0.0134567, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:20.996362: step 3058, loss 0.0328404, acc 0.96875, precision 0.95, recall 0.95 f1 0.95\n",
            "2022-11-30T23:06:22.311017: step 3059, loss 0.0101169, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:23.664894: step 3060, loss 0.00702104, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:24.946535: step 3061, loss 0.000842939, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:26.251398: step 3062, loss 0.0444307, acc 0.96875, precision 0.777778, recall 1 f1 0.875\n",
            "2022-11-30T23:06:27.551868: step 3063, loss 0.0287904, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:06:28.841651: step 3064, loss 0.0165719, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:06:30.110847: step 3065, loss 0.0169046, acc 0.984375, precision 1, recall 0.941176 f1 0.969697\n",
            "2022-11-30T23:06:31.384728: step 3066, loss 0.00788304, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:32.703591: step 3067, loss 0.00329991, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:34.013248: step 3068, loss 0.0411612, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T23:06:35.303080: step 3069, loss 0.00175795, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:36.573310: step 3070, loss 0.00366666, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:37.883169: step 3071, loss 0.00286038, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:39.192004: step 3072, loss 0.00466924, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:40.481533: step 3073, loss 0.00304475, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:41.769697: step 3074, loss 0.00193924, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:43.073597: step 3075, loss 0.00466541, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:44.370093: step 3076, loss 0.000725734, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:45.662359: step 3077, loss 0.000726893, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:47.025572: step 3078, loss 0.00144335, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:48.357725: step 3079, loss 0.0757334, acc 0.96875, precision 1, recall 0.857143 f1 0.923077\n",
            "2022-11-30T23:06:49.618788: step 3080, loss 0.00279221, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:50.935049: step 3081, loss 0.00415584, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:52.260457: step 3082, loss 0.00222961, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:53.582546: step 3083, loss 0.000971884, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:54.880789: step 3084, loss 0.00106707, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:56.178249: step 3085, loss 0.00604748, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:06:57.465558: step 3086, loss 0.0712177, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:06:58.786144: step 3087, loss 0.0082564, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:00.125202: step 3088, loss 0.00176944, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:01.402479: step 3089, loss 0.00857898, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:02.690229: step 3090, loss 0.0152047, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:07:04.003014: step 3091, loss 0.0104872, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:05.332290: step 3092, loss 0.101791, acc 0.96875, precision 0.846154, recall 1 f1 0.916667\n",
            "2022-11-30T23:07:06.647005: step 3093, loss 0.00824305, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:07.952280: step 3094, loss 0.000867415, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:09.239125: step 3095, loss 0.00432586, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:10.559209: step 3096, loss 0.00224824, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:11.862534: step 3097, loss 0.00313889, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:13.143902: step 3098, loss 0.0162614, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:14.432890: step 3099, loss 0.0529562, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T23:07:15.779924: step 3100, loss 0.000608509, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T23:07:19.450207: step 3100, loss 0.836444, acc 0.854422, precision 0.84, recall 0.546875 f1 0.662461\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-3100\n",
            "\n",
            "2022-11-30T23:07:20.906666: step 3101, loss 0.00382398, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:22.205452: step 3102, loss 0.00951409, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:23.515355: step 3103, loss 0.0666732, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T23:07:24.823945: step 3104, loss 0.0481673, acc 0.984375, precision 0.923077, recall 1 f1 0.96\n",
            "2022-11-30T23:07:26.160755: step 3105, loss 0.00145299, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:27.453995: step 3106, loss 0.000846374, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:28.733303: step 3107, loss 0.000518005, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:30.049679: step 3108, loss 0.0627674, acc 0.984375, precision 1, recall 0.947368 f1 0.972973\n",
            "2022-11-30T23:07:31.334762: step 3109, loss 0.0497096, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:07:32.597750: step 3110, loss 0.0178068, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:07:33.916241: step 3111, loss 0.00304471, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:35.204222: step 3112, loss 0.00481766, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:36.509969: step 3113, loss 0.0164323, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:37.792014: step 3114, loss 0.00629443, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:39.077709: step 3115, loss 0.00133481, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:40.356715: step 3116, loss 0.0564274, acc 0.96875, precision 0.909091, recall 1 f1 0.952381\n",
            "2022-11-30T23:07:41.652971: step 3117, loss 0.00698345, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:42.912814: step 3118, loss 0.0209787, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:44.228289: step 3119, loss 0.0140859, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:07:44.881330: step 3120, loss 0.0057504, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:46.165663: step 3121, loss 0.00175737, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:47.512808: step 3122, loss 0.00102522, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:48.865735: step 3123, loss 0.00414106, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:50.187275: step 3124, loss 0.000294598, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:51.454612: step 3125, loss 0.0495571, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T23:07:52.760240: step 3126, loss 0.000607507, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:54.066414: step 3127, loss 0.00809214, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:55.351045: step 3128, loss 0.0391182, acc 0.96875, precision 1, recall 0.866667 f1 0.928571\n",
            "2022-11-30T23:07:56.654123: step 3129, loss 0.0084607, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:07:57.966446: step 3130, loss 0.0172283, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:07:59.261357: step 3131, loss 0.0225713, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T23:08:00.612018: step 3132, loss 0.00082225, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:01.902827: step 3133, loss 0.0740859, acc 0.96875, precision 1, recall 0.875 f1 0.933333\n",
            "2022-11-30T23:08:03.214724: step 3134, loss 0.000975721, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:04.537637: step 3135, loss 0.000590387, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:05.831314: step 3136, loss 0.0275372, acc 0.984375, precision 1, recall 0.956522 f1 0.977778\n",
            "2022-11-30T23:08:07.130677: step 3137, loss 0.00403631, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:08.411675: step 3138, loss 0.00803188, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:09.684856: step 3139, loss 0.00251854, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:10.976069: step 3140, loss 0.0075443, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:12.250980: step 3141, loss 0.0100736, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:13.581179: step 3142, loss 0.00147395, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:14.910279: step 3143, loss 0.000825747, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:16.209467: step 3144, loss 0.00180664, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:17.592611: step 3145, loss 0.0131932, acc 0.984375, precision 0.9375, recall 1 f1 0.967742\n",
            "2022-11-30T23:08:18.934513: step 3146, loss 0.00648824, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:20.244587: step 3147, loss 0.00802963, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:21.524305: step 3148, loss 0.00220813, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:22.800673: step 3149, loss 0.00443594, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:24.074628: step 3150, loss 0.000276131, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:25.629493: step 3151, loss 0.00372049, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:27.695064: step 3152, loss 0.000687008, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:29.769745: step 3153, loss 0.0112001, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:31.426903: step 3154, loss 0.00700104, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:32.696870: step 3155, loss 0.00894723, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:33.996600: step 3156, loss 0.000472902, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:35.295921: step 3157, loss 0.00124357, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:36.599219: step 3158, loss 0.000864092, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:37.889223: step 3159, loss 0.038307, acc 0.96875, precision 0.904762, recall 1 f1 0.95\n",
            "2022-11-30T23:08:39.168887: step 3160, loss 0.000420097, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:40.425875: step 3161, loss 0.0233423, acc 0.984375, precision 0.928571, recall 1 f1 0.962963\n",
            "2022-11-30T23:08:41.722082: step 3162, loss 0.0249539, acc 0.96875, precision 0.875, recall 0.875 f1 0.875\n",
            "2022-11-30T23:08:42.993957: step 3163, loss 0.00659819, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:44.292826: step 3164, loss 0.0289016, acc 0.984375, precision 1, recall 0.944444 f1 0.971429\n",
            "2022-11-30T23:08:45.597554: step 3165, loss 0.00364285, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:46.906075: step 3166, loss 0.00229144, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:48.251426: step 3167, loss 0.00219296, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:49.577884: step 3168, loss 0.0335034, acc 0.984375, precision 0.95, recall 1 f1 0.974359\n",
            "2022-11-30T23:08:50.916959: step 3169, loss 0.00317098, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:52.218683: step 3170, loss 0.00271625, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:53.533227: step 3171, loss 0.0310859, acc 0.984375, precision 1, recall 0.9375 f1 0.967742\n",
            "2022-11-30T23:08:54.798547: step 3172, loss 0.00227311, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:56.101952: step 3173, loss 0.0103422, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:08:57.387406: step 3174, loss 0.0189225, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:08:58.694544: step 3175, loss 0.00580677, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:00.013685: step 3176, loss 0.00722743, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:01.308857: step 3177, loss 0.112191, acc 0.96875, precision 0.9, recall 1 f1 0.947368\n",
            "2022-11-30T23:09:02.587947: step 3178, loss 0.00601494, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:03.889318: step 3179, loss 0.0515309, acc 0.984375, precision 0.952381, recall 1 f1 0.97561\n",
            "2022-11-30T23:09:05.162490: step 3180, loss 0.0159692, acc 0.984375, precision 0.944444, recall 1 f1 0.971429\n",
            "2022-11-30T23:09:06.445119: step 3181, loss 0.00430512, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:07.765366: step 3182, loss 0.00381858, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:09.049806: step 3183, loss 0.0076301, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:10.363556: step 3184, loss 0.00746711, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:11.626767: step 3185, loss 0.00301255, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:12.901925: step 3186, loss 0.00472612, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:14.173678: step 3187, loss 0.00711764, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:15.483923: step 3188, loss 0.00166073, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:16.760323: step 3189, loss 0.00184674, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:18.102827: step 3190, loss 0.0136837, acc 0.984375, precision 1, recall 0.923077 f1 0.96\n",
            "2022-11-30T23:09:19.426487: step 3191, loss 0.00161545, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:20.696211: step 3192, loss 0.0550892, acc 0.984375, precision 1, recall 0.952381 f1 0.97561\n",
            "2022-11-30T23:09:21.980886: step 3193, loss 0.00634514, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:23.298762: step 3194, loss 0.0526034, acc 0.96875, precision 0.95, recall 0.95 f1 0.95\n",
            "2022-11-30T23:09:24.600496: step 3195, loss 0.0263591, acc 0.984375, precision 1, recall 0.928571 f1 0.962963\n",
            "2022-11-30T23:09:25.913685: step 3196, loss 0.00225794, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:27.237905: step 3197, loss 0.0183802, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:28.539387: step 3198, loss 0.00151164, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:29.822471: step 3199, loss 0.00166986, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:31.111609: step 3200, loss 0.000631405, acc 1, precision 1, recall 1 f1 1\n",
            "\n",
            "Evaluation:\n",
            "2022-11-30T23:09:34.669765: step 3200, loss 0.726836, acc 0.851701, precision 0.79021, recall 0.588542 f1 0.674627\n",
            "\n",
            "Saved model checkpoint to /content/runs/1669845417/checkpoints/model-3200\n",
            "\n",
            "2022-11-30T23:09:36.118527: step 3201, loss 0.00519048, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:37.385241: step 3202, loss 0.00356101, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:38.649511: step 3203, loss 0.0212664, acc 0.984375, precision 0.958333, recall 1 f1 0.978723\n",
            "2022-11-30T23:09:39.901891: step 3204, loss 0.00370864, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:41.187216: step 3205, loss 0.00202171, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:42.484577: step 3206, loss 0.00861595, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:43.794330: step 3207, loss 0.00131501, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:45.063762: step 3208, loss 0.00214045, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:46.354591: step 3209, loss 0.014527, acc 0.984375, precision 0.941176, recall 1 f1 0.969697\n",
            "2022-11-30T23:09:47.662967: step 3210, loss 0.00605473, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:49.022624: step 3211, loss 0.0275744, acc 0.984375, precision 1, recall 0.933333 f1 0.965517\n",
            "2022-11-30T23:09:50.333879: step 3212, loss 0.00282612, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:51.638707: step 3213, loss 0.000669425, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:52.943952: step 3214, loss 0.00132334, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:54.231182: step 3215, loss 0.00902656, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:55.507632: step 3216, loss 0.00429464, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:56.845041: step 3217, loss 0.0055725, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:58.135224: step 3218, loss 0.00159046, acc 1, precision 1, recall 1 f1 1\n",
            "2022-11-30T23:09:59.438810: step 3219, loss 0.0790252, acc 0.96875, precision 0.928571, recall 0.928571 f1 0.928571\n"
          ]
        }
      ],
      "source": [
        "! python cnn-text-classification-tf/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nphWwRV4GFL",
        "outputId": "2ff402f5-12d2-4661-a525-f32f2c87cd83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: runs/ (stored 0%)\n",
            "  adding: runs/1669754187/ (stored 0%)\n",
            "  adding: runs/1669754187/summaries/ (stored 0%)\n",
            "  adding: runs/1669754187/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754187/summaries/dev/events.out.tfevents.1669754187.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754187/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754187/summaries/train/events.out.tfevents.1669754187.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754187/vocab (deflated 54%)\n",
            "  adding: runs/1669754187/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669750826/ (stored 0%)\n",
            "  adding: runs/1669750826/summaries/ (stored 0%)\n",
            "  adding: runs/1669750826/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669750826/summaries/dev/events.out.tfevents.1669750826.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669750826/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669750826/summaries/train/events.out.tfevents.1669750826.276e31bbb49e (deflated 86%)\n",
            "  adding: runs/1669750826/vocab (deflated 54%)\n",
            "  adding: runs/1669750826/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669754918/ (stored 0%)\n",
            "  adding: runs/1669754918/summaries/ (stored 0%)\n",
            "  adding: runs/1669754918/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754918/summaries/dev/events.out.tfevents.1669754918.276e31bbb49e (deflated 87%)\n",
            "  adding: runs/1669754918/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754918/summaries/train/events.out.tfevents.1669754918.276e31bbb49e (deflated 87%)\n",
            "  adding: runs/1669754918/vocab (deflated 57%)\n",
            "  adding: runs/1669754918/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669754918/checkpoints/model-5400.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669754918/checkpoints/model-5600.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669754918/checkpoints/model-5700.index (deflated 47%)\n",
            "  adding: runs/1669754918/checkpoints/model-5300.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669754918/checkpoints/model-5400.index (deflated 47%)\n",
            "  adding: runs/1669754918/checkpoints/model-5500.meta (deflated 87%)\n",
            "  adding: runs/1669754918/checkpoints/model-5500.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669754918/checkpoints/model-5300.meta (deflated 87%)\n",
            "  adding: runs/1669754918/checkpoints/model-5600.index (deflated 47%)\n",
            "  adding: runs/1669754918/checkpoints/model-5700.meta (deflated 87%)\n",
            "  adding: runs/1669754918/checkpoints/model-5500.index (deflated 47%)\n",
            "  adding: runs/1669754918/checkpoints/model-5700.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669754918/checkpoints/model-5600.meta (deflated 87%)\n",
            "  adding: runs/1669754918/checkpoints/model-5400.meta (deflated 87%)\n",
            "  adding: runs/1669754918/checkpoints/model-5300.index (deflated 47%)\n",
            "  adding: runs/1669754918/checkpoints/checkpoint (deflated 80%)\n",
            "  adding: runs/1669751001/ (stored 0%)\n",
            "  adding: runs/1669751001/summaries/ (stored 0%)\n",
            "  adding: runs/1669751001/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669751001/summaries/dev/events.out.tfevents.1669751001.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669751001/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669751001/summaries/train/events.out.tfevents.1669751001.276e31bbb49e (deflated 87%)\n",
            "  adding: runs/1669751001/vocab (deflated 57%)\n",
            "  adding: runs/1669751001/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669751001/checkpoints/model-600.meta (deflated 87%)\n",
            "  adding: runs/1669751001/checkpoints/model-800.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669751001/checkpoints/model-900.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669751001/checkpoints/model-600.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669751001/checkpoints/model-900.meta (deflated 87%)\n",
            "  adding: runs/1669751001/checkpoints/model-800.meta (deflated 87%)\n",
            "  adding: runs/1669751001/checkpoints/model-700.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669751001/checkpoints/model-800.index (deflated 47%)\n",
            "  adding: runs/1669751001/checkpoints/model-700.index (deflated 47%)\n",
            "  adding: runs/1669751001/checkpoints/model-700.meta (deflated 87%)\n",
            "  adding: runs/1669751001/checkpoints/model-500.data-00000-of-00001 (deflated 10%)\n",
            "  adding: runs/1669751001/checkpoints/model-500.index (deflated 47%)\n",
            "  adding: runs/1669751001/checkpoints/model-600.index (deflated 47%)\n",
            "  adding: runs/1669751001/checkpoints/model-900.index (deflated 47%)\n",
            "  adding: runs/1669751001/checkpoints/model-500.meta (deflated 87%)\n",
            "  adding: runs/1669751001/checkpoints/checkpoint (deflated 80%)\n",
            "  adding: runs/1669754305/ (stored 0%)\n",
            "  adding: runs/1669754305/summaries/ (stored 0%)\n",
            "  adding: runs/1669754305/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754305/summaries/dev/events.out.tfevents.1669754305.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754305/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754305/summaries/train/events.out.tfevents.1669754305.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754305/vocab (deflated 54%)\n",
            "  adding: runs/1669754305/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669754583/ (stored 0%)\n",
            "  adding: runs/1669754583/summaries/ (stored 0%)\n",
            "  adding: runs/1669754583/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754583/summaries/dev/events.out.tfevents.1669754583.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754583/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754583/summaries/train/events.out.tfevents.1669754583.276e31bbb49e (deflated 87%)\n",
            "  adding: runs/1669754583/vocab (deflated 54%)\n",
            "  adding: runs/1669754583/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669754583/checkpoints/model-100.data-00000-of-00001 (deflated 11%)\n",
            "  adding: runs/1669754583/checkpoints/model-100.index (deflated 47%)\n",
            "  adding: runs/1669754583/checkpoints/model-100.meta (deflated 87%)\n",
            "  adding: runs/1669754583/checkpoints/checkpoint (deflated 50%)\n",
            "  adding: runs/1669754410/ (stored 0%)\n",
            "  adding: runs/1669754410/summaries/ (stored 0%)\n",
            "  adding: runs/1669754410/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754410/summaries/dev/events.out.tfevents.1669754410.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754410/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754410/summaries/train/events.out.tfevents.1669754410.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754410/vocab (deflated 54%)\n",
            "  adding: runs/1669754410/checkpoints/ (stored 0%)\n",
            "  adding: runs/1669754261/ (stored 0%)\n",
            "  adding: runs/1669754261/summaries/ (stored 0%)\n",
            "  adding: runs/1669754261/summaries/dev/ (stored 0%)\n",
            "  adding: runs/1669754261/summaries/dev/events.out.tfevents.1669754261.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754261/summaries/train/ (stored 0%)\n",
            "  adding: runs/1669754261/summaries/train/events.out.tfevents.1669754261.276e31bbb49e (deflated 88%)\n",
            "  adding: runs/1669754261/vocab (deflated 54%)\n",
            "  adding: runs/1669754261/checkpoints/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "! zip -r runs.zip runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFoeZnwA_QPR",
        "outputId": "55247201-1641-41da-9998-afe53ca31874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "update-alternatives: error: alternative path /usr/bin/python3.7 doesn't exist\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 497.6 MB 1.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (2.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (57.4.0)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.3.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.50.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.12)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (0.28.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.6.0\n",
            "  Downloading transformers-4.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.6.0) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.6.0) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.6.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.6.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.6.0) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1f0d407915800afbfb8d99f1402c4cd5ef00e01bb18ce315bc4cff2999dffcf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras==2.8.0 in /usr/local/lib/python3.8/dist-packages (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "# distilBERT-based classification\n",
        "\n",
        "# Upgrade Python\n",
        "! sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "! pip install tensorflow==2.8.0\n",
        "! pip install transformers==4.6.0\n",
        "! pip install keras==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfb95PkrK_Wa"
      },
      "outputs": [],
      "source": [
        "# Load post data\n",
        "posts = pd.read_json(\"data_labeled_lf_2class.json\")\n",
        "\n",
        "# Exclude posts with 0/empty comments\n",
        "posts = posts[posts[\"Comments Text\"].apply(lambda x: len(x) > 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "ffdf753e36724421acb6a8b1270b59ec",
            "195b4d7f3a8149b28d8ecfdf99394c45",
            "8ee5edb7bd6742a58fcb76c0f721fc09",
            "091652c5e7474f7283a9da77d63ad2f5",
            "adeffdbc359443f2a5468bd83765ce79",
            "cf7fc9e2513f439cbd33dcba2b4f83f2",
            "eb49d24fc7fd4842b235af83a653808c",
            "c356051735524d2e9cab5ea9c78868d6",
            "2fa81c28194f4933be0afd0615162b9f",
            "4e60c8502d964a0d914f5908a9c26f39",
            "72f49aac06ca4b65a99ecb83f2dba425",
            "fb2f8fd379a74008aeccc222e9f58633",
            "277366c7d020421eb3e58fe9e6af0014",
            "c6da862c63934d21a60c8fc53f6afcfb",
            "92509483c14d4fe7a4c5e3cb3798ae04",
            "3c470b2bb83348d4af35578b83c41895",
            "09b0285e2ea14344a3a62f7d294b7cf0",
            "ac8c4eb4c8584a758ebd6165d98d8e8a",
            "1f2d9be9f37e45b29eba7f15b57e1db5",
            "260313138f9b4566961b1fe705cecd54",
            "33a379ebaf5e4a959b7cc8cb74765376",
            "2d9f89a6df1c40bd84b86c1b18866735",
            "14f2ce06fde44ea3b8f3ce835ff88ac2",
            "d11a3d78b4574fcd949c0bd4cc2ddc1e",
            "dd59723f97014ddd8bb07b8341428a3c",
            "3c0c3f5ab6504352985f295cb0dec05a",
            "61441badb3744403bbf8d2cc59b97056",
            "ab48b02a580548dea82ee5cf680c2b81",
            "77722600c58744dc875c058eb2846277",
            "51c18bc1946243758029eb1b7dc8401e",
            "fdf33b226e0a4d72ad88182e20af3b46",
            "2bf4277135c641638bae40778d30f81a",
            "da98e0768acd4dc8902a9e8ae4b329af"
          ]
        },
        "id": "7aE1lL4ULSes",
        "outputId": "dbfb0697-c8d5-45dc-ff80-456f463972af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffdf753e36724421acb6a8b1270b59ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb2f8fd379a74008aeccc222e9f58633",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14f2ce06fde44ea3b8f3ce835ff88ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizer, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, create_optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenize text\n",
        "text_post = list(posts[\"Text\"])\n",
        "text_comments = [\" \".join(text) for text in posts[\"Comments Text\"]]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", truncation=True, padding=True)\n",
        "\n",
        "def fn_tokenize(texts):\n",
        "    return tokenizer(texts, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB95O28nLTO6"
      },
      "outputs": [],
      "source": [
        "unique_labels = set(label for label in posts[\"label_predicted\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_post, list(posts[\"label_predicted\"]), test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOpUkO1YPRv_"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(X_train, is_split_into_words=False, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(X_test, is_split_into_words=False, padding=True, truncation=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    y_test\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vApOMBBzRsLm",
        "outputId": "6fe8da89-4d4a-4724-8876-ce6164546f37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "206/206 [==============================] - ETA: 0s - loss: 0.4191 - accuracy: 0.8239 - precision_m: 0.1535 - recall_m: 0.5583 - f1_m: 0.2314"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "206/206 [==============================] - 214s 999ms/step - loss: 0.4191 - accuracy: 0.8239 - precision_m: 0.1535 - recall_m: 0.5583 - f1_m: 0.2314 - val_loss: 0.3401 - val_accuracy: 0.8819 - val_precision_m: 0.1501 - val_recall_m: 0.4973 - val_f1_m: 0.2174\n",
            "Epoch 2/10\n",
            "206/206 [==============================] - 204s 990ms/step - loss: 0.3049 - accuracy: 0.8794 - precision_m: 0.1678 - recall_m: 0.6427 - f1_m: 0.2561 - val_loss: 0.3001 - val_accuracy: 0.8831 - val_precision_m: 0.1796 - val_recall_m: 0.6650 - val_f1_m: 0.2707\n",
            "Epoch 3/10\n",
            "206/206 [==============================] - 204s 992ms/step - loss: 0.2361 - accuracy: 0.9092 - precision_m: 0.1722 - recall_m: 0.6703 - f1_m: 0.2636 - val_loss: 0.3221 - val_accuracy: 0.8855 - val_precision_m: 0.1868 - val_recall_m: 0.8009 - val_f1_m: 0.2910\n",
            "Epoch 4/10\n",
            "206/206 [==============================] - 205s 993ms/step - loss: 0.1613 - accuracy: 0.9433 - precision_m: 0.1965 - recall_m: 0.7978 - f1_m: 0.3050 - val_loss: 0.3265 - val_accuracy: 0.8733 - val_precision_m: 0.1964 - val_recall_m: 0.8046 - val_f1_m: 0.3043\n",
            "Epoch 5/10\n",
            "206/206 [==============================] - 204s 990ms/step - loss: 0.0982 - accuracy: 0.9671 - precision_m: 0.2009 - recall_m: 0.8606 - f1_m: 0.3166 - val_loss: 0.4029 - val_accuracy: 0.8782 - val_precision_m: 0.2039 - val_recall_m: 0.8759 - val_f1_m: 0.3195\n",
            "Epoch 6/10\n",
            "206/206 [==============================] - 204s 991ms/step - loss: 0.0633 - accuracy: 0.9781 - precision_m: 0.2132 - recall_m: 0.9097 - f1_m: 0.3334 - val_loss: 0.4147 - val_accuracy: 0.8745 - val_precision_m: 0.2084 - val_recall_m: 0.8850 - val_f1_m: 0.3247\n",
            "Epoch 7/10\n",
            "206/206 [==============================] - 204s 993ms/step - loss: 0.0415 - accuracy: 0.9878 - precision_m: 0.2161 - recall_m: 0.9594 - f1_m: 0.3404 - val_loss: 0.4763 - val_accuracy: 0.8685 - val_precision_m: 0.2092 - val_recall_m: 0.8984 - val_f1_m: 0.3283\n",
            "Epoch 8/10\n",
            "206/206 [==============================] - 204s 990ms/step - loss: 0.0326 - accuracy: 0.9890 - precision_m: 0.2136 - recall_m: 0.9350 - f1_m: 0.3384 - val_loss: 0.5070 - val_accuracy: 0.8770 - val_precision_m: 0.2126 - val_recall_m: 0.9417 - val_f1_m: 0.3352\n",
            "Epoch 9/10\n",
            "206/206 [==============================] - 204s 992ms/step - loss: 0.0266 - accuracy: 0.9918 - precision_m: 0.2142 - recall_m: 0.9626 - f1_m: 0.3405 - val_loss: 0.5314 - val_accuracy: 0.8721 - val_precision_m: 0.2048 - val_recall_m: 0.8844 - val_f1_m: 0.3217\n",
            "Epoch 10/10\n",
            "206/206 [==============================] - 204s 992ms/step - loss: 0.0096 - accuracy: 0.9985 - precision_m: 0.2174 - recall_m: 0.9529 - f1_m: 0.3440 - val_loss: 0.5862 - val_accuracy: 0.8721 - val_precision_m: 0.2119 - val_recall_m: 0.9353 - val_f1_m: 0.3335\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f090febac10>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")#chose the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)#define the loss function \n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)#build the model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=['accuracy', precision_m, recall_m, f1_m])# train the model \n",
        "model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
        "          validation_data=val_dataset.batch(BATCH_SIZE),\n",
        "          epochs=10,\n",
        "          batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQyQcA3EVySB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "091652c5e7474f7283a9da77d63ad2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e60c8502d964a0d914f5908a9c26f39",
            "placeholder": "​",
            "style": "IPY_MODEL_72f49aac06ca4b65a99ecb83f2dba425",
            "value": " 232k/232k [00:00&lt;00:00, 691kB/s]"
          }
        },
        "09b0285e2ea14344a3a62f7d294b7cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f2ce06fde44ea3b8f3ce835ff88ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d11a3d78b4574fcd949c0bd4cc2ddc1e",
              "IPY_MODEL_dd59723f97014ddd8bb07b8341428a3c",
              "IPY_MODEL_3c0c3f5ab6504352985f295cb0dec05a"
            ],
            "layout": "IPY_MODEL_61441badb3744403bbf8d2cc59b97056"
          }
        },
        "195b4d7f3a8149b28d8ecfdf99394c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf7fc9e2513f439cbd33dcba2b4f83f2",
            "placeholder": "​",
            "style": "IPY_MODEL_eb49d24fc7fd4842b235af83a653808c",
            "value": "Downloading: 100%"
          }
        },
        "1f2d9be9f37e45b29eba7f15b57e1db5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "260313138f9b4566961b1fe705cecd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "277366c7d020421eb3e58fe9e6af0014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b0285e2ea14344a3a62f7d294b7cf0",
            "placeholder": "​",
            "style": "IPY_MODEL_ac8c4eb4c8584a758ebd6165d98d8e8a",
            "value": "Downloading: 100%"
          }
        },
        "2bf4277135c641638bae40778d30f81a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9f89a6df1c40bd84b86c1b18866735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fa81c28194f4933be0afd0615162b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33a379ebaf5e4a959b7cc8cb74765376": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c0c3f5ab6504352985f295cb0dec05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf4277135c641638bae40778d30f81a",
            "placeholder": "​",
            "style": "IPY_MODEL_da98e0768acd4dc8902a9e8ae4b329af",
            "value": " 466k/466k [00:00&lt;00:00, 755kB/s]"
          }
        },
        "3c470b2bb83348d4af35578b83c41895": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e60c8502d964a0d914f5908a9c26f39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c18bc1946243758029eb1b7dc8401e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61441badb3744403bbf8d2cc59b97056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f49aac06ca4b65a99ecb83f2dba425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77722600c58744dc875c058eb2846277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ee5edb7bd6742a58fcb76c0f721fc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c356051735524d2e9cab5ea9c78868d6",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fa81c28194f4933be0afd0615162b9f",
            "value": 231508
          }
        },
        "92509483c14d4fe7a4c5e3cb3798ae04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a379ebaf5e4a959b7cc8cb74765376",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9f89a6df1c40bd84b86c1b18866735",
            "value": " 28.0/28.0 [00:00&lt;00:00, 196B/s]"
          }
        },
        "ab48b02a580548dea82ee5cf680c2b81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac8c4eb4c8584a758ebd6165d98d8e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adeffdbc359443f2a5468bd83765ce79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c356051735524d2e9cab5ea9c78868d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6da862c63934d21a60c8fc53f6afcfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f2d9be9f37e45b29eba7f15b57e1db5",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_260313138f9b4566961b1fe705cecd54",
            "value": 28
          }
        },
        "cf7fc9e2513f439cbd33dcba2b4f83f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d11a3d78b4574fcd949c0bd4cc2ddc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab48b02a580548dea82ee5cf680c2b81",
            "placeholder": "​",
            "style": "IPY_MODEL_77722600c58744dc875c058eb2846277",
            "value": "Downloading: 100%"
          }
        },
        "da98e0768acd4dc8902a9e8ae4b329af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd59723f97014ddd8bb07b8341428a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c18bc1946243758029eb1b7dc8401e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdf33b226e0a4d72ad88182e20af3b46",
            "value": 466062
          }
        },
        "eb49d24fc7fd4842b235af83a653808c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb2f8fd379a74008aeccc222e9f58633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_277366c7d020421eb3e58fe9e6af0014",
              "IPY_MODEL_c6da862c63934d21a60c8fc53f6afcfb",
              "IPY_MODEL_92509483c14d4fe7a4c5e3cb3798ae04"
            ],
            "layout": "IPY_MODEL_3c470b2bb83348d4af35578b83c41895"
          }
        },
        "fdf33b226e0a4d72ad88182e20af3b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffdf753e36724421acb6a8b1270b59ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_195b4d7f3a8149b28d8ecfdf99394c45",
              "IPY_MODEL_8ee5edb7bd6742a58fcb76c0f721fc09",
              "IPY_MODEL_091652c5e7474f7283a9da77d63ad2f5"
            ],
            "layout": "IPY_MODEL_adeffdbc359443f2a5468bd83765ce79"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}